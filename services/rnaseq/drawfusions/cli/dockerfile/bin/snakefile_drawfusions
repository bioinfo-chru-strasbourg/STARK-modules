##########################################################################
# Snakemakefile Version:   1.0
# Description:             Snakemake file to run drawfusions arriba module
##########################################################################

# DEV version 1.0 : 23/11/2022
# Authoring : Thomas LAVAUX

################## Context ##################
# launch snakemake -s  snakefile_drawfusions -c(numberofthreads) --config run=absolutepathoftherundirectory without / at the end of the path
# to launch the snakemake file, use --config to replace variables that must be properly set for the pipeline to work ie run path directory
# every variable defined in the yaml file can be change
# separate multiple variable with a space (ex  --config run=runname var1=0.05 var2=12)
# also use option --configfile another.yaml to replace and merge existing config.yaml file variables

# use -p to display shell commands
# use --lt to display docstrings of rules

# input file = bam files indexed and .arriba.fusions.tsv from arriba
# output file = arriba pdf report
################## Import libraries ##################

import os
import os.path
import glob
import pandas as pd
import json

from datetime import datetime
from shutil import copy2

################## Configuration file and PATHS ##################
configfile: "/app/config/default.yaml"
##################################################################

def finditemindico(sample_list, ext_list, dictionary, include_ext, exclude_ext):
	""" Function to search in a dictionary a file path itering by the first key (sample_list) and second key (extension_list) with an including and excluding filter """
	""" Result must be an non empty file """
	searchresult = ""
	for sample in sample_list:
		for ext in ext_list:
			try:
				items = dictionary[sample][ext]
				if include_ext in items and not exclude_ext in items:
					if os.path.exists(items) and os.path.getsize(items) != 0:
						searchresult = items
			except KeyError: pass
	return searchresult

def searchfiles(directory, search_arg, recursive_arg):
	""" Function to search all files in a directory, adding a search arguement append to the directory and a recursive_search options (True/False) """
	return sorted(filter(os.path.isfile, glob.glob(directory + search_arg, recursive=recursive_arg)))

def logsomefile(logfile, text, sep, items_list=None, items=None):
	""" Function to log a variable value or a list of values into a log file """
	with open(logfile, 'a+') as f:
		f.write(text + sep)
		if items_list:
			for items in items_list:
				if items == '':
					f.write(str('None') + sep)
				else:
					f.write(str(items) + sep)
		else:
			if items == '':
				f.write(str('None') + sep)
			else:
				f.write(str(items) + sep)

def extractlistfromfiles(file_list, ext_list, sep, position):
	""" Function for creating list from a file list, with a specific extension, a separator and the position of the string we want to extract """
	output_list = []
	for files in file_list:
		filesname = os.path.basename(files)
		for ext in ext_list:
			if filesname.endswith(ext):
				output_list.append(filesname.split(sep)[position])
	output_list = list(set(output_list))
	return output_list

def create_list(txtlistoutput, file_list, ext_list, pattern):
	""" Function to create a txt file from a list of files filtered by extension and pattern """
	with open(txtlistoutput, 'a+') as f:
		for files in file_list:
			for ext in ext_list:
				if pattern in files and files.endswith(ext):
					f.write(str(files) + "\n")

def extract_tag(tagfile, tag, tagsep, sep):
	""" Function to extract a tag """
	output_tag = ""
	row = open(tagfile, 'r').readline().strip().split(tagsep)
	for items in row:
		if tag in items and sep in items:
			output_tag = items.split(sep)[-1]
	return(output_tag)

### END OF FUNCTIONS ###
serviceName = config['serviceName']
date_time = datetime.now().strftime("%Y%m%d-%H%M%S")
runName = os.path.basename(os.path.normpath(config['run']))
inputDir = "/app/res/" + runName + "/" + date_time + "/input"
tmpDir = "/app/res/" + runName + "/" + date_time + "/tmp"
try:
	config['GROUP_NAME'] = os.path.normpath(config['run']).split('/')[4]
	config['APP_NAME'] = os.path.normpath(config['run']).split('/')[5]
except IndexError: pass
depotdir = config['depository'] + "/" + config['GROUP_NAME'] + "/" + config['APP_NAME'] + "/" + runName + "/"
if not config['OUTPUT_DIR']:
	config['OUTPUT_DIR'] = config['run']
logfile = tmpDir + "/" + serviceName + "." + date_time + '.parameters.log'

os.makedirs(inputDir, exist_ok = True)
os.makedirs(tmpDir, exist_ok = True)
os.makedirs(config['OUTPUT_DIR'], exist_ok = True)
if config['DEPOT_COPY'] == True:
	os.makedirs(depotdir, exist_ok = True)

# Search files in the rundir directory
files_list = searchfiles(os.path.normpath(config['run']), config['SEARCH_ARGUMENT'], config['RECURSIVE_SEARCH'])
# Create sample and aligner list
sample_list = extractlistfromfiles(files_list, config['PROCESS_FILE'], '.', 0)
aligner_list = extractlistfromfiles(files_list, config['PROCESS_FILE'], '.', 1)

# Exclude samples from the exclude_list case insensitive
for sample_exclude in config['EXCLUDE_SAMPLE']:
	for sample in sample_list:
		if sample.upper().startswith(sample_exclude.upper()):
			sample_list.remove(sample)

# If config['FILTER_SAMPLE'] variable is not empty, it will force the sample list
if config['FILTER_SAMPLE']:
	sample_list = list(config['FILTER_SAMPLE'])

# For validation analyse bam will be sample.aligner.validation.bam
if config['VALIDATION_ONLY'] == True:
	append_aligner = '.validation'
	aligner_list = [sub + append_aligner for sub in aligner_list]

runDict = {}
for samples in sample_list:
	runDict[samples] = {}
for samples in sample_list:
	for ext in config['EXT_INDEX_LIST']:
		for files in files_list:
			if os.path.basename(files).split(".")[0] == samples and os.path.basename(files).endswith(ext):
				if config['VALIDATION_ONLY'] == False:
					for filext in config['PROCESS_FILE']:
						if ext == filext and not 'validation' in files:
							runDict[samples][ext] = files
						if ext != config['PROCESS_FILE']:
							runDict[samples][ext] = files
				if config['VALIDATION_ONLY'] == True:
					for filext in config['PROCESS_FILE']:
						if ext == filext and 'validation' in files:
							runDict[samples][ext] = files
						if ext != config['PROCESS_FILE']:
							runDict[samples][ext] = files

logsomefile(logfile, 'Start of the analysis:', "\n", items = date_time)
logsomefile(logfile, 'List of samples:', "\n", items_list = sample_list)
logsomefile(logfile, 'Analyse run:', "\n", items = runName)
if aligner_list:
	logsomefile(logfile, 'Aligner list:', "\n", items_list = aligner_list)

################################################## RULES ##################################################

ruleorder: copy_bam > cramtobam

rule all:
	"""
	Rule will create an pdf for arriba result's visualization
	"""
	input:
		expand(tmpDir+"/{sample}.{aligner}.arribareport.pdf",aligner=aligner_list,sample=sample_list)

rule help:
	"""
	General help for drawfusions module
	Launch snakemake -s  snakefile_drawfusions -c(numberofthreads) --config DATA_DIR=absolutepathoftherundirectory (default is data) without / at the end of the path
	To launch the snakemake file, use --config to replace variables that must be properly set for the pipeline to work ie run path directory
	Every variable defined in the yaml file can be change
	Separate multiple variable with a space (ex  --config DATA_DIR=runname transProb=0.05 var1=0.05 var2=12)
	Also use option --configfile another.yaml to replace and merge existing config.yaml file variables
	Use -p to display shell commands
	Use --lt to display docstrings of rules
	Input file = bam files (if bai is needed, it will be generate), tsv output fusion list from arriba
	Output file = pdf report with arriba graphs
	"""

rule copy_tsv:
	"""
	Copy input files
	"""
	output:
		temp(inputDir+"/{sample}.arriba.fusions.tsv")
	params:
		process = config['PROCESS_CMD'],
		download_link = lambda wildcards: runDict[wildcards.sample]['arriba.fusions.tsv']
	shell:
		"""
		if [ "{params.process}" = "ls" ];
		then
		ln -sfn {params.download_link} {output}
		else
		rsync -azvh {params.download_link} {output}
		fi
		"""


rule copy_bam:
	"""
	Copy input files
	"""
	output:
		temp(inputDir+"/{sample}.{aligner}.bam")
	params:
		process = config['PROCESS_CMD'],
		download_link = lambda wildcards: runDict[wildcards.sample]['.bam']
	shell:
		"""
		if [ "{params.process}" = "ls" ];
		then
		ln -sfn {params.download_link} {output}
		else
		rsync -azvh {params.download_link} {output}
		fi
		"""

rule copy_cram:
	"""
	Copy input files
	"""
	output:
		temp(inputDir+"/{sample}.{aligner}.cram")
	params:
		process = config['PROCESS_CMD'],
		download_link = lambda wildcards: runDict[wildcards.sample]['.cram']
	shell:
		"""
		if [ "{params.process}" = "ls" ];
		then
		ln -sfn {params.download_link} {output}
		else
		rsync -azvh {params.download_link} {output}
		fi
		"""


rule cramtobam:
	"""
	Extract bam from a cram file
	"""
	input:
		rules.copy_cram.output
	output:
		temp(inputDir+"/{sample}.{aligner}.bam")
	params:
		refgenome = config['REFGENEFA_PATH']
	shell:
		"""
		samtools view -b -T {params.refgenome} -o {output} {input}
		"""

rule indexing:
	"""
	Indexing bam files with samtools
	"""
	input:
		inputDir+"/{sample}.{aligner}.bam"
	output:
		temp(inputDir+"/{sample}.{aligner}.bai")
	params:
		threads = config['THREADS']
	shell:
		"""
		samtools index -b -@ {params.threads} {input} {output}
		"""

# bam must be sorted by coordinate and indexed
rule DrawR:
	input:
		bam = inputDir+"/{sample}.{aligner}.bam", 
		arribafusiontsv = inputDir+"/{sample}.arriba.fusions.tsv",
		bai = inputDir+"/{sample}.{aligner}.bai"
	output:
		tmpDir + "/{sample}.{aligner}.arribareport.pdf"
	params:
		refgtfgencode = config['REFGTFGENCODE_PATH'],
		refprot = config['REFPROTDOMAIN_PATH'],
		cytoband = config['REFCYTOBAND_PATH'],
		arriba_scripts = config['ARRIBA_SCRIPTS']
	shell:
		"""
		Rscript {arriba_scripts}/draw_fusions.R --annotation={params.refgtfgencode} --fusions={input.arribafusiontsv} --output={output} --alignments={input.bam} --cytobands={params.cytoband} --proteinDomains={params.refprot}
		"""
onstart:
	shell("touch "+config['OUTPUT_DIR']+"/"+serviceName+"Running.txt")
	with open(logfile, "a+") as f:
		json.dump(config, f)
		f.write("\n")

onsuccess:
	include = config['INCLUDE_RSYNC']
	
	shell("touch "+config['OUTPUT_DIR']+"/"+serviceName+"Complete.txt")
	shell("rm -f "+config['OUTPUT_DIR']+"/"+serviceName+"Running.txt")
	
	date_time_end = datetime.now().strftime("%Y%m%d-%H%M%S")
	with open(logfile, "a+") as f:
		f.write("End of the analysis : ")
		f.write((date_time_end) + "\n")
	for sample in sample_list:
		shell("rm -f " + config['OUTPUT_DIR']+"/"+ sample + "/" + serviceName + "/* || true") # if rm stderr, the true will avoid exiting snakemake
	shell("rsync -azvh --include={include} --exclude='*' {tmpDir}/ "+config['OUTPUT_DIR']+"")
	for sample in sample_list:
		shell("cp " + config['OUTPUT_DIR']+"/"+sample+"/"+serviceName+"/"+sample+"_"+date_time+"_"+serviceName+"/* "+config['OUTPUT_DIR']+"/"+sample+"/"+serviceName+"/ || true")
	if config['DEPOT_COPY'] == True:
		shell("rsync -azvh --include={include} --exclude='*' {tmpDir}/ {depotdir}")
		for sample in sample_list:
			shell("cp " + depotdir+sample+"/"+serviceName+"/"+sample+"_"+date_time+"_"+serviceName+"/* "+depotdir+sample+"/"+serviceName+"/ || true")

onerror:
	include_log = config['INCLUDE_LOG_RSYNC']
	
	shell("touch "+config['OUTPUT_DIR']+"/"+serviceName+"Failed.txt")
	#shell("rm -f "+config['OUTPUT_DIR']+"/"+serviceName+"Running.txt")
	shell("rsync -azvh --include={include_log} --exclude='*' {tmpDir}/ "+config['OUTPUT_DIR']+"")