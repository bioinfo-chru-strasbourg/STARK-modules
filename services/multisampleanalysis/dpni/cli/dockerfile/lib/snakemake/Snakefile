import multiprocessing
import os
import shutil
import random
import string
import subprocess
import time
import numpy as np
from os.path import join as osj
from itertools import islice
import sys
from collections import OrderedDict, defaultdict
import re
import pandas as pd
from multiprocessing import Pool
import json


def systemcall(command):
	'''
	*passing command to the shell*
	*return list containing stdout lines*
	command - shell command (string)
	'''
	p = subprocess.Popen([command], stdout=subprocess.PIPE, shell=True)
	return p.stdout.read().decode('utf8').strip().split('\n')

def getbed(run):
	for samples in os.listdir(run):
		if os.path.isdir(osj(run, samples)):
			bed  = systemcall('find '+osj(run, samples)+' -name "'+samples+'.bed"')[0]
			if bed:
				return bed
	print("ERROR no bed in run "+run+" exit !")
	exit()
	

def configure(run, trio, genome, tools, output):
	family = {}
	family['tools'] = tools
	family['family'] = list()
	family['process'] = list()
	for samples in os.listdir(run):
		print("#[INFO] samples ", samples)
		if samples in trio and os.path.isdir(osj(run, samples)):
			samp = {}
			tagfile = systemcall('find '+osj(run, samples)+' -maxdepth 2  -name "'+samples+'.tag"')[0]
			if not tagfile:
				#TODO search ped file
				print("WARNING no tag file in sample "+ samples)
			else:
				with open(tagfile, 'r') as t:
					for lines in t:
						whole = lines.strip().split('!')
						print(whole)
						for tag in whole:
							if 'SEX' in tag and '_' in tag:
								sex = tag.split('_')[-1]
								samp['sex'] = sex
							elif 'SEX' in tag and '#' in tag:
								sex = tag.split('#')[-1]
								samp['sex'] = sex
								#family['family'][samples]['sex'] = sex
							#MOTHER, FOETUS, FATHER
							elif 'FOETAL' in tag:
								print(tag)
								fam = tag.split('#')[-1]
								if fam == 'FOETUS':
									family['foetus'] = samp
								else:
									samp['affinity'] = fam.lower()
									family['family'].append(samp)
			#Looking for bam files
			bamfile = systemcall('find '+osj(run, samples)+' -maxdepth 3 -name "*.bwamem.bam" ! -name "*.validation.*"')[0]
			samp['bam'] = bamfile
			if not bamfile:
				print("ERROR no bam file in sample "+samples+" exit !")
				exit()
			vcfile = systemcall('find '+osj(run, samples)+' -maxdepth 3 -name "'+samples+'.final.vcf"')[0]
			samp['vcf'] = vcfile
			samp['sample'] = samples
			if not vcfile:
				print("ERROR no vcf file in sample "+samples+" exit !")
				exit()
			if 'affinity' in samp:
				s = {}
				s[samples] = {}
				s[samples]['bam'] = bamfile
				family['process'].append(s)
			#family['family'] = samp
	bed = getbed(run)
	
	family['env'] = {}
	family['env']["output"] = "/app/res"
	family['env']["genome"] = genome
	family['env']["bed"] = bed
	

	json = writejson(family, output)
	return json



def vcfTodataframe(file, rheader=False):
	'''
	Take in input vcf file, or tsv and return a dataframe
	I"m gonna build my own vcf parser et puis c'est tout
	return 3 Dataframe, full, only sample, only info
	'''
	name, extension = os.path.splitext(file)
	
	header = []
	variants_tmp = []
	variants = []
	
	
	print("#[INFO] "+file)
	if extension == '.vcf':
		#print('[#INFO] VCF: '+file)
		with open(file) as f:
			for lines in f:
				if lines.startswith('#'):
					header.append(lines.strip())
				else:
					variants_tmp.append(lines)
						
	col = header[-1].strip().split('\t')
	for v in variants_tmp:
		variants.append(v.strip().split('\t'))

	rows = []
	#Creating Dataframe from the whole VCF	
	print("#[INFO] Whole VCF to Dataframe")
	for i, var in enumerate(variants):
		rows.append(pd.Series(var, index=col))
	#.T necessary to transpose index and columns
	dfVar = pd.concat(rows, axis=1, ignore_index=True).T
	
	if rheader:
		return dfVar, header
	else: 
		return dfVar

def uniqueChr(df):
	unique = df['#CHROM'].unique()
	return unique

def parseInfoField(dfVar):
	'''
	input: take a dataframe (from vcf)
	
	output: return a dataframe of the vcf when the info field is parsed 
	'''
	
	############
	#Parsing INFO field from dfVar dataframe containing all informations from vcf
	############
	
	#print(dfVar.head())
	infoList = []
	dicoInfo = []
	headers = []
	
	
	print("#[INFO] Parsing INFO field")
	for i,elems in dfVar.iterrows():
		infoList.append([x.split('=') for x in elems['INFO'].split(';')])
			
	print("\n")
	
	
	[headers.append(elems[0]) for ite in infoList for elems in ite]
	dfInfo = pd.DataFrame(columns=np.unique(np.array(headers)))
	#print(np.unique(np.array(headers)))
	
	
	#print(infoList[0:5])
	print("#[INFO] From INFO field to Dataframe")
	for j, elems in enumerate(infoList):
		add = {}
		for fields in elems:
			if len(fields) <= 1:
				f = {fields[0]: 'TRUE'}
				add.update(f)
			else:
				f = dict([fields])
				add.update(f)
				
		dicoInfo.append(add)
		
	print("\n")
	#print(dicoInfo.keys())
	#print(dict(list(dicoInfo.items())[0:2]))
	
	df_final = pd.DataFrame(dicoInfo, columns=np.unique(np.array(headers)))
	
	dfInfo = dfVar.join(df_final, how='inner')
	
	return dfInfo
	
def parseFile(folder, df, unique):
	'''
	input: folder containing cov depth file by chr, vcf of the sample and list of unique chr
	'''
	final = []
	#for each depth count file 
	for covfiles in os.listdir(folder):
		if not covfiles.startswith('COVFILELocus'):
			#iterate chr by chr
			print("#[INFO] chr"+covfiles)
			for j, string in enumerate(unique):
				if covfiles.endswith(string):
					#print("#[INFO] Chrcov "+covfiles)
					data = []
					#read cov file results from gatk depth coverage
					tmp = pd.read_csv(osj(folder, covfiles), sep='\t', names=['Locus_F', 'Total_Depth_F', 'Average_Depth_sample_F', 'POOL_F_Depth', 'POOL_F_base_counts', 'Locus_M', 'Total_Depth_M', 'Average_Depth_sample_M', 'POOL_M_Depth', 'POOL_M_base_counts'])
					cov = tmp.to_dict('records')
					
					#Iterate over dataframe from vcf norm sorted and unique
					for i, chpos in df[['#CHROM', 'POS']].iterrows():
						if string == chpos[0]:
							#print(chpos[1][1])
							#time.sleep(10)
							dot = ':'.join(chpos)
							
							val = [dico for dico in cov if dot in dico.values()]
							final.append(val[0])
							#add row in list
							#final.append(cov[(cov == dot).any(axis=1)])
							
				else:
					continue
	print("#[INFO] Length variants ", len(final))
	return final
	#time.sleep(10)

def addFrequence(df):
	#df.sort_values(by=['#CHROM', 'POS'], ascending=[False, True], key=lambda x: np.argsort(index_natsorted(df['#CHROM'] 
	
	data = []
	#print(df.columns)
	#print(df.head())
	for i, values in df.iterrows():
		#print(values)
		last = {}
		last['#CHROM'] = values['Locus'].strip().split(':')[0]
		last['POSITION'] = values['Locus'].strip().split(':')[1]
		tmp = values['Base_counts'].strip().split()
		tmp_data = {}
		tmp2_data = {}
		for bases in tmp:
			base = bases.split(':')
			tmp_data[base[0]] = base[1]
			#print(tmp_data)
		for count in tmp_data.items():
			#print(count)
			try:
				tmp2_data[count[0]] = str(round(float(count[1]) / values['Total_Depth'], 2))
			except:
				print("#[INFO] Depth is 0 for ", values['Locus'])
				tmp2_data[count[0]] = "0"
				
		count = '|'.join(':'.join((key,val)) for (key,val) in tmp_data.items())
		freq = '|'.join(':'.join((key,val)) for (key,val) in tmp2_data.items())
		
		last['Depth'] = values['Depth']
		last['Base_counts'] = count
		last['Base_frequence'] = freq
			
		data.append(last)
	final = pd.DataFrame(data)
		
	return final


def denovoFilter(df, res):
	#####merge df raw and df which new columns
	result = pd.merge(df, res, on=["#CHROM", "POS"])
	
	#####denovo
	result['denovo_filter'] = None
	for i, var in result.iterrows():
		ref = var['REF']
		mut = var['ALT']
		freq = [item.split(':') for item in var['POOL_F_base_counts'].split('|')]
		freq.extend([item.split(':') for item in var['POOL_M_base_counts'].split('|')])
		#[dict(item.split(':')[0] = item.split(':')[1]) for item in freq_f]
		val = []
		for values in freq:
			#print(values)
			#time.sleep(10)
			if values[0] == mut:
				#print(values[0], mut)
				val.append(int(values[1]))
		
		#filter only for SNV 
		if val and all(value < 3 for value in val) and var['POOL_F_Depth'] >= 300 and var['POOL_M_Depth'] >= 300 and len(mut) == 1 and len(ref) == 1:
			result.loc[i, 'denovo_filter'] = "Denovo"
			#print("#[INFO] counts", val)
			#print(result.loc[i, 'denovo_filter'])
			#time.sleep(5)
		elif val and all(value < 2 for value in val) and var['POOL_F_Depth'] >= 200 and var['POOL_M_Depth'] >= 200 and len(mut) == 1 and len(ref) == 1:
			result.loc[i, 'denovo_filter'] = "LikelyDenovo"
			#print("#[INFO] counts", val)
			#print(result.loc[i, 'denovo_filter'])
			#print(var)
		elif len(mut) > 1 and len(ref) == 1:
			result.loc[i, 'denovo_filter'] = "unknown_insertion"
		elif len(ref) > 1 and len(mut) == 1:
			result.loc[i, 'denovo_filter'] = "unknown_deletion"
		else:
			result.loc[i, 'denovo_filter'] = "Inherit"
	
	print(result['denovo_filter'].value_counts())
		
	return result

def addHeader(col, header):
	for field in col:
		if field == 'denovo_filter':
			info = '##INFO=<ID='+field+',Number=.,Type=String,Description="denovo_filter Filter established in genetics diagnosis team in the CHRU of Strasbourg, allele depth < 3 && Depth >= 300 > Denovo or allele depth < 2 && Depth >= 200 Likelydenovo, remaining variants are considered as inherit for snv and inherit_indels">\n'
			header.insert(header.index(header[-1]), info)
		else:
			info = '##INFO=<ID='+field+',Number=.,Type=String,Description="GATK '+field+'">\n'
			header.insert(header.index(header[-1]), info)
	return header

def createFinaldf(df, header, oup):
	raw = df[df.columns[0:10]]
	with open(oup, 'w+') as f:
		for lines in header:
			f.write(lines.strip()+'\n')
	raw.to_csv(oup, mode='a', index=False, header=False, sep='\t')
	return raw
	
def parseSpeed(covfiles, df, col):
	'''
	input: folder containing cov depth file by chr, vcf of the sample and list of unique chr
	'''
	#hide warning

	pd.set_option('mode.chained_assignment', None)
	
	
	print("#[INFO] chr "+covfiles)
	#print("#[INFO] Chrcov "+covfiles)
	data = []
	#read cov file results from gatk depth coverage
	tmp = pd.read_csv(covfiles, sep='\t', names=col)

	#print(tmp.columns)
	#print(tmp.head())
	#chr number
	chr = os.path.basename(covfiles)[7:]
	cov = tmp.loc[tmp['Locus'].str.startswith(chr+':')]#.to_dict('records')
	print("INFO COV", cov.head())

	df_tmp = df.loc[df['#CHROM'] == chr]
	#print(df_tmp.head())
	

	df_tmp.loc[:, 'Locus'] = df.loc[:,'#CHROM'].astype(str)+':'+df.loc[:,'POS'].astype(str)

	df_tmp = df_tmp.set_index('Locus')
	cov = cov.set_index('Locus')

	print(cov.head())
	
	print("INFO DF_TMP "+covfiles, df_tmp)
	print("INFO length dftmp ", len(df_tmp.index))
	#final_tmp = df_tmp.merge(cov, on='Locus', how='left')

	final = df_tmp.merge(cov, left_index=True, right_index=True, how='left')

	#final = pd.merge(df_tmp, tmp, on=['Locus'])
	final.reset_index(level=0, inplace=True)
	locus = final['Locus']
	final.drop(columns='Locus', inplace=True)
	final.insert(10, "Locus", locus)
	final = final.iloc[:, 1:]
	print("#[INFO] Length variants "+os.path.basename(covfiles), len(final.index))
	return final

def parse(dico):
	data={}
	for file in dico:
		name=dico[file]["vcf"].split("/")[-1].split(".")[0]
		data[name]=dico[file]["vcf"]
	return data

def get_pool_name(pool):
	dico_pool = {}
	for item in pool:
		name = pool[item]["vcf"].split("/")[-1].split(".")[0]
		dico_pool[name] = pool[item]["vcf"]
	return dico_pool

def writejson(d,f):
	"""
	Read dict, return a json file
	"""
	with open(f, 'w') as outfile:
		json.dump(d, outfile, indent=4)




#Prerequisites

	#trio = ["ADN2104279", "ADN2104278", "ASG184957"]
	#configure(config['run'], trio, "/STARK/databases/genomes/current/hg19.fa", "/app/res")
#configfile: "/app/res/analysis.json"
onstart: print(config)
output = config["env"]["output"]
log = output+"/.log"
#pool_name = get_pool_name(config["analysis"]["pool"])

if "threads" in config["tools"]:
	nbCPU =  config["tools"]["threads"]
else:
	if multiprocessing.cpu_count() > 2:
		nbCPU = multiprocessing.cpu_count()-1
	else:
		nbCPU = 1

interval_padding = 100


rule all:
	params:
		bgzip = config["tools"]["bgzip"],
		tabix = config["tools"]["tabix"]
	input:
		expand(output+"/{sample}.final.vcf", sample=config['family'])
	log:
		log+"/all.log"
	shell:
		"""
			for VCF in {input}; do {params.bgzip} --force $VCF && {params.tabix} -p vcf $VCF.gz; done
		"""


rule paste_vcf_pool:
	input:
		foetus = config['foetus']['vcf']
	output:
		output+"/pool.vcf"
	shell:
		"""
			cp {input} {output}
		"""

rule add_info:
	input:
		vcf = output+"/{sample}.norm.uniq.sorted.vcf",
		cov = output+"/all.gatk.cov"
		cov_sample = output+"/sample.gatk.cov"
	output:
		vcf = temp(output+"/{sample}.final.vcf")
	threads: config['tools']['threads']
	run:
		subprocess.call("mkdir -p "+output.vcf+".dict", shell=True)
		subprocess.call("awk -F \":\" '{print $0 >> (\""+output.vcf+".dict/COVFILE\" $1)}' "+input.cov, shell=True)
		#####new
		#transform raw vcf into dataframe and insert header in list
		df, head = vcfTodataframe(input.vcf, True)
		
		#unique chr cov file from gatk
		unique = uniqueChr(df)
		list_var = []
		
		col = ['Locus', 'Total_Depth', 'Average_Depth', 'Depth', 'Base_counts']
		#parse variants by chr to speed up analysis
		#prepare args for multiprocessing
		print("#[INFO] Parse variants by chr")
		list_cov = [(osj(output.vcf+'.dict', covfiles), df, col) for covfiles in os.listdir(output.vcf+'.dict') if not covfiles.startswith('COVFILELocus')]
		print("INFO LISTCV", list_cov)
		print("#[INFO] chr nbr", len(list_cov))
		with Pool(config['tools']['threads']) as pool:
			result = pool.starmap(parseSpeed, list_cov)
			for locus in result:
				list_var.append(locus)

		#print("#[INFO] LISTVAR ", list_var)
		#result = pd.concat(list_var, axis=1, ignore_index=True)
		print("#[INFO] Process annotations")
		#frequence annotations
		metrics = pd.concat(list_var)
		metrics = metrics.dropna(subset=['REF', 'ALT'], how='all')
		print(metrics.columns)
		print(len(metrics.index))

		frequence = addFrequence(metrics)
		print(frequence.columns)
		print(len(frequence.index))
		
		dfinal = pd.concat([metrics.reset_index(drop=True), frequence.reset_index(drop=True)], axis=1)
		print("#[INFO] length df merge "+str(len(dfinal.index)))
		print(dfinal.columns)
		info = frequence.loc[:, ['Depth', 'Base_counts', 'Base_frequence']]

		for items in list(info.columns):
			info[items] = items+'='+info[items].astype(str)
		info['INFOS'] = info[['Depth', 'Base_counts', 'Base_frequence']].agg(';'.join, axis=1)
		dfinal['INFO'] = dfinal['INFO']+';'+info['INFOS']
		print(dfinal)
		print(dfinal.columns)
		#Add new rows in header
		#newheader = addHeader(denovo.columns[10:], head)
		
		tmp_vcf = output.vcf+'.tmp'
		sample = os.path.basename(tmp_vcf).split('.')[0]
		
		#Create final vcf with new header
		createFinaldf(dfinal[['#CHROM', 'POS', 'ID', 'REF', 'ALT', 'QUAL', 'FILTER', 'INFO', 'FORMAT', sample]], head, tmp_vcf)
		
		#sort and supress dict of coverage files
		subprocess.call('grep "^#" '+tmp_vcf+' > '+output.vcf+' && grep -v "^#" '+tmp_vcf+' | sort -k1,1V -k2,2n >> '+output.vcf, shell=True)
		subprocess.call("rm -rf "+output.vcf+".dict", shell=True)

rule copy_vcf_pool:
	params:
		bcftools = config["tools"]["bcftools"]
	input:
		foetus = output+"/pool.vcf"
	output:
		tmp = temp(output+"/pool.renamed.vcf")
	log:
		log+"/pool.bcftools.log"
	shell:
		"""
			{params.bcftools} annotate --remove "^INFO/FindByPipelines,INFO/GenotypeConcordance" {input} > {output.tmp} 2>> {log}
		"""

rule copy_vcf_sample:
	params:
		bcftools = config["tools"]["bcftools"]
	input:
		lambda wildcards: config["family"][wildcards.sample]["vcf"]
	output:
		temp(output+"/{sample}.renamed.vcf")
	log:
		log+"/{sample}.bcftools.log"
	shell:
		"""
			{params.bcftools} annotate --remove "^INFO/FindByPipelines,INFO/GenotypeConcordance" {input} > {output} 2>> {log}
		"""

rule compress_index_vcf:
	params:
		bgzip = config["tools"]["bgzip"],
		tabix = config["tools"]["tabix"]
	input:
		output+"/{sample}.renamed.vcf"
	output:
		gz  = output+"/{sample}.renamed.vcf.gz",
		tbi = output+"/{sample}.renamed.vcf.gz.tbi"
	shell:
		"""
			{params.bgzip} -c {input} > {output.gz} && {params.tabix} -p vcf {output.gz}
		"""

rule merge_vcf:
	params:
		bcftools=config["tools"]["bcftools"]
	input:
		sample_vcf = expand(output+"/{sample}.renamed.vcf.gz", sample=config["family"]),
		pool_vcf = output+"/pool.renamed.vcf.gz"
	output:
		output+"/{sample}.merged.vcf"
	log:
		log+"/{sample}.bcftools.log"
	shell:
		"""
			{params.bcftools} merge -m none {input.sample_vcf} {input.pool_vcf} -o {output} 2>> {log}
		"""

#rule howard_trio:
#	params:
#		howard = config["tools"]["howard"]
#	input:
#		output+"/{sample}.merged.vcf"
#	output:
#		output+"/{sample}.trio.howard.vcf"
#	log:
#		log+"/{sample}.howard.log"
#	shell:
#		"""
#			{params.howard} --input={input} --output={output} --calculation=BARCODE --trio=POOL_M,POOL_F,{wildcards.sample} &>> {log}
#		"""

rule normalize_vcf:
	params:
		bcftools = config["tools"]["bcftools"],
		genome = config["env"]["genome"]
	input:
		output+"/{sample}.merged.vcf"
	output:
		temp(output+"/{sample}.norm.vcf")
	log:
		log+"/{sample}.bcftools.normalize.log"
	shell:
		"""
			{params.bcftools} norm -m-any {input} | {params.bcftools} norm -d none -f {params.genome} -o {output} 2>> {log}
		"""

#only for multiple in trio in batch
rule uniq_trio:
	params:
		bcftools=config["tools"]["bcftools"]
	input:
		output+"/{sample}.norm.vcf",

	output:
		temp(output+"/{sample}.norm.uniq.vcf")
	log:
		log+"/{sample}.bcftools.log"
	shell:
		"""
			{params.bcftools} view -s {wildcards.sample} {input} | grep -v '^##HOWARD' > {output} 2>> {log}
		"""

rule sort_vcf:
	input:
		output+"/{sample}.norm.uniq.vcf"
	output:
		output+"/{sample}.norm.uniq.sorted.vcf"
	shell:
		"""
			grep "^#" {input} > {output} && grep -v "^#" {input} | sort -k1,1V -k2,2n >> {output}
		"""


rule depth_of_coverage_v2:
	params:
		cpu     = config["tools"]["threads"],
		padding = interval_padding,
		java    = config["tools"]["java"],
		gatk    = config["tools"]["gatk"]
	input:
		genome  = config["env"]["genome"],
		bed     = config["env"]["bed"],
		bam     = config["foetus"]["bam"]
		
	output:
		temp(output+"/foetus.gatk.cov"),
		temp(output+"/foetus.gatk.cov.sample_cumulative_coverage_counts"),
		temp(output+"/foetus.gatk.cov.sample_cumulative_coverage_proportions"),
		temp(output+"/foetus.gatk.cov.sample_statistics"),
		temp(output+"/foetus.gatk.cov.sample_summary")
	log:
		log+"/foetus.gatk.cov.log"
	shell:
		"""
			{params.java} -jar {params.gatk} -nt {params.cpu} -omitIntervals -T DepthOfCoverage -R {input.genome} -o {output[0]} -I {input.bam} --includeDeletions --minMappingQuality 10 --printBaseCounts -L {input.bed} --interval_padding {params.padding} &>> {log};
		"""

rule rename_pool_coverage_v2:
	input:
		output+"/foetus.gatk.cov"
	output:
		output+"/all.gatk.cov"
	shell:
		"""
			sed '1 s/^.*$/Locus\tTotal_Depth\tAverage_Depth\tDepth\tBase_counts/' {input} > {output}
		"""

rule depth_of_coverage_sample:
	params:
		cpu     = config["tools"]["threads"],
		padding = interval_padding,
		java    = config["tools"]["java"],
		gatk    = config["tools"]["gatk"]
	input:
		genome  = config["env"]["genome"],
		bed     = config["env"]["bed"],
		lambda wildcards: config["family"][wildcards.sample]["bam"]
		
	output:
		temp(output+"/{sample}.gatk.cov"),
		temp(output+"/{sample}.gatk.cov.sample_cumulative_coverage_counts"),
		temp(output+"/{sample}.gatk.cov.sample_cumulative_coverage_proportions"),
		temp(output+"/{sample}.gatk.cov.sample_statistics"),
		temp(output+"/{sample}.gatk.cov.sample_summary")
	log:
		log+"/{sample}.gatk.cov.log"
	shell:
		"""
			{params.java} -jar {params.gatk} -nt {params.cpu} -omitIntervals -T DepthOfCoverage -R {input.genome} -o {output[0]} -I {input.bam} --includeDeletions --minMappingQuality 10 --printBaseCounts -L {input.bed} --interval_padding {params.padding} &>> {log};
		"""

rule merge_coverage_v2:
	input:
		expand(output+"/{sample}.gatk.renamed.cov", sample=config["family"])
	output:
		output+"/sample.gatk.cov"
	shell:
		"""
			paste {input} > {output}
		"""