##########################################################################
# Snakemakefile Version:   0.1
# Description:             Snakemake file to run SCRAMBLE module
##########################################################################

# DEV version 0.1 : 10/11/2021
# INT version 0.1 : 17/03/2022
# Authoring : Thomas LAVAUX

################## Context ##################
# launch snakemake -s  snakefile_scramble -c(numberofthreads) --config run=absolutepathoftherundirectory without / at the end of the path
# to launch the snakemake file, use --config to replace variables that must be properly set for the pipeline to work ie run path directory
# every variable defined in the yaml file can be change
# separate multiple variable with a space (ex  --config run=runname var1=0.05 var2=12)
# also use option --configfile another.yaml to replace and merge existing config.yaml file variables

# use -p to display shell commands
# use --lt to display docstrings of rules

# input file = bam files (if bai is needed, it will be generate)
# output file = vcf annoted with AnnotSV 3.x for each sample/bam, and a global vcf file will all the samples ; a set of files by design/panel
# warning : bcftools merge will crash if there's only 1 sample, so we need to cp files
################## Import libraries ##################
import os
import os.path
import glob
import pandas as pd
import json

from datetime import datetime
from shutil import copy2
from shutil import move

################## Configuration file and PATHS ##################
configfile: "/app/config/default.yaml"

### COMMON VARIABLES ###
# Analysis only validation bam/cram files
validation_only = config['VALIDATION_ONLY']

# Debug mode
debug = config['DEBUG_MODE']

# Subdir of module results
serviceName = config['serviceName']

# Datas directory
rundir = config['run']

# Repository/depository directories
repositorydir = config['repository']
depositorydir = config['depository']
outputdir = config['OUTPUT_DIR']

# Copy to depot (archive)
depotcopy = config['DEPOT_COPY']

# Group and app name to construct output
group_name = config['GROUP_NAME']
app_name = config['APP_NAME']

# Design.bed or SAMPLE.bed or SAMPLE.aligner.design.bed in STARK folder
bed_file = config['BED_FILE']
# Panel.SAMPLE.APP.manifest.genes.bed or SAMPLE.APP.manifest.genes.bed in STARK folder
genes_file = config['GENES_FILE']
# Transcripts file containing NM notation SAMPLE.transcripts in STARK folder
transcripts_file = config['TRANSCRIPTS_FILE']
# List genes contains list of gene's files
list_genes = config['LIST_GENES']

# Search options to find files to process
recursive_search = config['RECURSIVE_SEARCH']
search_argument = config['SEARCH_ARGUMENT']

# Sample to remove, set in config_scramble.yaml file
sample_exclude_list = config['EXCLUDE_SAMPLE_LIST']
filter_sample_list = config['FILTER_SAMPLE_LIST']

# Ext to index into dictionary
ext_list = config['EXT_INDEX_LIST']
# Option for processing (cp/ls/samtools)
process_cmd = config['PROCESS_CMD']
# Option for processing bam or cram
process_file = config['PROCESS_FILE']

# Option to append an aligner name to a file (case if the bam don't have an aligner name == sample.bam)
aligner_name = config['ALIGNER_NAME']

# Rsync options for copying results/log
include_file = config['INCLUDE_RSYNC']
exclude_file = config['EXCLUDE_RSYNC']
include_file_log = config['INCLUDE_LOG_RSYNC']

### Variables specific for the tools ###
# Refgene variable
refgene = config['REFGENEFA_PATH']
# Scripts path
scriptspath = config['SCRIPTS_PATH']

### FUNCTIONS ###

def finditemindico(sample_list, ext_list, dictionary, include_ext, exclude_ext):
	""" Function to search in a dictionary a file path itering by the first key (sample_list) and second key (extension_list) with an including and excluding filter """
	""" Result must be an non empty file """
	searchresult = ""
	for sample in sample_list:
		for ext in ext_list:
			try:
				items = dictionary[sample][ext]
				if include_ext in items and not exclude_ext in items:
					if os.path.exists(items) and os.path.getsize(items) != 0:
						searchresult = items
			except KeyError: pass
	return searchresult

def searchfiles(directory, search_arg, recursive_arg):
	""" Function to search all files in a directory, addind a search arguement append to the directory and a recursive_search options (True/False) """
	return sorted(filter(os.path.isfile, glob.glob(directory + search_arg, recursive=recursive_arg)))

def processalignedfiles(storedir, dictonnary, sample, ext, aligner_list, process_cmd, name=None):
	""" Function to process bam or cram files who's path are stored in a dictionary by sample/ext name"""
	""" You can either cp, ls bam files ; or use samtools to extract bam from a cram file into a storedir"""
	""" You can append a name to the files if you specify a name """
	inputfile = dictonnary[sample][ext]
	file_without_ext = os.path.splitext((os.path.basename(inputfile)))[0]
	# option to append name file with an aligner name (if file is sample.bam we need to rename it to sample.aligner.bam for the pipeline)
	if name:
		outputfile = storedir + "/" + file_without_ext + name
		aligner_list.append(name)
	else:
		outputfile = storedir + "/" + file_without_ext
	for aligner in aligner_list:
		if aligner in inputfile:
			if process_cmd == 'samtools':
				shell("samtools view -b -T "+refgene+" -o "+outputfile+".bam "+inputfile)	# samtools view -b -T ref_sequence.fa -o sample.bam sample.cram
			if process_cmd == 'cp':
				copy2(inputfile, outputfile+ext)
			if process_cmd == 'ls':
				shell("ln -sfn "+inputfile+" "+outputfile+ext)

def logsomefile(logfile, text, sep, items_list=None, items=None):
	""" Function to log variable value or list values into a log file """
	with open(logfile, 'a+') as f:
		f.write(text + sep)
		if items_list:
			for items in items_list:
				f.write(str(items) + sep)
		else:
			f.write(str(items) + sep)

# structure of bam files are sample.aligner.(validation).bam or sample.archive.cram
# samples name are taken from bam files
# [x] for x part of the filename separated by '.'
# [0] is sample ; [-1] is extension ; [1] is aligner
def extractlistfromfiles(file_list, extension, sep, position):
	""" Function for creating list from a file list, with a specific extension, a separator and the position of the string we want to extract """
	output_list = []
	for files in file_list:
		filesname = os.path.basename(files)
		if filesname.endswith(extension):
			output_list.append(filesname.split(sep)[position])
	output_list = list(set(output_list))
	return output_list


def aligned_list(txtlistoutput, file_list, ext, pattern):
	""" Function to create a txt file from a list of files filtered by extension and pattern """
	with open(txtlistoutput, 'a+') as f:
		for files in file_list:
			if pattern in files and files.endswith(ext):
				f.write(str(files) + "\n")

### END OF FUNCTIONS ###

# Variables initialisation
# set datetime to add to output file name
date_time = datetime.now().strftime("%Y%m%d-%H%M%S")

# Return last directory of the path run (without /)
# ex == /STARK/output/repository/GROUP/APP/run
nameoftherun = os.path.basename(rundir)

# Set directories to save tempory results and input files
inputdir = "/app/res/" + nameoftherun + "/input"
tmpdir = "/app/res/" + nameoftherun + "/tmp"

# Get group [4] and app [5] name from run (or -1 and -2 ?)
# run structure is "/STARK/output/repository/group/app/run"
# Default is UNKNOW in the yaml file
try:
	group_name = rundir.split('/')[4]
	app_name = rundir.split('/')[5]
except IndexError: pass

# Construct repository and depository dir structure (outputdir will be the default output)
depotdir = depositorydir + "/" + group_name + "/" + app_name + "/" + nameoftherun + "/"

if not outputdir:
	outputdir = repositorydir + "/" + group_name + "/" + app_name + "/" + nameoftherun + "/"
 
# Set the path of the customnumberingfile for makeCNVcalls
customnumberingfile = tmpdir + "/" + serviceName + "." +  date_time + '.CustomNumbering.txt'

# Set log path file
logfile = tmpdir + "/" + serviceName + "." + date_time + '.parameters.log'

# Set annotation file path
annotation_file = tmpdir + "/" + serviceName + "." + date_time + '.AnnotSV.txt'

# Set Decon Bed file
deconbed_file = tmpdir + "/" + serviceName + "." + date_time + '.bed'

# Create directories
os.makedirs(inputdir, exist_ok = True)
os.makedirs(tmpdir, exist_ok = True)
os.makedirs(outputdir, exist_ok = True)
if depotcopy == True:
	os.makedirs(depotdir, exist_ok = True)

# Search files in the rundir directory
files_list = searchfiles(rundir, search_argument, recursive_search)

# Create sample and aligner list
sample_list = extractlistfromfiles(files_list, process_file, '.', 0)
aligner_list = extractlistfromfiles(files_list, process_file, '.', 1)

# Exclude samples from the exclude_list
for sample_exclude in sample_exclude_list:
	for sample in sample_list:
		if sample.startswith(sample_exclude):
			sample_list.remove(sample)

# If filter_sample_list variable is not empty, it will force the sample list
if filter_sample_list:
	sample_list = list(filter_sample_list)

# For validation analyse bam will be sample.aligner.validation.bam
if validation_only == True:
	append_aligner = '.validation'
	aligner_list = [sub + append_aligner for sub in aligner_list]

# Init dictionary
dico_run = {}
for samples in sample_list:
	dico_run[samples] = {}
# Populating dictionary
for samples in sample_list:
	for ext in ext_list:
		for files in files_list:
			if os.path.basename(files).startswith(samples) and os.path.basename(files).endswith(ext):
				if validation_only == False:
					if ext == process_file and not 'validation' in files:
						dico_run[samples][ext] = files
					if ext != process_file and not 'validation' in files:
						dico_run[samples][ext] = files
				if validation_only == True:
					if ext == process_file and 'validation' in files:
						dico_run[samples][ext] = files
					if ext != process_file and not 'validation' in files:
						dico_run[samples][ext] = files

# Separate cram/bam processing
# for cram extract to bam into the input directory
# for bam copy or symlink the bam files to input directory
for sample in sample_list:
	processalignedfiles(inputdir, dico_run, sample, process_file, aligner_list, process_cmd, name=aligner_name)

# Create the file_list by searching the depot directory
files_list_depot = searchfiles(inputdir, '/*', False)

# log
logsomefile(logfile, 'Input file:', "\n", items_list = files_list_depot)

# Creating a txt list for the bam files per aligner
for aligner in aligner_list:
	bamlist = tmpdir + "/" + serviceName + "." + date_time + "." + aligner + '.list.txt'
	aligned_list(bamlist, files_list_depot, process_file, aligner)

# Find bed file (Design)
if not bed_file:
	bed_file = finditemindico(sample_list, ext_list, dico_run, '.design.bed', '.genes.bed')

# Find genes file (Panel)
if not genes_file:
	genes_file = finditemindico(sample_list, ext_list, dico_run, '.genes', '.list.genes')

# Find transcripts files (NM)
if not transcripts_file:
	transcripts_file = finditemindico(sample_list, ext_list, dico_run, '.transcripts', '.list.transcripts')

# If transcript file exist, create the annotation file for AnnotSV
if os.path.exists(transcripts_file) and os.path.getsize(transcripts_file) != 0:
	df = pd.read_csv(transcripts_file, sep='\t', names=["NM", "Gene"])
	df = df.drop(['Gene'], 1)
	NM_list = df.loc[:,'NM'].tolist()
	with open(annotation_file, 'w+') as f:
		f.write('\t'.join(NM_list))
else:
	with open(annotation_file, 'w') as f:
		f.write("No NM found")

# Find list.genes files 
if not list_genes:
	list_genes = finditemindico(sample_list, ext_list, dico_run, '.list.genes', '.list.transcripts')

# Transform list_genes into a list if list_genes exist, else use genes_file if exist
panel_list = []
panel_list_trunc = []
if list_genes:
	with open(list_genes) as f:
		panel_list = f.read().splitlines()
elif genes_file and not list_genes:
	panel_list.append(os.path.basename(genes_file).split(".bed",1)[0]) # we split the .bed ext because list don't have .bed but genes_file does
# cp files from panel_list to inputdir and rename them
if panel_list:
	for panel in panel_list:
		inputfile = os.path.dirname(list_genes) + "/" + panel + ".bed" # panel_list don't have the bed extension, need that for the copy
		# cut sample. and .genes from file name so files will have the same name as the truncated name list
		panel_trunc = panel.split(".", 1)[1].split(".genes",1)[0]
		outputfile = tmpdir + "/" + panel_trunc
		copy2(inputfile, outputfile)
		# Create a new list for expand, names are filenames without sample and .genes.bed ext
		panel_list_trunc.append(panel_trunc)

# Log
logsomefile(logfile, 'Start of the analysis:', "\n", items = date_time)
logsomefile(logfile, 'List of samples:', "\n", items_list = sample_list)
logsomefile(logfile, 'Aligner list:', "\n", items_list = aligner_list)
logsomefile(logfile, 'Analyse run:', "\n", items = nameoftherun)
logsomefile(logfile, 'Design Bed:', "\n", items = bed_file)
logsomefile(logfile, 'Panel Bed:', "\n", items = genes_file)
logsomefile(logfile, 'Transcripts list:', "\n", items = transcripts_file)
logsomefile(logfile, 'List of genes files:', "\n", items = list_genes)

# Copy2 bed_file & genes_file & transcripts_file (for debug)
if debug:
	try:
		copy2(bed_file, inputdir)
		copy2(genes_file, inputdir)
		copy2(transcripts_file, inputdir)
	except FileNotFoundError: pass

################################################## RULES ##################################################
# The input for the rule all is the output of the pipeline
# The expand for the {sample} {aligner} is done only at this step
# Final results will follow this structure in the output directory 
# /nameoftherun/SAMPLENAME/ServiceName (ie SCRAMBLE)/ individual results
# /nameoftherun/global results & global logs with ServiceName index
#
# rule all will different depending on the bed files available
# design = .bed, vcf will be vcf.design ; panel = genes, vcf will be vcf.panel ; not bed or genes = vcf will be vcf.full
# warning : bcftools merge will crash if there's only 1 sample
############################################################################################################

##### NOTE ####
# Tools that output a file adding a extension should be set with params: params.output as the output of the command without the extension, and output: output.ext exist only to connect rules
# For programs that do not have an explicit log parameter, you may always use 2> {log} to redirect standard output to a file (here, the log file) in Linux-based systems. Note that it is also supported to have multiple (named) log files being specified
# 1 is stdout, 2 is stderr 

# check the number of sample for copy or merge vcf rule
sample_count = len(sample_list) 

# add more input with expand to display rules help
if bed_file and genes_file:
	rule all:
		"""
		Rule will create a design and panel vcf.gz if a gene list file is provided, else a design vcf.gz only
		"""
		input:
			vcfgzpanel = expand(tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Panel.{panel}.vcf.gz", sample=sample_list, aligner=aligner_list, panel=panel_list_trunc),
			vcfgzallpanel = expand(tmpdir + "/" + serviceName + "." + date_time + ".all.Panel.{panel}.vcf.gz",  panel=panel_list_trunc),
			cfgzalldesign = tmpdir + "/" + serviceName + "." + date_time + ".all.Design.vcf.gz",
			vcfgzallfull = tmpdir + "/" + serviceName + "." + date_time + ".all.Full.vcf.gz"

if bed_file and not genes_file:
	rule all:
		"""
		Rule will create a design and panel vcf.gz if a gene list file is provided, else a design vcf.gz only
		"""
		input:
			vcfgzdesign = expand(tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf.gz", sample=sample_list, aligner=aligner_list),
			vcfgzalldesign = tmpdir + "/" + serviceName + "." + date_time + ".all.Design.vcf.gz",
			vcfgzallfull = tmpdir + "/" + serviceName + "." + date_time + ".all.Full.vcf.gz"

if not bed_file and not genes_file:
	rule all:
		"""
		Rule will create a design and panel vcf.gz if a gene list file is provided, else a design vcf.gz only
		"""
		input:
			vcfgzdesign = expand(tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Full.vcf.gz", sample=sample_list, aligner=aligner_list),
			vcfgzallfull = tmpdir + "/" + serviceName + "." + date_time + ".all.Full.vcf.gz"

rule help:
	"""
	General help for SCRAMBLE module
	Launch snakemake -s  snakefile_scramble -c(numberofthreads) --config DATA_DIR=absolutepathoftherundirectory (default is data) without / at the end of the path
	To launch the snakemake file, use --config to replace variables that must be properly set for the pipeline to work ie run path directory
	Every variable defined in the yaml file can be change
	Separate multiple variable with a space (ex  --config DATA_DIR=runname transProb=0.05 var1=0.05 var2=12)
	Also use option --configfile another.yaml to replace and merge existing config.yaml file variables
	Use -p to display shell commands
	Use --lt to display docstrings of rules
	Input file = bam files (if bai is needed, it will be generate)
	Output file = vcf annoted with AnnotSV 3.x for each sample/bam, and a global vcf file will all the samples ; a set of vcf files by design/panel
	"""

rule indexing:
	"""
	Indexing bam files with samtools
	"""
	input:
		bam = inputdir + "/{sample}.{aligner}.bam"
	output:
		bai = inputdir + "/{sample}.{aligner}.bai"
	params:
		threads = config['THREADS']
	shell:
		"""
		samtools index -b -@ {params.threads} {input.bam} {output.bai}
		"""

rule cluster_identifier:
	"""
	Cluster identifier will identify soft clipped clusters.
	The output is a tab delimited text file with clipped cluster consensus sequences. The columns are as follows:
	1. Coordinate
	2. Side of read where soft-clipped occurred
	3. Clipped read consensus
	4. Anchored read consensus
	Requirement : .bam file must have a .bai file associated in the same folder
	"""
	input:
		bam = inputdir + "/{sample}.{aligner}.bam",
		bai = inputdir + "/{sample}.{aligner}.bai"
	output:
		txt = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.clusters.txt"
	params:
		mini = config['m'],
		soft = config['s'],
		region = config['r']
	shell:
		"""
		cluster_identifier -m {params.mini} -s {params.soft} -r {params.region} {input.bam} > {output.txt}
		"""

rule scramble:
	"""
	Calling SCRAMble.R with --eval-meis produces a tab delimited file. If a genomereference.fa file is provided, then a VCF is produced as well.
	The <out-name>_MEIs.txt output is a tab delimited text file with MEI calls. If no MEIs are present an output txt file will still be produced with only the header, and a dummy vcf will be output as well.
	"""
	input:
		txt = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.clusters.txt"
	params:
		scrambledir = config['SCRAMBLE_PATH'],
		refmei = config['REFMEI_PATH'],
		nCluster = config['nCluster'],
		meiscore = config['mei-score'],
		polyafrac = config['poly-a-frac'],
		polyadist = config['poly-a-dist'],
		output = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.uncorr"
	output:
		vcf = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.uncorr.vcf"
	log: log1 = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.ScrambleR.log", log2 = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.ScrambleR.err"
	shell:
		"""
		Rscript --vanilla {params.scrambledir}/SCRAMble.R \
		--cluster-file {input.txt} \
		--out-name {params.output} \
		--install-dir {params.scrambledir} \
		--mei-refs {params.refmei} \
		--ref {refgene} \
		--mei-score {params.meiscore} --nCluster {params.nCluster} --poly-a-dist {params.polyadist} --poly-a-frac {params.polyafrac} --eval-meis 1> {log.log1} 2> {log.log2} && [[ -s {output.vcf} ]] || cat {scriptspath}/empty.vcf > {output.vcf} 
		"""

# escape " with \" for bash or awk or use triple """ to avoid the use of \ ; use double {{ and }} for awk integration to differentiate snakefile variable and awk variable
rule correctvcf:
	"""
	Correction of vcf output, add sample name and genotype to be consistent with the vcf format specification.
	"""
	input:
		vcf = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.uncorr.vcf"
	output:
		vcf = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.vcf"
	shell:
		"""
		(grep "^##" {input.vcf} && echo '##FORMAT=<ID=GT,Number=1,Type=String,Description="Genotype">' && grep "^#CHROM" {input.vcf} | awk -v SAMPLE={wildcards.sample} '{{print $0"\tFORMAT\t"SAMPLE}}' && grep "^#" -v {input.vcf} | awk '{{print $0"\tGT\t0/1"}}') > {output.vcf} && [[ -s {output.vcf} ]] || cat {scriptspath}/empty.vcf > {output.vcf}
		"""

# AnnotSV need bedtools to work ie you have to specify the directory of bedtools binaries in -bedtools argument
rule AnnotSV:
	"""
	AnnotSV will annotate and rank Structural Variations (SV) from a vcf file. Output will be an AnnotSV tsv file.
	-annotationMode can be : split by exons/introns or full by genes
	-txtFile : path to a file containing a list of preferred genes transcripts to be used in priority during the annotation, preferred genes transcripts names should be tab or space separated
	-genomeBuild must be specified if not hg19
	"""
	input:
		vcf = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.vcf"
	output:
		tsvuncorr = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.AnnotSV_uncorr.tsv"
	log:
		txt = tmpdir + "/{sample}/" + serviceName + "/log/" + serviceName + "." + date_time + ".{sample}.{aligner}.AnnotSV.log"
	params:
		bedtools = config['BEDTOOLS_PATH'],
		bcftools = config['BCFTOOLS_PATH'],
		genome = config['genomeBuild'],
		overlap = config ['overlap'],
		mode = config['annotationMode']
	shell:
		"""
		AnnotSV -SVinputFile {input.vcf} -outputFile {output.tsvuncorr} -bedtools {params.bedtools} -annotationMode {params.mode} -txFile {annotation_file} -genomeBuild {params.genome} -overlap {params.overlap} > {log.txt} && [[ -s {output.tsvuncorr} ]] || cat {scriptspath}/emptyAnnotSV.tsv | sed 's/SAMPLENAME/{wildcards.sample}/g' > {output.tsvuncorr}
		"""

# AnnotSV drop chr from input 
# we need to re-add chr to the SV_chrom column (column number 2)
rule patchchr:
	input:
		tsvuncorr = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.AnnotSV_uncorr.tsv"
	output:
		tsv = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.AnnotSV.tsv"
	shell:
		"""
		awk '{{if(NR==1){{print; next}}; $2="chr"$2; print}}' {input.tsvuncorr} > {output.tsv}
		"""

rule AnnotSV2vcf:
	input:
		tsv = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.AnnotSV.tsv"
	output:
		vcfunsort = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Fullunsort.vcf"
	log:
		log2 = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.AnnotSV2vcf.log"
	shell:
		"""
		python3.9 {scriptspath}/file_converter.py  -i {input.tsv} -o {output.vcfunsort} -fi annotsv -fo vcf -c {scriptspath}/fileconversion/config_annotsv3.json 2> {log.log2} && [[ -s {output.vcfunsort} ]] || cat {scriptspath}/emptyAnnotSV.vcf | sed 's/NAMEOFTHEINPUTFILE/{input.tsv}/g' | sed 's/DATE/${{date +%d/%m/%Y}}/g' | sed 's /REFNAME/{refgene}/g' | sed 's/SAMPLENAME/{wildcards.sample}/g' > {output.vcfunsort}
		"""


# bcftools sort don't sort correctly
rule sortvcf:
	input:
		vcfunsort = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Fullunsort.vcf"
	output:
		vcfsort = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Full.vcf"
	shell:
		"""
		grep \"^#\" {input.vcfunsort} > {output.vcfsort} && grep -v \"^#\" {input.vcfunsort} | sort -k1,1V -k2,2g >> {output.vcfsort} && [[ -s {output.vcfsort} ]] || cat {scriptspath}/empty.vcf > {output.vcfsort}
		"""

# bcftools need vcf.gz.tbi index for the merge all
rule vcf2gz:
	input:
		vcfsort = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Full.vcf"
	output:
		vcfgz = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Full.vcf.gz"
	shell:
		"""
		bgzip -c {input.vcfsort} > {output.vcfgz} ; tabix {output.vcfgz}
		"""

rule cpvcffull:
	input:
		vcfgz = expand(tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Full.vcf.gz", aligner=aligner_list, sample=sample_list)
	output:
		vcfgzallfull = tmpdir + "/" + serviceName + "." + date_time + ".all.Full.vcf.gz"
	shell:
		"""
		if [ {sample_count} -eq 1 ]
		then
			cp {input.vcfgz} {output.vcfgzallfull} && tabix {output.vcfgzallfull}
		else
			bcftools merge {input.vcfgz} -O z -o {output.vcfgzallfull} && tabix {output.vcfgzallfull}
		fi
		"""

# bcftools need vcf.gz.tbi index
rule filtervcfdesign:
	input:
		vcfgz = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Full.vcf.gz"
	output:
		vcfgzdesign = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf.gz"
	params:
		bed = bed_file
	shell:
		"""
		bcftools view {input.vcfgz} -R {params.bed} -O z -o {output.vcfgzdesign} && tabix {output.vcfgzdesign}
		"""

rule cpvcfdesign:
	input:
		vcfgzdesign = expand(tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf.gz", sample=sample_list, aligner=aligner_list)
	output:
		vcfgzalldesign = tmpdir + "/" + serviceName + "." + date_time + ".all.Design.vcf.gz"
	shell:
		"""
		if [ {sample_count} -eq 1 ]
		then
			cp {input.vcfgzdesign} {output.vcfgzalldesign} && tabix {output.vcfgzalldesign}
		else
			bcftools merge {input.vcfgzdesign} -O z -o {output.vcfgzalldesign} && tabix {output.vcfgzalldesign}
		fi
		"""

rule filtervcfpanel:
	input:
		vcfgzdesign = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf.gz"
	output:
		vcfgzpanel = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Panel.{panel}.vcf.gz"
	params:
		genes = expand(tmpdir + "/{panel}", panel=panel_list_trunc)
	shell:
		"""
		bcftools view {input.vcfgzdesign} -R {params.genes} -O z -o {output.vcfgzpanel} && tabix {output.vcfgzpanel}
		"""

rule cpvcfpanel:
	input:
		vcfgzpanel = expand(tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Panel.{panel}.vcf.gz", sample=sample_list, aligner=aligner_list, panel=panel_list_trunc)
	output:
		vcfgzallpanel = expand(tmpdir + "/" + serviceName + "." + date_time + ".all.Panel.{panel}.vcf.gz", panel=panel_list_trunc)
	shell:
		"""
		if [ {sample_count} -eq 1 ]
		then
			cp {input.vcfgzpanel} {output.vcfgzallpanel} && tabix {output.vcfgzallpanel}
		else
			bcftools merge {input.vcfgzpanel} -O z -o {output.vcfgzallpanel} && tabix {output.vcfgzallpanel}
		fi
		"""

onstart:
	shell("touch " + outputdir + "/" + serviceName + "Running.txt")
	# Add the snakemake parameters to log
	with open(logfile, "a+") as f:
		json.dump(config, f)
		f.write("\n")

onsuccess:
	shell("touch " + outputdir + "/" + serviceName + "Complete.txt")
	shell("rm " + outputdir + "/" + serviceName + "Running.txt")
	# Add end time of the analysis to the log file
	date_time_end = datetime.now().strftime("%Y%m%d-%H%M%S")
	with open(logfile, "a+") as f:
		f.write("End of the analysis : ")
		f.write((date_time_end) + "\n")
	
	# Copy files to final destination
	shell("rsync -azvh --include={include_file} --exclude {exclude_file} {tmpdir}/ {outputdir}")
	if depotcopy == True:
		shell("rsync -azvh --include={include_file} --exclude {exclude_file} {tmpdir}/ {depotdir}")

onerror:
	shell("touch " + outputdir + "/" + serviceName + "Failed.txt")
	shell("rm " + outputdir + "/" + serviceName + "Running.txt")
	shell("rsync -azvh --include={include_file_log} --exclude {exclude_file} {tmpdir}/ {outputdir}")