##########################################################################
# Snakemakefile Version:   3
# Description:             Snakemake file to run SCRAMBLE module
##########################################################################

# DEV version 0.1 : 10/11/2021
# INT version 0.1 : 17/03/2022
# PROD version 1 : 03/06/2022
# Authoring : Thomas LAVAUX


# PROD version 2 : 14/10/2022 changelog
	# remove panel vcf filtering (output was essentially empty) ; rename full to unfiltered
	# exclude samples list case insensitive
	# copying new analysis per sample in the root sample dir & removing the old ones
	# add vcf2tsv converter (https://github.com/sigven/vcf2tsvpy) & corresponding rules : each vcf will be convert to tsv
	# correct run path by removing ending '/' if exist
	# keep "Running.txt" file if failed, avoiding multiple analysis launch by the listener

# PROD version 3 : 19/09/2023 changelog
	# AnnotSV version 3.3.7 (include the vcf converter)
	# Add the deletion mode
	# refactor snakemake code a lot

################## Context ##################
# launch snakemake -s  snakefile_scramble -c(numberofthreads) --config run=absolutepathoftherundirectory without / at the end of the path
# to launch the snakemake file, use --config to replace variables that must be properly set for the pipeline to work ie run path directory
# every variable defined in the yaml file can be change
# separate multiple variable with a space (ex  --config run=runname var1=0.05 var2=12)
# also use option --configfile another.yaml to replace and merge existing config.yaml file variables

# use -p to display shell commands
# use --lt to display docstrings of rules

# input file = bam files (if bai is needed, it will be generate)
# output file = vcf annoted with AnnotSV 3.x for each sample/bam, and a global vcf file will all the samples ; a set of files by design/panel
# warning : bcftools merge will crash if there's only 1 sample, so we need to cp files
################## Import libraries ##################

import os
import os.path
import glob
import pandas as pd
import json

from datetime import datetime
from shutil import copy2


################## Configuration file ##################
configfile: "/app/config/snakefile/default.yaml"

####################### FUNCTIONS #####################

def parse_samplesheet(samplesheet_path):
	"""
	samplesheet_path: absolute path of a samplesheet file, Illumina format
	return: a dataframe containing 9 columns :
	Sample_ID,Sample_Plate,Sample_Well,I7_Index_ID,index,Manifest,GenomeFolder,Sample_Project,Description
	The description field contains tag separated by ! ; the name of the tag and the value is separated by # (ex : SEX#F!APP#DIAG.BBS_RP)
	"""
	samplesheet_data = []
	samplesheet_header = []
	with open(samplesheet_path, 'r') as f:
		v = False
		for lines in f:
			lines = lines.strip()
			if v:
				samplesheet_data.append(lines.split(','))
			if 'Sample_ID' in lines:
				v = True
				samplesheet_header.append(lines.split(','))
	df = pd.DataFrame(samplesheet_data, columns=samplesheet_header)
	sample_list = df.iloc[:, 0].tolist()
	return df

def getSampleInfos(samplesheet_path, dictionary):
	"""
	samplesheet_path: absolute path of a samplesheet file, Illumina format
	return a dictionnary with Sample_ID from samplesheet as key and 'gender': 'F or M or NULL'
	"""
	dictionary = {}
	if samplesheet_path:
		parse_samplesheet(samplesheet_path)
		for i, rows in parse_samplesheet(samplesheet_path).iterrows():
			if not any(exclude in rows['Sample_ID'] for exclude in config['EXCLUDE_SAMPLE']):
				sampleID = rows["Sample_ID"]
				dictionary[sampleID] = {}
				tags = rows['Description'].split('!')
				for tag in tags:
					if 'SEX' in tag and '_' in tag:
						tag = tag.split('_')[-1]
						dictionary[sampleID]['gender'] = tag
						break
					elif 'SEX' in tag and '#' in tag:
						tag = tag.split('#')[-1]
						dictionary[sampleID]['gender'] = tag
						break
					else:
						tag = ''
						dictionary[sampleID]['gender'] = tag
	return dictionary
	
def finditemindico(sample_list, ext_list, dictionary, include_ext, exclude_ext):
	""" Function to search in a dictionary a file path itering by the first key (sample_list) and second key (extension_list) with an including and excluding filter """
	""" Result must be an non empty file """
	searchresult = ""
	for sample in sample_list:
		for ext in ext_list:
			try:
				items = dictionary[sample][ext]
				if include_ext in items and not exclude_ext in items:
					if os.path.exists(items) and os.path.getsize(items) != 0:
						searchresult = items
			except KeyError: pass
	return searchresult

def searchfiles(directory, search_arg, recursive_arg):
	""" Function to search all files in a directory, adding a search arguement append to the directory and a recursive_search options (True/False) """
	return sorted(filter(os.path.isfile, glob.glob(directory + search_arg, recursive=recursive_arg)))

def logsomefile(logfile, text, sep, items_list=None, items=None):
	""" Function to log a variable value or a list of values into a log file """
	with open(logfile, 'a+') as f:
		f.write(text + sep)
		if items_list:
			for items in items_list:
				if items == '':
					f.write(str('None') + sep)
				else:
					f.write(str(items) + sep)
		else:
			if items == '':
				f.write(str('None') + sep)
			else:
				f.write(str(items) + sep)

def extractlistfromfiles(file_list, ext_list, sep, position):
	""" Function for creating list from a file list, with a specific extension, a separator and the position of the string we want to extract """
	output_list = []
	for files in file_list:
		filesname = os.path.basename(files)
		for ext in ext_list:
			if filesname.endswith(ext):
				output_list.append(filesname.split(sep)[position])
	output_list = list(set(output_list))
	return output_list

def create_list(txtlistoutput, file_list, ext_list, pattern):
	""" Function to create a txt file from a list of files filtered by extension and pattern """
	with open(txtlistoutput, 'a+') as f:
		for files in file_list:
			for ext in ext_list:
				if pattern in files and files.endswith(ext):
					f.write(str(files) + "\n")

def extract_tag(tagfile, tag, tagsep, sep):
	""" Function to extract a tag """
	output_tag = ""
	row = open(tagfile, 'r').readline().strip().split(tagsep)
	for items in row:
		if tag in items and sep in items:
			output_tag = items.split(sep)[-1]
	return(output_tag)

### END OF FUNCTIONS ###
serviceName = config['serviceName']
date_time = datetime.now().strftime("%Y%m%d-%H%M%S")
runName = os.path.basename(os.path.normpath(config['run']))
inputDir = "/app/res/" + runName + "/" + date_time + "/input"
tmpDir = "/app/res/" + runName + "/" + date_time + "/tmp"
try:
	config['GROUP_NAME'] = os.path.normpath(config['run']).split('/')[4]
	config['APP_NAME'] = os.path.normpath(config['run']).split('/')[5]
except IndexError: pass
depotdir = config['depository'] + "/" + config['GROUP_NAME'] + "/" + config['APP_NAME'] + "/" + runName + "/"
if not config['OUTPUT_DIR']:
	config['OUTPUT_DIR'] = config['run']
logfile = tmpDir + "/" + serviceName + "." + date_time + '.parameters.log'
annotation_file = tmpDir + "/" + serviceName + "." + date_time + '.AnnotSV.txt'

os.makedirs(inputDir, exist_ok = True)
os.makedirs(tmpDir, exist_ok = True)
os.makedirs(config['OUTPUT_DIR'], exist_ok = True)
if config['DEPOT_COPY'] == True:
	os.makedirs(depotdir, exist_ok = True)

# Search files in the rundir directory
files_list = searchfiles(os.path.normpath(config['run']), config['SEARCH_ARGUMENT'], config['RECURSIVE_SEARCH'])
# Create sample and aligner list
sample_list = extractlistfromfiles(files_list, config['PROCESS_FILE'], '.', 0)
aligner_list = extractlistfromfiles(files_list, config['PROCESS_FILE'], '.', 1)

# Exclude samples from the exclude_list case insensitive
for sample_exclude in config['EXCLUDE_SAMPLE']:
	for sample in sample_list:
		if sample.upper().startswith(sample_exclude.upper()):
			sample_list.remove(sample)

# If config['FILTER_SAMPLE'] variable is not empty, it will force the sample list
if config['FILTER_SAMPLE']:
	sample_list = list(config['FILTER_SAMPLE'])

# For validation analyse bam will be sample.aligner.validation.bam
if config['VALIDATION_ONLY'] == True:
	append_aligner = '.validation'
	aligner_list = [sub + append_aligner for sub in aligner_list]

runDict = {}
for samples in sample_list:
	runDict[samples] = {}
for samples in sample_list:
	for ext in config['EXT_INDEX_LIST']:
		for files in files_list:
			if os.path.basename(files).split(".")[0] == samples and os.path.basename(files).endswith(ext):
				if config['VALIDATION_ONLY'] == False:
					for filext in config['PROCESS_FILE']:
						if ext == filext and not 'validation' in files:
							runDict[samples][ext] = files
						if ext != config['PROCESS_FILE']:
							runDict[samples][ext] = files
				if config['VALIDATION_ONLY'] == True:
					for filext in config['PROCESS_FILE']:
						if ext == filext and 'validation' in files:
							runDict[samples][ext] = files
						if ext != config['PROCESS_FILE']:
							runDict[samples][ext] = files

# Find bed file (Design)
if not config['BED_FILE']:
	config['BED_FILE'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], runDict, '.design.bed', '.genes.bed')
# Find genes file (Panel)
if not config['GENES_FILE']:
	config['GENES_FILE'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], runDict, '.genes', '.list.genes')
# Find transcripts files (NM)
if not config['TRANSCRIPTS_FILE']:
	config['TRANSCRIPTS_FILE'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], runDict, '.transcripts', '.list.transcripts')

# If transcript file exist, create the annotation file for AnnotSV
if os.path.exists(config['TRANSCRIPTS_FILE']) and os.path.getsize(config['TRANSCRIPTS_FILE']) != 0:
	df = pd.read_csv(config['TRANSCRIPTS_FILE'], sep='\t', names=["NM", "Gene"])
	df = df.drop(columns=['Gene'])
	NM_list = df.loc[:,'NM'].tolist()
	with open(annotation_file, 'w+') as f:
		f.write('\t'.join(NM_list))
else:
	with open(annotation_file, 'w') as f:
		f.write("No NM found")

# Find list.genes files 
if not config['LIST_GENES']:
	config['LIST_GENES'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], runDict, '.list.genes', '.list.transcripts')

# Transform list_genes into a list if list_genes exist, else use genes_file if exist
panel_list = []
panel_list_trunc = []
if config['LIST_GENES']:
	with open(config['LIST_GENES']) as f:
		panel_list = f.read().splitlines()
elif config['GENES_FILE'] and not config['LIST_GENES']:
	panel_list.append(os.path.basename(config['GENES_FILE']).split(".bed",1)[0]) # we split the .bed ext because list don't have .bed but genes_file does
# cp files from panel_list to inputDir and rename them
if panel_list:
	for panel in panel_list:
		inputfile = os.path.dirname(config['LIST_GENES']) + "/" + panel + ".bed" # panel_list don't have the bed extension, need that for the copy
		# cut sample. and .genes from file name so files will have the same name as the truncated name list
		panel_trunc = panel.split(".", 1)[1].split(".genes",1)[0]
		outputfile = tmpDir + "/" + panel_trunc
		copy2(inputfile, outputfile)
		# Create a new list for expand, names are filenames without sample and .genes.bed ext
		panel_list_trunc.append(panel_trunc)

logsomefile(logfile, 'Start of the analysis:', "\n", items = date_time)
logsomefile(logfile, 'List of samples:', "\n", items_list = sample_list)
logsomefile(logfile, 'Analyse run:', "\n", items = runName)
if aligner_list:
	logsomefile(logfile, 'Aligner list:', "\n", items_list = aligner_list)
if config['BED_FILE']:
	logsomefile(logfile, 'Design Bed:', "\n", items = config['BED_FILE'])
if config['GENES_FILE']:
	logsomefile(logfile, 'Panel Bed:', "\n", items = config['GENES_FILE'])
if config['TRANSCRIPTS_FILE']:
	logsomefile(logfile, 'Transcripts list:', "\n", items = config['TRANSCRIPTS_FILE'])
if config['LIST_GENES']:
	logsomefile(logfile, 'List of genes files:', "\n", items = config['LIST_GENES'])

# Copy2 bed_file & genes_file & transcripts_file for debug
if config['DEBUG_MODE']:
	try:
		copy2(config['BED_FILE'], inputDir)
		copy2(config['GENES_FILE'], inputDir)
		copy2(config['TRANSCRIPTS_FILE'], inputDir)
	except FileNotFoundError: pass

################################################## RULES ##################################################
# rule all will be different depending on the bed files available
# design = .bed, vcf will be vcf.design ; panel = genes, vcf will be vcf.panel ; not bed or genes = vcf will be vcf.unfiltered
############################################################################################################

# check the number of sample for copy or merge vcf rule
sample_count = len(sample_list) 

ruleorder: copy_bam > cramtobam


if not config['BED_FILE']:
	rule all:
		"""
		Rule will create an unfiltered vcf.gz and corresponding tsv
		"""
		input:
			expand(tmpDir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.AnnotSV.Full.vcf.gz", sample=sample_list, aligner=aligner_list),
			tmpDir + "/" + serviceName + "." + date_time + ".all.AnnotSV.Full.tsv"

else:
	rule all:
		"""
		Rule will create an unfiltered vcf.gz & design vcf.gz and corresponding tsv
		"""
		input:
			expand(tmpDir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}AnnotSV.Full.vcf.gz", sample=sample_list, aligner=aligner_list),
			expand(tmpDir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.AnnotSV.Design.tsv", sample=sample_list, aligner=aligner_list),
			tmpDir + "/" + serviceName + "." + date_time + ".all.AnnotSV.Full.tsv",
			tmpDir + "/" + serviceName + "." + date_time + ".all.AnnotSV.Design.tsv"

rule help:
	"""
	General help for SCRAMBLE module
	Launch snakemake -s  snakefile_scramble -c(numberofthreads) --config DATA_DIR=absolutepathoftherundirectory (default is data) without / at the end of the path
	To launch the snakemake file, use --config to replace variables that must be properly set for the pipeline to work ie run path directory
	Every variable defined in the yaml file can be change
	Separate multiple variable with a space (ex  --config DATA_DIR=runname transProb=0.05 var1=0.05 var2=12)
	Also use option --configfile another.yaml to replace and merge existing config.yaml file variables
	Use -p to display shell commands
	Use --lt to display docstrings of rules
	Input file = bam files (if bai is needed, it will be generate)
	Output file = vcf annoted with AnnotSV 3.x for each sample/bam, and a global vcf file will all the samples ; a set of vcf files by design/panel
	"""

rule copy_bam:
	"""
	Copy input files
	"""
	output:
		temp(inputDir+"/{sample}.{aligner}.bam")
	params:
		process = config['PROCESS_CMD'],
		download_link = lambda wildcards: runDict[wildcards.sample]['.bam']
	shell:
		"""
		if [ "{params.process}" = "ls" ];
		then
		ln -sfn {params.download_link} {output}
		else
		rsync -azvh {params.download_link} {output}
		fi
		"""


rule copy_cram:
	"""
	Copy input files
	"""
	output:
		temp(inputDir+"/{sample}.{aligner}.cram")
	params:
		process = config['PROCESS_CMD'],
		download_link = lambda wildcards: runDict[wildcards.sample]['.cram']
	shell:
		"""
		if [ "{params.process}" = "ls" ];
		then
		ln -sfn {params.download_link} {output}
		else
		rsync -azvh {params.download_link} {output}
		fi
		"""

rule cramtobam:
	"""
	Extract bam from a cram file
	"""
	input:
		rules.copy_cram.output
	output:
		temp(inputDir+"/{sample}.{aligner}.bam")
	params:
		refgenome = config['REFGENEFA_PATH']
	shell:
		"""
		samtools view -b -T {params.refgenome} -o {output} {input}
		"""

rule indexing:
	"""
	Indexing bam files with samtools
	"""
	input:
		inputDir+"/{sample}.{aligner}.bam"
	output:
		temp(inputDir+"/{sample}.{aligner}.bai")
	params:
		threads = config['THREADS']
	shell:
		"""
		samtools index -b -@ {params.threads} {input} {output}
		"""

rule cluster_identifier:
	"""
	Cluster identifier will identify soft clipped clusters.
	The output is a tab delimited text file with clipped cluster consensus sequences. The columns are as follows:
	1. Coordinate
	2. Side of read where soft-clipped occurred
	3. Clipped read consensus
	4. Anchored read consensus
	Requirement : .bam file must have a .bai file associated in the same folder
	"""
	input:
		bam = inputDir+"/{sample}.{aligner}.bam",
		bai = inputDir+"/{sample}.{aligner}.bai"
	output:
		temp(tmpDir+"/{sample}/"+ serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.clusters.txt")
	params:
		mini = config['m'],
		soft = config['s'],
		region = config['r']
	shell:
		"""
		cluster_identifier -m {params.mini} -s {params.soft} -r {params.region} {input.bam} > {output}
		"""

# Indels option : you will need a reference genome indexed with the following command for blast (indels mode) to work : makeblastdb -in refgene.fa -dbtype nucl
rule scramble:
	"""
	Calling SCRAMble.R with --eval-meis produces a tab delimited file. If a genomereference.fa file is provided, then a VCF is produced as well.
	The <out-name>_MEIs.txt output is a tab delimited text file with MEI calls. If no MEIs are present an output txt file will still be produced with only the header, and a dummy vcf will be output as well.
	Calling SCRAMble.R with --eval-dels produced a VCF and a tab delimted file. The <out-name>_PredictedDeletions.txt output is a tab delimited text file with deletion calls. If no deletions are present an output file will still be produced with only the header.
	"""
	input:
		rules.cluster_identifier.output
	params:
		scrambledir = config['SCRAMBLE_PATH'],
		refmei = config['REFMEI_PATH'],
		nCluster = config['nCluster'],
		meiscore = config['mei-score'],
		polyafrac = config['poly-a-frac'],
		polyadist = config['poly-a-dist'],
		mode = config['scramble_mode'],
		refgene = config['REFGENEFA_PATH'],
		dummypath = config['DUMMY_PATH'],
		output = tmpDir+"/{sample}/"+ serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.raw"
	output:
		tmpDir+"/{sample}/"+ serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.raw.vcf"
	log: 
		log1 = tmpDir+"/{sample}/"+ serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.ScrambleR.log", log2 = tmpDir+"/{sample}/"+ serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.ScrambleR.err"
	shell:
		"""
		Rscript --vanilla {params.scrambledir}/SCRAMble.R \
		--cluster-file {input} \
		--out-name {params.output} \
		--install-dir {params.scrambledir} \
		--mei-refs {params.refmei} \
		--ref {params.refgene} \
		--mei-score {params.meiscore} --nCluster {params.nCluster} --poly-a-dist {params.polyadist} --poly-a-frac {params.polyafrac} {params.mode} > {log.log1} 2> {log.log2} && [[ -s {output} ]] || cat {params.dummypath}/empty.vcf > {output}
		"""

rule correctvcf:
	"""
	Correction of vcf output, add sample name and genotype to be consistent with the vcf format specification.
	"""
	input:
		rules.scramble.output
	output:
		temp(tmpDir+"/{sample}/"+ serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.corr.vcf")
	params:
		mode = config['scramble_mode'],
		dummypath = config['DUMMY_PATH']
	shell:
		"""
		if [ "{params.mode}" = "--eval-dels" ];
		then
		(grep "^##" {input} && echo '##FORMAT=<ID=GT,Number=1,Type=String,Description="Genotype">' && grep "^#CHROM" {input} | awk -v SAMPLE={wildcards.sample} '{{print $0"\tFORMAT\t"SAMPLE}}' && grep "^#" -v {input} | awk '{{print $0"\tGT\t0/1"}}' | awk '{{FS=OFS="\t"}}; {{$4=substr($4, 0, 1); print}}' | awk '{{FS=OFS="\t"}} $3=="DEL" {{$5="<DEL>"}}1') > {output} && [[ -s {output} ]] || cat {params.dummypath}/empty.vcf > {output}
		else
		(grep "^##" {input} && echo '##FORMAT=<ID=GT,Number=1,Type=String,Description="Genotype">' && grep "^#CHROM" {input} | awk -v SAMPLE={wildcards.sample} '{{print $0"\tFORMAT\t"SAMPLE}}' && grep "^#" -v {input} | awk '{{print $0"\tGT\t0/1"}}') > {output} && [[ -s {output} ]] || cat {params.dummypath}/empty.vcf > {output}
		fi
		"""

rule AnnotSV:
	"""
	AnnotSV will annotate and rank Structural Variations (SV) from a vcf file. Output will be an AnnotSV vcf file.
	-annotationMode can be : split by exons/introns or full by genes
	-txtFile : path to a file containing a list of preferred genes transcripts to be used in priority during the annotation, preferred genes transcripts names should be tab or space separated
	-genomeBuild must be specified if not hg19
	"""
	input:
		rules.correctvcf.output
	output:
		tmpDir+"/{sample}/"+ serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.AnnotSV.Full_unsort.vcf"
	log:
		tmpDir+"/{sample}/"+ serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.AnnotSV.log"
	params:
		genome = config['genomeBuild'],
		overlap = config ['overlap'],
		mode = config['annotationMode'],
		annotation = config['annotationdir'],
		dummypath = config['DUMMY_PATH'],
		output = tmpDir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.AnnotSV.Full_unsort"
	shell:
		"""
		AnnotSV -SVinputFile {input} -outputFile {params.output} -annotationMode {params.mode} -annotationsDir {params.annotation} -txFile {annotation_file} -genomeBuild {params.genome} -overlap {params.overlap} -vcf 1 > {log} && [[ -s {output} ]] || cat {params.dummypath}/emptyAnnotSV.vcf | sed 's/SAMPLENAME/{wildcards.sample}/g' > {output}
		"""

rule sortvcf:
	"""
	Bash script to sort a vcf
	"""
	input:
		rules.AnnotSV.output
	output:
		temp(tmpDir+"/{sample}/"+ serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.AnnotSV.Full_unfilter.vcf")
	params:
		dummypath = config['DUMMY_PATH']
	shell:
		"""
		grep \"^#\" {input} > {output} && grep -v \"^#\" {input} | sort -k1,1V -k2,2g >> {output} && [[ -s {output} ]] || cat {params.dummypath}/empty.vcf | sed 's/SAMPLENAME/{wildcards.sample}/g' > {output}
		"""


rule bcftools_filter:
	"""
	Filter with bcftools
	"""
	input:
		rules.sortvcf.output	
	output:
		temp(tmpDir+"/{sample}/"+ serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.AnnotSV.Full.vcf")
	params:
		bcffilter = config['BCFTOOLS_FILTER']
	shell:
		"""
		bcftools view {params.bcffilter} {input} -o {output} 
		"""

# bcftools need vcf.gz.tbi index for the merge all
rule vcf2gz:
	"""
	Compress vcf with bgzip
	"""
	input:
		rules.bcftools_filter.output
	output:
		tmpDir+"/{sample}/"+ serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.AnnotSV.Full.vcf.gz"
	shell:
		"""
		bgzip -c {input} > {output} ; tabix {output}
		"""

rule cpvcffull:
	"""
	Copy or merge with bcftools several vcfs
	"""
	input:
		expand(tmpDir+"/{sample}/"+ serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.AnnotSV.Full.vcf.gz", aligner=aligner_list, sample=sample_list)
	output:
		tmpDir+"/"+serviceName+"."+date_time+".all.AnnotSV.Full.vcf.gz"
	shell:
		"""
		if [ {sample_count} -eq 1 ]
		then
			cp {input} {output} && tabix {output}
		else
			bcftools merge {input} -O z -o {output} && tabix {output}
		fi
		"""

# bcftools need vcf.gz.tbi index
rule filtervcfdesign:
	"""
	Filter vcf with a bed file
	"""
	input:
		rules.vcf2gz.output
	output:
		tmpDir+"/{sample}/"+ serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.AnnotSV.Design.vcf.gz"
	params:
		bed = config['BED_FILE']
	shell:
		"""
		bcftools view {input} -R {params.bed} -O z -o {output} && tabix {output}
		"""

rule cpvcfdesign:
	"""
	Copy or merge with bcftools several vcfs
	"""
	input:
		expand(tmpDir+"/{sample}/"+ serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.AnnotSV.Design.vcf.gz", sample=sample_list, aligner=aligner_list)
	output:
		tmpDir+"/"+serviceName+"."+date_time+".all.AnnotSV.Design.vcf.gz"
	shell:
		"""
		if [ {sample_count} -eq 1 ]
		then
			cp {input} {output} && tabix {output}
		else
			bcftools merge {input} -O z -o {output} && tabix {output}
		fi
		"""

rule convertvcf_sampledesign:
	"""
	Vcf 2 tsv conversion
	"""
	input:
		rules.filtervcfdesign.output
	output:
		tmpDir+"/{sample}/"+ serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.AnnotSV.Design.tsv"
	log:
		tmpDir+"/{sample}/"+ serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.vcf2tsv_converter.log"
	shell:
		"""
		vcf2tsvpy --keep_rejected_calls --input_vcf {input} --out_tsv {output} 2> {log}
		"""

rule convertvcf_alldesign:
	"""
	Vcf 2 tsv conversion
	"""
	input:
		rules.cpvcfdesign.output
	output:
		tmpDir+"/"+serviceName+"."+date_time+".all.AnnotSV.Design.tsv"
	log:
		tmpDir+"/"+serviceName+"."+date_time+".all.AnnotSV.Design.vcf2tsv_converter.log"
	shell:
		"""
		vcf2tsvpy --keep_rejected_calls --input_vcf {input} --out_tsv {output} 2> {log}
		"""

rule convertvcf_allunfiltered:
	"""
	Vcf 2 tsv conversion
	"""
	input:
		rules.cpvcffull.output
	output:
		tmpDir+"/"+serviceName+"."+date_time+".all.AnnotSV.Full.tsv"
	log:
		tmpDir+"/"+serviceName+"."+date_time+".all.AnnotSV.Full.vcf2tsv_converter.log"
	shell:
		"""
		vcf2tsvpy --keep_rejected_calls --input_vcf {input} --out_tsv {output} 2> {log}
		"""
onstart:
	shell("touch " + config['OUTPUT_DIR'] + serviceName + "Running.txt")
	# Add the snakemake parameters to log
	with open(logfile, "a+") as f:
		json.dump(config, f)
		f.write("\n")
		
onsuccess:
	include = config['INCLUDE_RSYNC']
		
	shell("touch "+config['OUTPUT_DIR']+serviceName+"Complete.txt")
	shell("rm -f "+config['OUTPUT_DIR']+serviceName+"Running.txt")
	
	date_time_end = datetime.now().strftime("%Y%m%d-%H%M%S")
	with open(logfile, "a+") as f:
		f.write("End of the analysis : ")
		f.write((date_time_end) + "\n")
	for sample in sample_list:
		shell("rm -f "+config['OUTPUT_DIR']+sample+"/"+serviceName+"/* || true") # if rm stderr, the true will avoid exiting snakemake
	shell("rsync -azvh --include={include} {tmpDir}/ "+config['OUTPUT_DIR']+"")
	for sample in sample_list:
		shell("cp "+config['OUTPUT_DIR']+sample+"/"+serviceName+"/"+sample+"_"+date_time+"_"+serviceName+"/* "+config['OUTPUT_DIR']+sample+"/"+serviceName+"/ || true")
	if config['DEPOT_COPY'] == True:
		shell("rsync -azvh --include={include} {tmpDir}/ {depotdir}")
		for sample in sample_list:
			shell("cp "+depotdir+sample+"/"+serviceName+"/"+sample+"_"+date_time+"_"+serviceName+"/* "+depotdir+sample+"/"+serviceName+"/ || true")

onerror:
	include_log = config['INCLUDE_LOG_RSYNC']
	
	shell("touch "+config['OUTPUT_DIR']+serviceName+"Failed.txt")
	#shell("rm -f "+config['OUTPUT_DIR']+serviceName+"Running.txt")
	shell("rsync -azvh --include={include_log} {tmpDir}/ "+config['OUTPUT_DIR']+"")