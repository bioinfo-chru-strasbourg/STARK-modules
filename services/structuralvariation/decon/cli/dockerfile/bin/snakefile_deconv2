##########################################################################
# Snakemakefile Version:   0.1
# Description:             Snakemake file to run DECoN module (Detection of Exon Copy Number variants)
##########################################################################

# DEV version 0.1 : 10/11/2021
# INT version 0.1 : 17/03/2022
# PROD version 1 : 03/06/2022
# Authoring : Thomas LAVAUX

# PROD version 2 : 16/06/2022 changelog
	# add the possibility to analyse gender separately, extract gender from tag files ; gender analysis can be set with REMOVE_F & REMOVE_M & REMOVE_A options
	# add a gene list restriction to limit the analysis to a certain list of gene's name (GENE_FILTER & GENE_LIST_RESTRICT)
	# rewrite DECON bed / custom numbering formating
	# remove panel vcf filtering (output was essentially empty)

	# TOTEST merge tsv from AnnotSV in one file
	# TODO copy results files in DECON dir

################## Context ##################
# launch snakemake -s  snakefile_decon -c(numberofthreads) --config run=absolutepathoftherundirectory without / at the end of the path
# to launch the snakemake file, use --config to replace variables that must be properly set for the pipeline to work ie run path directory
# every variable defined in the yaml file can be change
# separate multiple variable with a space (ex  --config run=runname transProb=0.05 var1=0.05 var2=12)
# also use option --configfile another.yaml to replace and merge existing config.yaml file variables

# use -p to display shell commands
# use --lt to display docstrings of rules
# input file = bam files (if bai is needed, it will be generate)
# output file = vcf annoted with AnnotSV 3.x for each sample/bam, and a global vcf file will all the samples ; a set of vcf files by design/panel

################## Import libraries ##################
import os
import os.path
import glob
import pandas as pd

from datetime import datetime
from shutil import copy2
from shutil import move

################## Configuration file and PATHS ##################
configfile: "/app/config/default.yaml"

### COMMON VARIABLES ###
# Analysis only validation bam/cram files
validation_only = config['VALIDATION_ONLY']

# Debug mode
debug = config['DEBUG_MODE']

# Subdir of module results
serviceName = config['serviceName']

# Datas directory
rundir = config['run']

# Repository/depository directories
repositorydir = config['repository']
depositorydir = config['depository']
outputdir = config['OUTPUT_DIR']

# Copy to depot (archive)
depotcopy = config['DEPOT_COPY']

# Group and app name to construct output
group_name = config['GROUP_NAME']
app_name = config['APP_NAME']

# Design.bed or SAMPLE.bed or SAMPLE.aligner.design.bed in STARK folder
bed_file = config['BED_FILE']
# Panel.SAMPLE.APP.manifest.genes.bed or SAMPLE.APP.manifest.genes.bed in STARK folder
genes_file = config['GENES_FILE']
# Transcripts file containing NM notation SAMPLE.transcripts in STARK folder
transcripts_file = config['TRANSCRIPTS_FILE']
# List genes contains list of gene's files
list_genes = config['LIST_GENES']

# Search options to find files to process
recursive_search = config['RECURSIVE_SEARCH']
search_argument = config['SEARCH_ARGUMENT']

# Sample to remove, set in config_scramble.yaml file
sample_exclude_list = config['EXCLUDE_SAMPLE']
filter_sample_list = config['FILTER_SAMPLE']

# Ext to index into dictionary
ext_list = config['EXT_INDEX_LIST']
# Option for processing (cp/ls/samtools)
process_cmd = config['PROCESS_CMD']
# Option for processing bam or cram
process_file = config['PROCESS_FILE']

# Option to append an aligner name to a file (case if the bam don't have an aligner name == sample.bam)
aligner_name = config['ALIGNER_NAME']

# Rsync options for copying results/log
include_file = config['INCLUDE_RSYNC']
exclude_file = config['EXCLUDE_RSYNC']
include_file_log = config['INCLUDE_LOG_RSYNC']

### Variables specific for the tools ###
# RefSeq variable
refseqgene = config['REFSEQGENE_PATH']
# Refgene variable
refgene = config['REFGENEFA_PATH']
# Scripts path
scriptspath = config['SCRIPTS_PATH']

### Variable for DECON ###
# DECON dir for R scripts
decondir = config['DECON_PATH']

### DECON Bed processing ###
# Set DECON to custom exon analysis
custom_exon = config['customexon']
# Regen exon number with IntersectBed and a refseqgene reference 
customexon_regen = config['EXON_REGEN']
keep_unknown = config['KEEP_UNKNOWN']
# Patch for some gene name formatting
exon_sep = config['EXON_SEP']
# Formatting Bed old style
old_bed = config['OLD_BED']
# Gene filter option
gene_filter = config['GENE_FILTER']
# List of gene to analysis
gene_list_restrict = config['GENE_LIST_RESTRICT']


### FUNCTIONS ###

def finditemindico(sample_list, ext_list, dictionary, include_ext, exclude_ext):
	""" Function to search in a dictionary a file path itering by the first key (sample_list) and second key (extension_list) with an including and excluding filter """
	""" Result must be an non empty file """
	searchresult = ""
	for sample in sample_list:
		for ext in ext_list:
			try:
				items = dictionary[sample][ext]
				if include_ext in items and not exclude_ext in items:
					if os.path.exists(items) and os.path.getsize(items) != 0:
						searchresult = items
			except KeyError: pass
	return searchresult

def searchfiles(directory, search_arg, recursive_arg):
	""" Function to search all files in a directory, addind a search arguement append to the directory and a recursive_search options (True/False) """
	return sorted(filter(os.path.isfile, glob.glob(directory + search_arg, recursive=recursive_arg)))

def processalignedfiles(storedir, dictionary, sample, ext, aligner_list, process_cmd, name=None):
	""" Function to process bam or cram files who's path are stored in a dictionary by sample/ext name """
	""" You can either cp, ls bam files ; or use samtools to extract bam from a cram file into a storedir or filter a bam with a specific bed """
	""" You can append a name to the files if you specify a name """
	inputfile = dictionary[sample][ext]
	file_without_ext = os.path.splitext((os.path.basename(inputfile)))[0]
	# option to append name file with an aligner name (if file is sample.bam we need to rename it to sample.aligner.bam for the pipeline)
	if name:
		outputfile = storedir + "/" + file_without_ext + "." + name
		aligner_list.append(name)
	else:
		outputfile = storedir + "/" + file_without_ext
	for aligner in aligner_list:
		if aligner in inputfile:
			if process_cmd == 'cram':
				shell("samtools view -b -T "+refgene+" -o "+outputfile+".bam "+inputfile)	# samtools view -b -T ref_sequence.fa -o sample.bam sample.cram
			if process_cmd == 'cp':
				copy2(inputfile, outputfile+ext)
			if process_cmd == 'ls':
				shell("ln -sfn "+inputfile+" "+outputfile+ext)
			if process_cmd == 'filter':
				shell("samtools view -b -T "+refgene+" -L "+bed_file+" -o "+outputfile+".bam "+inputfile)	# samtools view -b -h -L bedfile.bed -o newbam.bam originalbam.bam

def logsomefile(logfile, text, sep, items_list=None, items=None):
	""" Function to log a variable value or a list of values into a log file """
	with open(logfile, 'a+') as f:
		f.write(text + sep)
		if items_list:
			for items in items_list:
				if items == '':
					f.write(str('None') + sep)
				else:
					f.write(str(items) + sep)
		else:
			if items == '':
				f.write(str('None') + sep)
			else:
				f.write(str(items) + sep)

# structure of bam files are sample.aligner.(validation).bam or sample.archive.cram
# samples name are taken from bam files
# [x] for x part of the filename separated by '.'
# [0] is sample ; [-1] is extension ; [1] is aligner
def extractlistfromfiles(file_list, extension, sep, position):
	""" Function for creating list from a file list, with a specific extension, a separator and the position of the string we want to extract """
	output_list = []
	for files in file_list:
		filesname = os.path.basename(files)
		if filesname.endswith(extension):
			output_list.append(filesname.split(sep)[position])
	output_list = list(set(output_list)) # we don't want duplicate in the list
	return output_list

def deconbedfromdesign(inputbed, outputbed, sepgenecolumn):
	""" Function to extract a DECON bed 4 columns from another bed, assuming that the 4th column contains a gene name with a unique separator """
	df = pd.read_csv(inputbed, sep='\t', names=["Chr", "Start", "End", "Gene"], index_col=False)
	df['Gene'] = df['Gene'].str.split(sepgenecolumn).str[0] # we extract the gene name == string before separator (ex TP73_chr1_3598878_3599018 sep is _chr to get TP73)
	df.to_csv(outputbed, sep='\t', index=False, header=False)

def create_list(txtlistoutput, file_list, ext, pattern):
	""" Function to create a txt file from a list of files filtered by extension and pattern """
	with open(txtlistoutput, 'a+') as f:
		for files in file_list:
			if pattern in files and files.endswith(ext):
				f.write(str(files) + "\n")

# tag ex SEX#M!POOL#POOL_HFV72AFX3_M_10#POOL_HFV72AFX3_F_11!
# extractfromtag(tagfile, 'gender', dico_run, 'SEX', '#', '!')
def extractfromtag(tagfile, key, dictionary, tag, tagsep, globalsep):
	""" Function to extract a specific tag in a tag file """
	""" ex : SEX#M! or SEX#F! tag is SEX, tagsep is #, globalsep is ! """
	with open(tagfile, 'r') as f:
		row = f.read()
		sample = os.path.basename(tagfile).split(".")[0]
		if tag in row and globalsep in row:
			tagfirst = row.split(globalsep)[0]
			tagfinal = tagfirst.split(tagsep)[1]
			dictionary[sample][key] = tagfinal
		else:
			tagfinal = None
			dictionary[sample][key] = tagfinal

def intersectbed(inputbed, refbed, outputbed):
	""" Function to intersect a bed with a ref bed using bedtools """
	shell("intersectBed -a "+inputbed+" -b "+refbed+" -loj > "+outputbed+" ")

def concatenatelist(inputlist, outputfile):
	""" Function to concatenate lines tab separated in a file """
	shell(" xargs --delimiter='\\n' cat <"+inputlist+" >> "+outputfile+" ")

def keep_unknown(columnname, pattern, prefix):
	""" Function to rename gene """
	i=0
	for j, gene in df[columnname].items():
		if gene == pattern:
			df.at[j, columnname] = prefix+str(i)
		i+=1

### END OF FUNCTIONS ###

# Variables initialisation
# set datetime to add to output file name
date_time = datetime.now().strftime("%Y%m%d-%H%M%S")

# Return last directory of the path run (without /)
# ex == /STARK/output/repository/GROUP/APP/run
nameoftherun = os.path.basename(rundir)

# Set directories to save tempory results and input files
inputdir = "/app/res/" + nameoftherun + "/input/"+ date_time
tmpdir = "/app/res/" + nameoftherun + "/tmp"

# Get group [4] and app [5] name from run (or -1 and -2 ?)
# run structure is "/STARK/output/repository/group/app/run"
# Default is UNKNOWN in the yaml file
try:
	group_name = rundir.split('/')[4]
	app_name = rundir.split('/')[5]
except IndexError: pass

# Construct repository and depository dir structure (outputdir will be the default output)
depotdir = depositorydir + "/" + group_name + "/" + app_name + "/" + nameoftherun + "/"

if not outputdir:
	outputdir = repositorydir + "/" + group_name + "/" + app_name + "/" + nameoftherun + "/"

# Set log path file
logfile = tmpdir + "/" + serviceName + "." + date_time + '.parameters.log'

# Set annotation file path
annotation_file = tmpdir + "/" + serviceName + "." + date_time + '.AnnotSV.txt'

# Create directories
os.makedirs(inputdir, exist_ok = True)
os.makedirs(tmpdir, exist_ok = True)
os.makedirs(outputdir, exist_ok = True)
if depotcopy == True:
	os.makedirs(depotdir, exist_ok = True)

# Search files in repository 
files_list = searchfiles(rundir, search_argument, recursive_search)

# Create sample and aligner list
sample_list = extractlistfromfiles(files_list, process_file, '.', 0)
aligner_list = extractlistfromfiles(files_list, process_file, '.', 1)

# Exclude samples from the exclude_list
for sample_exclude in sample_exclude_list:
	for sample in sample_list:
		if sample.startswith(sample_exclude):
			sample_list.remove(sample)

# If filter_sample_list variable is not empty, it will force the sample list
if filter_sample_list:
	sample_list = list(filter_sample_list)

# For validation analyse bam will be sample.aligner.validation.bam
if validation_only == True:
	append_aligner = '.validation'
	aligner_list = [sub + append_aligner for sub in aligner_list]

# Init dictionary
dico_run = {}
for samples in sample_list:
	dico_run[samples] = {}
# Populating dictionary
for samples in sample_list:
	for ext in ext_list:
		for files in files_list:
			if os.path.basename(files).split(".")[0] == samples and os.path.basename(files).endswith(ext):
				if validation_only == False:
					if ext == process_file and not 'validation' in files:
						dico_run[samples][ext] = files
					if ext != process_file and not 'validation' in files:
						dico_run[samples][ext] = files
				if validation_only == True:
					if ext == process_file and 'validation' in files:
						dico_run[samples][ext] = files
					if ext != process_file and not 'validation' in files:
						dico_run[samples][ext] = files

# Set a filelist with all the files tag
# File format is sample.tag
tagfile_list = []
for sample in sample_list:
	tagfile_list.append(dico_run[sample]['.tag'])

# Extract the tags from the list
for tagfile in tagfile_list:
	extractfromtag(tagfile, 'gender', dico_run, 'SEX', '#', '!')

# Extract the gender_list from dictionary with the key gender
gender_list = ['A'] # should be in config ?
for sample in sample_list:
	if dico_run[sample]['gender'] == 'F':
		gender_list.append('F')
	if dico_run[sample]['gender'] == 'M':
		gender_list.append('M')
# Removing duplicate
gender_list = list(set(gender_list))

# Find bed file (Design)
if not bed_file:
	bed_file = finditemindico(sample_list, ext_list, dico_run, '.design.bed', '.genes.bed')

# Find genes file (Panel)
# Note : we can't use .genes files because .list.genes and .genes are not distinctable from the indexing we made
if not genes_file:
	genes_file = finditemindico(sample_list, ext_list, dico_run, '.genes.bed', '.list.genes')

# Find transcripts files (NM)
if not transcripts_file:
	transcripts_file = finditemindico(sample_list, ext_list, dico_run, '.transcripts', '.list.transcripts')

# Separate cram/bam processing
# for cram extract to bam into the input directory
# for bam copy or symlink the bam files to input directory
for sample in sample_list:
	processalignedfiles(inputdir, dico_run, sample, process_file, aligner_list, process_cmd, name=aligner_name)

# Create file_list of bam by gender by searching the depot directory, depending on the gender_list
# A = all ; F = Female only ; M = Male only
# files_list_A contains the full path of all files (bam files for ex)
files_list_A = searchfiles(inputdir, '/*', False)
files_list_M = []
files_list_F = []
for files in files_list_A:
	sample = os.path.basename(files).split(".")[0]
	if dico_run[sample]['gender'] == 'F':
		files_list_F.append(files)
	if dico_run[sample]['gender'] == 'M':
		files_list_M.append(files)

# log the file lists
logsomefile(logfile, 'Input for all files:', "\n", items_list = files_list_A)
logsomefile(logfile, 'Input files for male analysis:', "\n", items_list = files_list_F)
logsomefile(logfile, 'Input files for female analysis:', "\n", items_list = files_list_M)

# Creating a txt list for the bam files per aligner per gender (A, M & F)
# the file_list_whatever must be prefiltered
for aligner in aligner_list:
	for gender in gender_list:
		if gender == 'A':
			bamlist = tmpdir + "/" + serviceName + "." + date_time + "." + gender + '.list.txt'
			create_list(bamlist, files_list_A, process_file, aligner)
		if gender == 'F':
			bamlist = tmpdir + "/" + serviceName + "." + date_time + "." + gender + '.list.txt'
			create_list(bamlist, files_list_F, process_file, aligner)
		if gender == 'M':
			bamlist = tmpdir + "/" + serviceName + "." + date_time + "." + gender + '.list.txt'
			create_list(bamlist, files_list_M, process_file, aligner)

# If transcript file exist, create the annotation file for AnnotSV
if os.path.exists(transcripts_file) and os.path.getsize(transcripts_file) != 0:
	df = pd.read_csv(transcripts_file, sep='\t', names=["NM", "Gene"])
	df = df.drop(['Gene'], 1)
	NM_list = df.loc[:,'NM'].tolist()
	with open(annotation_file, 'w+') as f:
		f.write('\t'.join(NM_list))
else:
	with open(annotation_file, 'w') as f:
		f.write("No NM found")

# Find list.genes files 
if not list_genes:
	list_genes = finditemindico(sample_list, ext_list, dico_run, '.list.genes', '.list.transcripts')

### DECON bed ###
# DECON need a 4 columns bed for the ReadInBams rule, if bed is not set correctly ReadInBams will crash
# Chromosome Start End Gene but no header, bed format (tsv)
# Chr must be formated chr1, Gene column must be a string, no negative numbers
# ex	:	chr17	4511	4889	BCRA1

### DECON Custom.Exon ###
# DECON can use a customNumbering.txt exons description for the accurate formating of pdfs graph (makeCNVcalls) and the IdentifyFailures rule, file is not mandatory
# Format is tab separated with a header	:	'Chr'	'Start'	'End'	'Gene'	'Custom.Exon'
# exemple	:	17	4511	4889	BCRA1	26
# ! No chr before chr number & documentation forgot to mention Gene column

inputbed = inputdir+"/"+serviceName+".input.bed"
cat_panels_bed = inputdir+"/"+serviceName+".concatenatepanels.bed"
cat_list_genes = inputdir+"/"+serviceName+".fullpath.genes.list"
cat_noduplicate_panels_bed = inputdir+"/"+serviceName+".concatenatepanelsnoduplicate.bed"
outputbed = inputdir + "/" + serviceName + ".output.bed"
intermediatebed = inputdir + "/" + serviceName + '.intermediate.bed'

# Case if genes file bed don't have gene name & exon number
# We always create a custom exon file from the design bed and intersectBed with a refseqgene database to get the exon number
# Warning the refseqgene database must be generated before
if bed_file and customexon_regen == True:
	# Generating the custom.exon file from scratch (intersectBed with refseqgene file)
	df = pd.read_csv(bed_file, sep='\t', names=["Chr", "Start", "End"], index_col=False)
	df.to_csv(inputbed, sep='\t', index=False, header=False)
	####### Intersect bed with refseqgene using bedtools #######
	intersectbed(inputbed, refseqgene, outputbed)
	df = pd.read_csv(outputbed, sep='\t', names=["Chr", "Start", "End", "4", "5", "6", "Tosplit", "Gene", "9"])
	df['Custom.Exon'] = df['Tosplit'].str.split(',').str[-1]
	df['Custom.Exon'] = df['Custom.Exon'].str.replace('exon','')
	# Filter with NM_list if NM_list is not empty (avoid multiple NM/exons for the graphs)
	# Specific for customexon_regen == True
	if os.path.exists(transcripts_file) and os.path.getsize(transcripts_file) != 0:
		df['NM'] = df['Tosplit'].str.split(',').str[0]
		df = df[df['NM'].isin(NM_list)]
	# Formating unknown gene name: intersect will output -1 if no gene name exist, replace with "Unknown"
	if keep_unknown == True:
		keep_unknown('Gene', '-1', 'Unknown')
	else:
		df = df[df.Gene != '-1']
	df = df.drop_duplicates()
	if gene_filter == True:
		for gene in gene_list_restrict:
			df = df[df['Chr'].isin(gene)]
	df.to_csv(intermediatebed, sep='\t', index=False, header=True)

# Old bed compatibility
# Intersect bed design with panel intersectBed -a design.bed -b panel.bed -loj > RESULT.bed = columns 1 2 3 and the gene column from the panel is 10
# chr1	65300206	65300354	JAK1_chr1_65300207_65300354	0	+	chr1	65300240	65300344	JAK1_ex25	0	- (gene is JAK1_ex25)
# chr1	3598877	3599018	TP73_chr1_3598878_3599018	0	+	.	-1	-1	.	-1	. (gene is .)
# New bed 13 cols
# Intersect bed design with panel intersectBed -a design.bed -b panel.bed -loj > RESULT.bed = columns 1 2 3 & 10 are retained for the bed, column 13 for the exon number
# ex = chr1	27022390	27024129	NM_006015_ARID1A_5utr-ex1_chr1_27022391_27024129	0	+	.	-1	-1	.	-1	.
if bed_file and genes_file and customexon_regen == False:
	if list_genes:
		basepath = os. path. dirname(list_genes)
		with open(list_genes) as f:
			cat_panel_list = f.read().splitlines()
			cat_panel_list = [basepath + "/" + s for s in cat_panel_list]
		with open(cat_list_genes, 'w+') as f:
			f.write('\t'.join(cat_panel_list))
		shell(" xargs --delimiter='\\n' cat <"+cat_list_genes+" >> "+cat_panels_bed+" ")
	else:
		shell(" xargs --delimiter='\\n' cat <"+genes_file+" >> "+cat_panels_bed+" ")
	df = pd.read_csv(cat_panels_bed,  sep='\t')
	df = df.drop_duplicates()
	df.to_csv(cat_noduplicate_panels_bed, sep='\t', index=False, header=False)
	####### Intersect bed with genes files using bedtools #######
	intersectbed(bed_file, cat_noduplicate_panels_bed, outputbed)
	if old_bed == False:
		df = pd.read_csv(outputbed, sep='\t', names=["Chr", "Start", "End", "4", "5", "6", "7", "8", "9", "Gene", "11", "12", "Custom.Exon"])
		df = df.drop(['4', '5', '6', '7', '8', '9', '11', '12'], 1)
	else:
		df = pd.read_csv(outputbed, sep='\t', names=["Chr", "Start", "End", "4", "5", "6", "7", "8", "9", "Gene", "11", "12"])
		df = df.drop(['4', '5', '6', '7', '8', '9', '11', '12'], 1)
	# Keep unknown option
	if keep_unknown == True:
		keep_unknown('Gene', '.', 'Unknown')
	else:
		df = df[df.Gene != '.']
	# Fix for some bed that remove _ex part if gene is noted genename_ex
	if exon_sep == True: 
		df['Gene'] = df['Gene'].str.split('_ex').str[0]
	#df = df[pd.to_numeric(df['Custom.Exon'], errors='coerce').notnull()] # value is NaN if errors
	df = df.drop_duplicates()
	# Option to filter the bed to analyse only certain genes
	if gene_filter == True:
		for gene in gene_list_restrict:
			df = df[df['Chr'].isin(gene)]
	df.to_csv(intermediatebed, sep='\t', index=False, header=False)

	# Set Decon Bed files
	deconbed_file_A = tmpdir + "/" + serviceName + "." + date_time + '.A.bed'
	deconbed_file_F = tmpdir + "/" + serviceName + "." + date_time + '.F.bed'
	deconbed_file_M = tmpdir + "/" + serviceName + "." + date_time + '.M.bed'

	# DECON bed formating from intermediatebed file
	df = pd.read_csv(intermediatebed, sep='\t', names=["Chr", "Start", "End", "Gene"], index_col=False)
	df = df[-df['Chr'].isin(['chrX', 'chrY'])] # removing all chrX & chrY reference
	df.to_csv(deconbed_file_A, sep='\t', index=False, header=False) # for decon.be no header

	df = pd.read_csv(intermediatebed, sep='\t', names=["Chr", "Start", "End", "Gene"], index_col=False)
	df = df[df['Chr'].isin(['chrX'])] # keep only chrX ref
	df.to_csv(deconbed_file_F, sep='\t', index=False, header=False) # for decon.be no header

	df = pd.read_csv(intermediatebed, sep='\t', names=["Chr", "Start", "End", "Gene"], index_col=False)
	df = df[df['Chr'].isin(['chrX', 'chrY'])] # keep only chrY & chrX ref
	df.to_csv(deconbed_file_M, sep='\t', index=False, header=False) # for decon.be no header

	if custom_exon == True:
		# Set the customnumbering files for makeCNVcalls& Identify_failures
		customnumberingfile_A = tmpdir + "/" + serviceName + "." +  date_time + '.A.CustomNumbering.txt'
		customnumberingfile_F = tmpdir + "/" + serviceName + "." +  date_time + '.F.CustomNumbering.txt'
		customnumberingfile_M = tmpdir + "/" + serviceName + "." +  date_time + '.M.CustomNumbering.txt'

		# Autosome
		df = pd.read_csv(intermediatebed, sep='\t', names=["Chr", "Start", "End", "Gene", "Custom.Exon"], index_col=False)
		df = df[-df['Chr'].isin(['chrX', 'chrY'])] # removing all chrX & chrY reference
		df['Chr'] = df['Chr'].str.replace('chr','') # remove prefix to Chr column value (for custom.exon file)
		df.to_csv(customnumberingfile_A, sep='\t', index=False, header=True) # for custom.exon file, need a header

		# Gonosome X
		df = pd.read_csv(intermediatebed, sep='\t', names=["Chr", "Start", "End", "Gene", "Custom.Exon"], index_col=False)
		df = df[df['Chr'].isin(['chrX'])] # keep only chrX ref
		df['Chr'] = df['Chr'].str.replace('chr','') # remove prefix to Chr column value (for custom.exon file)
		df.to_csv(customnumberingfile_F, sep='\t', index=False, header=True) # for custom.exon file, need a header

		# Gonosome Y
		df = pd.read_csv(intermediatebed, sep='\t', names=["Chr", "Start", "End", "Gene", "Custom.Exon"], index_col=False)
		df = df[df['Chr'].isin(['chrY'])] # keep only chrY ref
		df['Chr'] = df['Chr'].str.replace('chr','') # remove prefix to Chr column value (for custom.exon file)
		df.to_csv(customnumberingfile_M, sep='\t', index=False, header=True) # for custom.exon file, need a header

if not bed_file:
	logsomefile(logfile, 'No bed found, DECON cannot continue, exiting', "\n")
	exit()

# Option to remove gender in the gender_list to force A, M or F analysis only
remove_F = config['REMOVE_F']
remove_M = config['REMOVE_M']
remove_A = config['REMOVE_A']
if remove_M == True:
	gender_list.remove('M')
if remove_F == True:
	gender_list.remove('F')
if remove_A == True:
	gender_list.remove('A')

# Log
logsomefile(logfile, 'Start of the analysis:', "\n", items = date_time)
logsomefile(logfile, 'List of samples:', "\n", items_list = sample_list)
logsomefile(logfile, 'List of gender:', "\n", items_list = gender_list)
logsomefile(logfile, 'Aligner list:', "\n", items_list = aligner_list)
logsomefile(logfile, 'Analyse run:', "\n", items = nameoftherun)
logsomefile(logfile, 'Design Bed:', "\n", items = bed_file)
logsomefile(logfile, 'Panel Bed:', "\n", items = genes_file)
logsomefile(logfile, 'Transcripts list:', "\n", items = transcripts_file)
logsomefile(logfile, 'List of genes files:', "\n", items = list_genes)

# Copy2 bed_file & genes_file & transcripts_file (for debug)
if debug:
	try:
		copy2(bed_file, inputdir)
		copy2(genes_file, inputdir)
		copy2(transcripts_file, inputdir)
	except FileNotFoundError: pass

################################################## RULES ##################################################
# The input for the rule all is the output of the pipeline
# The expand for the {sample} {aligner} is done only at this step
# Final results will follow this structure in the output directory
# /nameoftherun/SAMPLENAME/ServiceName/ individual results
# /nameoftherun/global results & global logs with ServiceName index
#
# rule all will different depending on the bed files available
# design = .bed, vcf will be vcf.design ; panel = genes, vcf will be vcf.panel ; not bed or genes = vcf will be vcf.full
# warning : bcftools merge will crash if there's only 1 sample, so we need to cp files
############################################################################################################

##### NOTE ####
# Tools that output a file adding a extension should be set with params: params.output as the output of the command without the extension, and output: output.ext exist only to connect rules
# For programs that do not have an explicit log parameter, you may always use 2> {log} to redirect standard output to a file (here, the log file) in Linux-based systems. Note that it is also supported to have multiple (named) log files being specified
# 1 is stdout, 2 is stderr 
##################

ruleorder: indexing > ReadInBams

rule all:
	"""
	Rule will create a design and panel vcf.gz if a gene list file is provided, else a design vcf.gz only
	"""
	input:
		bai = expand(inputdir + "/" + "{sample}.{aligner}.bai", aligner=aligner_list, sample=sample_list),
		allfailtxt = tmpdir + "/" + serviceName + "." + date_time + ".Identify_Failures.txt",
		vcfgzdesign = expand(tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf.gz", aligner=aligner_list, sample=sample_list),
		vcfgzalldesign = tmpdir + "/" + serviceName + "." + date_time + ".all.Design.vcf.gz"

rule help:
	"""
	General help for DECON
	Launch snakemake -s  snakefile_decon -c(numberofthreads) --config DATA_DIR=absolutepathoftherundirectory (default is data) without / at the end of the path
	To launch the snakemake file, use --config to replace variables that must be properly set for the pipeline to work ie run path directory
	Every variable defined in the yaml file can be change
	Separate multiple variable with a space (ex  --config DATA_DIR=runname transProb=0.05 var1=0.05 var2=12)
	Also use option --configfile another.yaml to replace and merge existing config.yaml file variables
	Use -p to display shell commands
	Use --lt to display docstrings of rules
	Input file = bam or cram files (if bai is needed, it will be generate)
	Output file = vcf annoted with AnnotSV 3.x for each sample/bam, and a global vcf file will all the samples ; a set of vcf files by design/panel
	"""

rule indexing:
	"""
	Indexing bam files with samtools
	"""
	input:
		bam = inputdir + "/" + "{sample}.{aligner}.bam"
	output:
		bai = inputdir + "/" + "{sample}.{aligner}.bai"
	params:
		threads = config['THREADS']
	shell:
		"""
		samtools index -b -@ {params.threads} {input.bam} {output.bai}
		"""

# Keep the chr prefix to the Chr list (chr1 and not 1) if your ref genome got chr1 notation (and not 1) if not ReadInBams will crash
# ReadInBams must run R in the decondir directory to be able to load packrat
rule ReadInBams:
	"""
	DECoN uses a list of BAM files and a BED file of exons to calculate a coverage metric called the fragment per kilobase and million base pairs (FPKM) for each exon specified in the BED file in each sample’s BAM file
	Rscript ReadInBams.R --bams listofbams.txt --bed file.bed (unique for all bams) --fasta refgenome.fasta --out output.(prefix).RData
	Requirement : all .bam files must have a .bai file associated with in the same folder
	"""
	input:
		bamlist = tmpdir + "/" + serviceName + "." + date_time + ".{gender}.list.txt",
		bai = expand(inputdir + "/" + "{sample}.{aligner}.bai", sample=sample_list, aligner=aligner_list)
	output:
		rdata = tmpdir + "/" + serviceName + '.' + date_time + ".{aligner}.{gender}.RData"
	params:
		output = tmpdir + "/" + serviceName + '.' + date_time + ".{aligner}.{gender}",
		deconbed_file = tmpdir + "/" + serviceName + "." + date_time + ".{gender}.bed"
	log: 
		log1 = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.ReadInBams.log", log2 = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.ReadInBams.err"
	shell:
		"""
		cd {decondir} && Rscript ReadInBams.R --bams {input.bamlist} --bed {params.deconbed_file} --fasta {refgene} --out {params.output} 1> {log.log1} 2> {log.log2}
		"""

# makeCNVcalls must run R in the decondir directory to be able to load packrat
# We add a if then condition because --custom FALSE will not work if we provide a --exons file
rule makeCNVcalls:
	"""
	This step calls exon CNVs in each sample by selecting reference samples from all other samples contained in the input summary .RData file. The correlation between samples and the number of samples used as a reference are thus calculated and outputted to aid interpretation of call quality
	Rscript makeCNVcalls.R --Rdata summary.file --transProb 0.01 (default) --exons listofexonsforspecificannotation --custom TRUE/FALSE --out output.prefix –-plot  None, Custom or All --plotFolder Plotfoldertoset
	output is _all.txt and a summary Rdata associated ; if custom is true, a second file is ouputed named _custom.txt
	"""
	input:
		rdata = tmpdir + "/" + serviceName + '.' + date_time + ".{aligner}.{gender}.RData"
	params:
		prob = config['transProb'],
		pdf = tmpdir + "/pdfs/",
		exons = tmpdir + "/" + serviceName + "." +  date_time + ".{aligner}.{gender}.CustomNumbering.txt",
		plot = config['plot'],
		customplot = config['customexon'],
		output = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.Design_results",
		analysisfailure = tmpdir + "/" + serviceName + "." + date_time + ".{gender}.AnalysisFailed.txt"
	output:
		calltxt = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.Design_results_all.txt"
	log: 
		log1 = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.makeCNVcalls.log", log2 = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.makeCNVcalls.err"
	shell:
		"""
		if [ {params.customplot} = True ];
		then
			cd {decondir} && Rscript makeCNVcalls.R --Rdata {input.rdata} --plot {params.plot} --exons {params.exons} --custom {params.customplot} --transProb {params.prob} --plotFolder {params.pdf} --out {params.output} 1> {log.log1} 2> {log.log2}  && [[ -s {output.calltxt} ]] || touch {output.calltxt} || touch {params.analysisfailure}
		else
			cd {decondir} && Rscript makeCNVcalls.R --Rdata {input.rdata} --plot {params.plot} --transProb {params.prob} --plotFolder {params.pdf} --out {params.output} 1> {log.log1} 2> {log.log2} && [[ -s {output.calltxt} ]] || touch {output.calltxt} && touch {params.analysisfailure}
		fi
		"""

# IdentifyFailures must run R in the decondir directory to be able to load packrat
# We add a if then condition because --custom FALSE will not work if we provide a --exons file
rule IdentifyFailures:
	"""
	The summary .RData file outputted can be used to flag any samples or exons where exon CNV calling may be suboptimal. Both exons and samples are evaluated based on their median coverage level. When coverage is low, accuracy of detection will be compromised and caution should be exercised when interpreting results. Samples are also evaluated based on their correlation with other samples. Samples which do not have a high correlation with other samples in the set are likely to have suboptimal detection across the entire target
	Rscript IdentifyFailures.R --Rdata file.RData --mincorr .98 --mincov 100 --exons listofexonsforspecificannotation --custom TRUE/FALSE --out output.prefix
	output is _Failures.txt ; if --custom is true a second file is output named _custom_Failures.txt
	"""
	input:
		rdata = tmpdir + "/" + serviceName + '.' + date_time + ".{aligner}.{gender}.RData"
	params:
		exons = tmpdir + "/" + serviceName + "." +  date_time + ".{aligner}.{gender}.CustomNumbering.txt",
		mincorr = config['mincorr'],
		mincov = config['mincov'],
		customplot = config['customexon'],
		output = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.Identify",
		analysisfailure = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.MetricsFailed.txt"
	output:
		failtxt = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.Identify_Failures.txt"
	log: 
		log1 = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.Identify_Failures.log", log2 = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.Identify_Failures.err"
	shell:
		"""
		if [ {params.customplot} = True ];
		then
			cd {decondir} && Rscript IdentifyFailures.R --Rdata {input.rdata} --exons {params.exons} --custom {params.customplot} --mincorr {params.mincorr} --mincov {params.mincov} --out {params.output} 1> {log.log1} 2> {log.log2} && [[ -s {output.failtxt} ]] || touch {output.failtxt} || touch {params.analysisfailure}
		else
			cd {decondir} && Rscript IdentifyFailures.R --Rdata {input.rdata} --mincorr {params.mincorr} --mincov {params.mincov} --out {params.output} 1> {log.log1} 2> {log.log2} && [[ -s {output.failtxt} ]] || touch {output.failtxt} && touch {params.analysisfailure}
		fi
		"""


rule merge_makeCNVcalls:
	"""
	Merge all call results
	"""
	input:
		calltxt =  expand(tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.Design_results_all.txt", gender=gender_list, aligner=aligner_list)
	output:
		allcalltxt = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.Design_results_all.txt"
	shell:
		"""
		cat {input.calltxt} | sort -u | sort -r >> {output.allcalltxt}
		"""


rule merge_IdentifyFailures:
	"""
	Merge all metrics results
	"""
	input:
		failtxt = expand(tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.Identify_Failures.txt", gender=gender_list, aligner=aligner_list)
	output:
		allfailtxt = tmpdir + "/" + serviceName + "." + date_time + ".Identify_Failures.txt"
	shell:
		"""
		cat {input.failtxt} | sort -u | sort -r >> {output.allfailtxt}
		"""

# crash if conversion goes wrong
rule Decon_tsv2vcf:
	input:
		allcalltxt = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.Design_results_all.txt"
	output:
		vcfunsortall = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.allsamples.Design.unsort.vcf"
	log:
		log2 = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.DeconTsv2Vcf.log"
	shell:
		"""
		python {scriptspath}/file_converter.py -i {input.allcalltxt} -o {output.vcfunsortall} -fi tsv -fo vcf -c {scriptspath}/fileconversion/config_decon.json 2> {log.log2}
		"""

# bcftools sort don't sort correctly
rule sortvcfall:
	input:
		vcfunsortall = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.allsamples.Design.unsort.vcf"
	output:
		vcfsortall = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.allsamples.Noannotation.vcf"
	shell:
		"""
		grep \"^#\" {input.vcfunsortall} > {output.vcfsortall} && grep -v \"^#\" {input.vcfunsortall} | sort -k1,1V -k2,2g >> {output.vcfsortall} && [[ -s {output.vcfsortall} ]] || cat {scriptspath}/empty.vcf > {output.vcfsortall}
		"""

# bgzip -c to keep input file
rule vcf2gzall:
	input:
		vcfsortall = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.allsamples.Noannotation.vcf"
	output:
		vcfgzdesign = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.allsamples.Noannotation.vcf.gz"
	shell:
		"""
		bgzip -c {input.vcfsortall} > {output.vcfgzdesign} && [[ -s {output.vcfgzdesign} ]] || cat {scriptspath}/empty.vcf.gz > {output.vcfgzdesign} ; tabix {output.vcfgzdesign}
		"""

# --force-samples will output a vcf file with all annotations but without a sample name if the sample did not exist
# so we output a dummy vcf.gz
rule splitvcf:
	"""
	Split vcf with bcftools
	-s {sample} : comma-separated list of samples to include
	-Oz : output vcf compressed
	-c1 : minimum allele count (INFO/AC) of sites to be printed
	"""
	input:
		vcfgzdesign = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.allsamples.Noannotation.vcf.gz"
	output:
		vcfgzsplit = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.Noannotation.vcf.gz"
	params:
		path = tmpdir + "/{sample}/" + serviceName + "/"
	shell:
		"""
		mkdir -p {params.path} && bcftools view -c1 -Oz -s {wildcards.sample} -o {output.vcfgzsplit} {input.vcfgzdesign} && [[ -s {output.vcfgzsplit} ]] || cat {scriptspath}/empty.vcf.gz > {output.vcfgzsplit} ; tabix {output.vcfgzsplit}
		"""

# AnnotSV need bedtools to work ie you have to specify the directory of bedtools binaries in -bedtools argument
# add -hpo {params.hpofile} if hpofile exist
rule AnnotSV:
	"""
	AnnotSV will annotate and rank Structural Variations (SV) from a vcf file. Output will be an AnnotSV tsv file.
	-annotationMode can be : split by exons/introns or full by genes
	-txtFile : path to a file containing a list of preferred genes transcripts to be used in priority during the annotation, preferred genes transcripts names should be tab or space separated
	-genomeBuild must be specified if not hg19
	"""
	input:
		vcf = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.Noannotation.vcf.gz"
	output:
		tsvuncorr = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design_uncorr.tsv"
	log:
		txt = tmpdir + "/{sample}/" + serviceName +  "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.log"
	params:
		bedtools = config['BEDTOOLS_PATH'],
		bcftools = config['BCFTOOLS_PATH'],
		genome = config['genomeBuild'],
		overlap = config ['overlap'],
		mode = config['annotationMode']
	shell:
		"""
		AnnotSV -SVinputFile {input.vcf} -outputFile {output.tsvuncorr} -bedtools {params.bedtools} -annotationMode {params.mode} -txFile {annotation_file} -genomeBuild {params.genome} -overlap {params.overlap} > {log.txt} && [[ -s {output.tsvuncorr} ]] || echo "No data to annotate" > {output.tsvuncorr}
		"""

# AnnotSV drop chr from input 
#  we need to re-add chr to the SV_chrom column (column number 2)
rule patchchr:
	"""
	Add chr to the SV_chrom column
	"""
	input:
		tsvuncorr = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design_uncorr.tsv"
	output:
		tsv = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.tsv"
	shell:
		"""
		awk '{{{{FS=OFS="\t"}};if(NR==1){{print; next}}; $2="chr"$2; print}}' {input.tsvuncorr} > {output.tsv}
		"""

rule AnnotSV2vcf:
	"""
	File converter will convert tsv AnnotSV into vcf
	"""
	input:
		tsv = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.tsv"
	output:
		vcfunsort = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design_unsort.vcf"
	log:
		log2 = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.AnnotSV2vcf.log"
	shell:
		"""
		python {scriptspath}/file_converter.py  -i {input.tsv} -o {output.vcfunsort} -fi annotsv -fo vcf -c {scriptspath}/fileconversion/config_annotsv3.json 2> {log.log2} && [[ -s {output.vcfunsort} ]] || echo "No data to annotate" > {output.vcfunsort}
		"""

# bcftools sort don't sort correctly
rule sortvcf:
	"""
	Bash script to sort a vcf
	"""
	input:
		vcfunsort = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design_unsort.vcf"
	output:
		vcfsort = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf"
	shell:
		"""
		grep \"^#\" {input.vcfunsort} > {output.vcfsort} && grep -v \"^#\" {input.vcfunsort} | sort -k1,1V -k2,2g >> {output.vcfsort} && [[ -s {output.vcfsort} ]] || cat {scriptspath}/empty.vcf | sed 's/SAMPLENAME/{wildcards.sample}/g' > {output.vcfsort}
		"""

# bcftools need vcf.gz.tbi index for the merge all
rule vcf2gz:
	"""
	Compress vcf with bgzip
	"""
	input:
		vcfsort = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf"
	output:
		vcfgzdesign = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf.gz"
	shell:
		"""
		bgzip -c {input.vcfsort} > {output.vcfgzdesign} ; tabix {output.vcfgzdesign}
		"""


rule mergedesign:
	"""
	Copy or merge with bcftools several vcfs
	"""
	input:
		vcfgzdesign = expand(tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf.gz", sample=sample_list, aligner=aligner_list)
	output:
		vcfgzalldesign = tmpdir + "/" + serviceName + "." + date_time + ".all.Design.vcf.gz"
	shell:
		"""
		bcftools merge {input.vcfgzdesign} -O z -o {output.vcfgzalldesign} && tabix {output.vcfgzalldesign}
		"""

rule mergeAnnotSV_tsv:
	"""
	Merge AnnotSV tsv results
	"""
	input:
		tsv = expand(tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.tsv",sample=sample_list, aligner=aligner_list)
	output:
		alltsv = tmpdir + "/" + serviceName + "." + date_time + ".all.AnnotSV.Design.tsv"
	shell:
		"""
		cat {input.tsv} | sort -u | sort -r | >> {output.alltsv}
		"""


onstart:
	shell("touch " + outputdir + "/" + serviceName + "Running.txt")
	# Add the snakemake parameters to log
	with open(logfile, "a+") as f:
		json.dump(config, f)
		f.write("\n")

onsuccess:
	shell("touch " + outputdir + "/" + serviceName + "Complete.txt")
	shell("rm -f " + outputdir + "/" + serviceName + "Running.txt")
	# Sort pdfs file into each sample directory
	pdf_list = os.listdir(tmpdir+"/pdfs")
	for sample in sample_list:
		for pdf in pdf_list:
			if sample in pdf:
				# Change directoy for destination
				# Structure of output is /SAMPLE/ServiceName/pdfs 
				shell("mkdir -p " + outputdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/pdfs")
				shell("mv "+tmpdir+ "/pdfs/"+pdf+" " +outputdir+"/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/pdfs/")
	# if move rmdir the /pdfs dir
	shell("rm -rf "+tmpdir+"/pdfs")
	# Add end time of the analysis to the log file
	date_time_end = datetime.now().strftime("%Y%m%d-%H%M%S")
	with open(logfile, "a+") as f:
		f.write("End of the analysis : ")
		f.write((date_time_end) + "\n")
	# Copy files to final destination
	shell("rsync -azvh --include={include_file} --exclude {exclude_file} {tmpdir}/ {outputdir}")
	if depotcopy == True:
		shell("rsync -azvh --include={include_file} --exclude {exclude_file} {tmpdir}/ {depotdir}")

onerror:
	shell("touch " + outputdir + "/" + serviceName + "Failed.txt")
	shell("rm -f " + outputdir + "/" + serviceName + "Running.txt")
	shell("rsync -azvh --include={include_file_log} --exclude {exclude_file} {tmpdir}/ {outputdir}")