##########################################################################
# Snakemakefile Version:   3
# Description:             Snakemake file to run DECoN module (Detection of Exon Copy Number variants)
##########################################################################

# DEV version 0.1 : 10/11/2021
# INT version 0.1 : 17/03/2022
# PROD version 1 : 03/06/2022
# Authoring : Thomas LAVAUX

# PROD version 2 : 14/10/2022 changelog
	# add the possibility to analyse gender separately, extract gender from tag files ; gender analysis can be set with REMOVE_F & REMOVE_M & REMOVE_A options
	# add a gene list restriction to limit the analysis to a certain list of gene's name (GENE_LIST_RESTRICT)
	# rewrite DECON bed / custom numbering formating
	# remove panel vcf filtering (output was essentially empty)
	# add a chr filter option to remove chromosome (essentially chrY) (CHR_LIST_RESTRICT)
	# add a chrX Male gender analysis only to avoid chrY failure (REMOVE_M_noY)
	# rename failures and results_all from txt to tsv
	# exclude samples list case insensitive
	# copying new analysis per sample in the root sample dir & removing the old ones
	# add vcf2tsv converter ((https://github.com/sigven/vcf2tsvpy) & corresponding rules : each vcf will be convert to tsv
	# correct run path by removing ending '/' if exist
	# keep "Running.txt" file if failed, avoiding multiple analysis launch by the listener

# PROD version 3 : 19/09/2023 changelog
	# AnnotSV version 3.3.6 (include the vcf converter)
	# more options to process DECON bed, update the processalignedfiles function
	# R scripts rewrite, to speed up ReadInBam, separate plotting, call CNV with a reference bam list (see changlog in the R scripts)
	# re-arrange code for snakemake

################## Context ##################
# launch snakemake -s  snakefile_decon -c(numberofthreads) --config run=absolutepathoftherundirectory without / at the end of the path
# to launch the snakemake file, use --config to replace variables that must be properly set for the pipeline to work ie run path directory
# every variable defined in the yaml file can be change
# separate multiple variable with a space (ex  --config run=runname transProb=0.05 var1=0.05 var2=12)
# also use option --configfile another.yaml to replace and merge existing config.yaml file variables

# use -p to display shell commands
# use --lt to display docstrings of rules
# input file = bam files (if bai is needed, it will be generate)
# output file = vcf annoted with AnnotSV 3.x for each sample/bam, and a global vcf file will all the samples ; a set of vcf files by design/panel

################## Import libraries ##################
import os
import os.path
import glob
import pandas as pd
import json

from datetime import datetime
from shutil import copy2

################## Configuration file ##################
configfile: "/app/config/snakefile/default.yaml"

####################### FUNCTIONS ######################

def finditemindico(sample_list, ext_list, dictionary, include_ext, exclude_ext):
	""" Function to search in a dictionary a file path itering by the first key (sample_list) and second key (extension_list) with an including and excluding filter """
	""" Result must be an non empty file """
	searchresult = ""
	for sample in sample_list:
		for ext in ext_list:
			try:
				items = dictionary[sample][ext]
				if include_ext in items and not exclude_ext in items:
					if os.path.exists(items) and os.path.getsize(items) != 0:
						searchresult = items
			except KeyError: pass
	return searchresult

def searchfiles(directory, search_arg, recursive_arg):
	""" Function to search all files in a directory, adding a search arguement append to the directory and a recursive_search options (True/False) """
	return sorted(filter(os.path.isfile, glob.glob(directory + search_arg, recursive=recursive_arg)))

def processalignedfiles(storedir, dictionary, sample, ext_list, aligner_list, process_cmd, name=None,refgenome=None, bedfile=None):
	""" Function to process files who's path are stored in a dictionary by sample/ext name"""
	""" You can either cp, rsync, ls files ; or use samtools to extract bam from a cram file into a storedir"""
	""" You can append a name to the end of the files (before extension) if you specify a name value """
	for ext in ext_list:
		inputfiles = dictionary[sample][ext]
		file_without_ext = os.path.basename(inputfiles.replace(ext,''))
		if name:
			outputfile = storedir + "/" + file_without_ext  + "." + name
			aligner_list.append(name)
		else:
			outputfile = storedir + "/" + file_without_ext
		for aligner in aligner_list:
			if aligner in inputfiles:
				if process_cmd == 'cram':
					print("Uncompress cram files with samtools")
					print(inputfiles)
					shell("samtools view -b -T "+refgenome+" -o "+outputfile+".bam "+inputfiles)	# samtools view -b -T ref_sequence.fa -o sample.bam sample.cram
				if process_cmd == 'cp':
					print("Copying files to analyse with copy2")
					print(inputfiles)
					copy2(inputfiles, outputfile+ext)
				if process_cmd == 'rsync':
					print("Copying files to analyse with rsync")
					print(inputfiles)
					shell("rsync -azvh "+inputfiles+" "+storedir)
				if process_cmd == 'ls':
					print("Creating symobling links with ln for the files to analyse")
					print(inputfiles)
					shell("ln -sfn "+inputfiles+" "+outputfile+ext)
				if process_cmd == 'filterbam':
					print("Filter bam files with samtools with the bed provided")
					print(inputfiles)
					shell("samtools view -b -T "+refgenome+" -L "+bedfile+" -o "+outputfile+".bam "+inputfiles)	# samtools view -b -h -L bedfile.bed -o newbam.bam originalbam.bam

def logsomefile(logfile, text, sep, items_list=None, items=None):
	""" Function to log a variable value or a list of values into a log file """
	with open(logfile, 'a+') as f:
		f.write(text + sep)
		if items_list:
			for items in items_list:
				if items == '':
					f.write(str('None') + sep)
				else:
					f.write(str(items) + sep)
		else:
			if items == '':
				f.write(str('None') + sep)
			else:
				f.write(str(items) + sep)

# structure of bam files are sample.aligner.(validation).bam or sample.archive.cram
# samples name are taken from bam files
# [x] for x part of the filename separated by '.'
# [0] is sample ; [-1] is extension ; [1] is aligner
def extractlistfromfiles(file_list, ext_list, sep, position):
	""" Function for creating list from a file list, with a specific extension, a separator and the position of the string we want to extract """
	output_list = []
	for files in file_list:
		filesname = os.path.basename(files)
		for ext in ext_list:
			if filesname.endswith(ext):
				output_list.append(filesname.split(sep)[position])
	output_list = list(set(output_list))
	return output_list

def deconbedfromdesign(inputbed, outputbed, sepgenecolumn):
	""" Function to extract a DECON bed 4 columns from another bed, assuming that the 4th column contains a gene name with a unique separator """
	df = pd.read_csv(inputbed, sep='\t', names=["Chr", "Start", "End", "Gene"], index_col=False)
	df['Gene'] = df['Gene'].str.split(sepgenecolumn).str[0] # we extract the gene name == string before separator (ex TP73_chr1_3598878_3599018 sep is _chr to get TP73)
	df.to_csv(outputbed, sep='\t', index=False, header=False)

def create_list(txtlistoutput, file_list, ext_list, pattern):
	""" Function to create a txt file from a list of files filtered by extension and pattern """
	with open(txtlistoutput, 'a+') as f:
		for files in file_list:
			for ext in ext_list:
				if pattern in files and files.endswith(ext):
					f.write(str(files) + "\n")

def intersectbed(inputbed, refbed, outputbed):
	""" Function to intersect a bed with a ref bed using bedtools """
	shell("intersectBed -a "+inputbed+" -b "+refbed+" -loj > "+outputbed+" ")

def concatenatelist(inputlist, outputfile):
	""" Function to concatenate lines tab separated in a file """
	shell(" xargs --delimiter='\\n' cat <"+inputlist+" >> "+outputfile+" ")

def keep_unknown(columnname, pattern, prefix):
	""" Function to rename gene """
	i=0
	for j, gene in df[columnname].items():
		if gene == pattern:
			df.at[j, columnname] = prefix+str(i)
		i+=1

def extract_tag(tagfile, tag, tagsep, sep):
	""" Function to extract a tag """
	output_tag = ""
	row = open(tagfile, 'r').readline().strip().split(tagsep)
	for items in row:
		if tag in items and sep in items:
			output_tag = items.split(sep)[-1]
	return(output_tag)

def kmerisation(kmerSize, bedFile, kmerBedFile):
	""" Bed kmerisation """
	count = 0
	print(str(kmerSize) + ' kmerisation of your bed ' + bedFile + ' in progress')
	with open(bedFile, 'r') as readBed:
		with open(kmerBedFile, 'w+') as writeKbed:
			for line in readBed.readlines():
				count += 1
				if line.startswith('#'):
					continue
				else:
					line = line.split()
					diff = int(line[2])-int(line[1])
					chr = line[0]
					start = line[1]
					end = line[2]
					gene = line[3]
					while diff >= kmerSize:
						newEnd = int(start) + int(kmerSize) - 1
						newLine = chr + '\t' + str(start) + '\t' + str(newEnd) + '\t' + gene
						diff = diff - kmerSize
						start = int(newEnd) + 1
						writeKbed.write(newLine + '\n')
					if diff > 0:
						newLine = chr + '\t' + str(start) + '\t' + str(end) + '\t' + gene
						writeKbed.write(newLine + '\n')
	print('Kmerisation done')

### END OF FUNCTIONS ###

# Variables initialisation
# set datetime to add to output file name
date_time = datetime.now().strftime("%Y%m%d-%H%M%S")

# Return last directory of the path run (without /)
# ex == /STARK/output/repository/GROUP/APP/run
nameoftherun = os.path.basename(os.path.normpath(config['run']))

# Set directories to save tempory results and input files
inputdir = "/app/res/" + nameoftherun + "/" + date_time + "/input"
tmpdir = "/app/res/" + nameoftherun + "/" + date_time + "/tmp"

# Get group [4] and app [5] name from run (or -1 and -2 ?)
# run structure is "/STARK/output/repository/group/app/run"
# Default is UNKNOWN in the yaml file
try:
	config['GROUP_NAME'] = os.path.normpath(config['run']).split('/')[4]
	config['APP_NAME'] = os.path.normpath(config['run']).split('/')[5]
except IndexError: pass

# Construct repository and depository dir structure config['OUTPUT_DIR'] will be the default output)
depotdir = config['depository'] + "/" + config['GROUP_NAME'] + "/" + config['APP_NAME']  + "/" + nameoftherun + "/"

if not config['OUTPUT_DIR']:
	config['OUTPUT_DIR'] = config['repository'] + "/" + config['GROUP_NAME'] + "/" + config['APP_NAME']  + "/" + nameoftherun + "/"

# Set log path file
serviceName = config['serviceName']
logfile = tmpdir + "/" + serviceName + "." + date_time + '.parameters.log'

# Set annotation file path
annotation_file = tmpdir + "/" + serviceName + "." + date_time + '.AnnotSV.txt'

# Create directories
os.makedirs(inputdir, exist_ok = True)
os.makedirs(tmpdir, exist_ok = True)
os.makedirs(config['OUTPUT_DIR'], exist_ok = True)
if config['DEPOT_COPY'] == True:
	os.makedirs(depotdir, exist_ok = True)

# Search files in repository 
files_list = searchfiles(os.path.normpath(config['run']), config['SEARCH_ARGUMENT'],  config['RECURSIVE_SEARCH'])

# Create sample and aligner list
sample_list = extractlistfromfiles(files_list, config['PROCESS_FILE'], '.', 0)
aligner_list = extractlistfromfiles(files_list, config['PROCESS_FILE'], '.', 1)

# Exclude samples from the exclude_list
# Case insensitive
for sample_exclude in config['EXCLUDE_SAMPLE']:
	for sample in sample_list:
		if sample.upper().startswith(sample_exclude.upper()):
			sample_list.remove(sample)

# If filter_sample_list variable is not empty, it will force the sample list
if config['FILTER_SAMPLE']:
	sample_list = list(config['FILTER_SAMPLE'])

# For validation analyse bam will be sample.aligner.validation.bam
if config['VALIDATION_ONLY'] == True:
	append_aligner = '.validation'
	aligner_list = [sub + append_aligner for sub in aligner_list]

# Init dictionary
dico_run = {}
for samples in sample_list:
	dico_run[samples] = {}
# Populating dictionary
for samples in sample_list:
	for ext in config['EXT_INDEX_LIST']:
		for files in files_list:
			if os.path.basename(files).split(".")[0] == samples and os.path.basename(files).endswith(ext):
				if config['VALIDATION_ONLY'] == False:
					if ext == config['PROCESS_FILE'] and not 'validation' in files:
						dico_run[samples][ext] = files
					if ext != config['PROCESS_FILE'] and not 'validation' in files:
						dico_run[samples][ext] = files
				if config['VALIDATION_ONLY'] == True:
					if ext == config['PROCESS_FILE'] and 'validation' in files:
						dico_run[samples][ext] = files
					if ext != config['PROCESS_FILE'] and not 'validation' in files:
						dico_run[samples][ext] = files

# Set a filelist with all the files tag
# File format is sample.tag
tagfile_list = []
for sample in sample_list:
	try:
		if dico_run[sample]['.tag']:
			tagfile_list.append(dico_run[sample]['.tag'])
	except KeyError: pass

# Extract the tags from the list
# tag ex SEX#M!POOL#POOL_HFV72AFX3_M_10#POOL_HFV72AFX3_F_11!
if tagfile_list:
	for sample in sample_list:
		for tagfile in tagfile_list:
			output_tag = extract_tag(tagfile,'SEX', '!', '#')
			sample = os.path.basename(tagfile).split(".")[0]
			dico_run[sample]['gender'] = output_tag

# Extract the gender_list from dictionary with the key gender
gender_list = ['A']
try:
	for sample in sample_list:
			if dico_run[sample]['gender'] == 'F':
				gender_list.append('XX')
			if dico_run[sample]['gender'] == 'M':
				gender_list.append('XY')
except KeyError: pass

# Removing duplicate
gender_list = list(set(gender_list))

# Find bed file (Design)
if not config['BED_FILE']:
	config['BED_FILE'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], dico_run, '.design.bed', '.genes.bed')

# Find genes file (Panel)
# Note : we can't use .genes files because .list.genes and .genes are not distinctable from the indexing we made
if not config['GENES_FILE']:
	config['GENES_FILE'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], dico_run, '.genes.bed', '.list.genes')

# Find transcripts files (NM)
if not config['TRANSCRIPTS_FILE']:
	config['TRANSCRIPTS_FILE'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], dico_run, '.transcripts', '.list.transcripts')

# If transcript file exist, create the annotation file for AnnotSV
if os.path.exists(config['TRANSCRIPTS_FILE']) and os.path.getsize(config['TRANSCRIPTS_FILE']) != 0:
	df = pd.read_csv(config['TRANSCRIPTS_FILE'], sep='\t', names=["NM", "Gene"])
	df = df.drop(columns=['Gene'])
	NM_list = df.loc[:,'NM'].tolist()
	with open(annotation_file, 'w+') as f:
		f.write('\t'.join(NM_list))
else:
	with open(annotation_file, 'w') as f:
		f.write("No NM found")

# Find list.genes files 
if not config['LIST_GENES']:
	config['LIST_GENES'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], dico_run, '.list.genes', '.list.transcripts')

### DECON bed ###
# DECON need a 4 columns bed for the ReadInBams rule, if bed is not set correctly ReadInBams will crash
# Chromosome Start End Gene but no header, bed format (tsv)
# Chr must be formated chr1, Gene column must be a string, no negative numbers
# ex	:	chr17	4511	4889	BCRA1
# DECON can also use a bed with the exons number for an accurate formating of pdfs graph (makeCNVcalls) and the IdentifyFailures metrics
# ex	:	chr17	4511	4889	BCRA1	26

inputbed = inputdir+"/"+serviceName+".input.bed"
cat_panels_bed = inputdir+"/"+serviceName+".concatenatepanels.bed"
cat_list_genes = inputdir+"/"+serviceName+".fullpath.genes.list"
cat_noduplicate_panels_bed = inputdir+"/"+serviceName+".concatenatepanelsnoduplicate.bed"
outputbed = inputdir + "/" + serviceName + ".output.bed"
intermediatebed = inputdir + "/" + serviceName + '.intermediate.bed'

# Case if genes file bed don't have gene name & exon number
# We always create a custom exon file from the design bed and intersectBed with a refseqgene database to get the exon number
# Warning the refseqgene database must be generated before
if config['BED_FILE'] and config['BED_PROCESS'] == 'REGEN':
	# Generating the custom.exon file from scratch (intersectBed with refseqgene file)
	df = pd.read_csv(config['BED_FILE'], sep='\t', names=["Chr", "Start", "End"], index_col=False)
	df.to_csv(inputbed, sep='\t', index=False, header=False)
	####### Intersect bed with refseqgene using bedtools #######
	intersectbed(inputbed, config['REFSEQGENE_PATH'], outputbed)
	df = pd.read_csv(outputbed, sep='\t', names=["Chr", "Start", "End", "4", "5", "6", "Tosplit", "Gene", "9"])
	df['Custom.Exon'] = df['Tosplit'].str.split(',').str[-1]
	df['Custom.Exon'] = df['Custom.Exon'].str.replace('exon','')
	# Filter with NM_list if NM_list is not empty (avoid multiple NM/exons for the graphs)
	# Specific for config['BED_PROCESS'] == 'REGEN'
	if os.path.exists(config['TRANSCRIPTS_FILE']) and os.path.getsize(config['TRANSCRIPTS_FILE']) != 0:
		df['NM'] = df['Tosplit'].str.split(',').str[0]
		df = df[df['NM'].isin(NM_list)]
		df = df.drop(columns=['NM'])
	# Formating unknown gene name: intersect will output -1 if no gene name exist, replace with "Unknown"
	if config['KEEP_UNKNOWN'] == True:
		keep_unknown('Gene', '-1', 'Unknown')
	else:
		df = df[df.Gene != '-1']
	if config['GENE_LIST_RESTRICT']:
		df = df[df['Gene'].isin(config['GENE_LIST_RESTRICT'])]
	if config['CHR_LIST_RESTRICT']:
		df = df[-df['Chr'].isin(config['CHR_LIST_RESTRICT'])]
	df = df.drop(columns=['4', '5', '6', 'Tosplit', '9'])
	df = df.drop_duplicates()
	df.to_csv(intermediatebed, sep='\t', index=False, header=False)

# Old bed compatibility
# Intersect bed design with panel intersectBed -a design.bed -b panel.bed -loj > RESULT.bed = columns 1 2 3 and the gene column from the panel is 10
# chr1	65300206	65300354	JAK1_chr1_65300207_65300354	0	+	chr1	65300240	65300344	JAK1_ex25	0	- (gene is JAK1_ex25)
# chr1	3598877	3599018	TP73_chr1_3598878_3599018	0	+	.	-1	-1	.	-1	. (gene is .)
# New bed 13 cols
# Intersect bed design with panel intersectBed -a design.bed -b panel.bed -loj > RESULT.bed = columns 1 2 3 & 10 are retained for the bed, column 13 for the exon number
# ex = chr1	27022390	27024129	NM_006015_ARID1A_5utr-ex1_chr1_27022391_27024129	0	+	.	-1	-1	.	-1	.
if config['BED_FILE'] and config['GENES_FILE'] and config['BED_PROCESS'] == 'STANDARD':
	if config['LIST_GENES']:
		basepath = os. path. dirname(config['LIST_GENES'])
		with open(config['LIST_GENES']) as f:
			cat_panel_list = f.read().splitlines()
			cat_panel_list = [basepath + "/" + s for s in cat_panel_list]
		with open(cat_list_genes, 'w+') as f:
			f.write('\t'.join(cat_panel_list))
		shell(" xargs --delimiter='\\t' cat <"+cat_list_genes+" >> "+cat_panels_bed+" ")
	else:
		shell(" xargs --delimiter='\\t' cat <"+config['GENES_FILE']+" >> "+cat_panels_bed+" ")
	df = pd.read_csv(cat_panels_bed,  sep='\t')
	df = df.drop_duplicates()
	df.to_csv(cat_noduplicate_panels_bed, sep='\t', index=False, header=False)
	####### Intersect bed with genes files using bedtools #######
	intersectbed(config['BED_FILE'], cat_noduplicate_panels_bed, outputbed)
	if config['OLD_BED'] == False:
		df = pd.read_csv(outputbed, sep='\t', names=["Chr", "Start", "End", "4", "5", "6", "7", "8", "9", "Gene", "11", "12", "13", "14", "15", "16", "17", "18", "Custom.Exon"])
		df = df.drop(columns=['4', '5', '6', '7', '8', '9', '11', '12','13', '14', '15', '16', '17', '18'])
	else:
		df = pd.read_csv(outputbed, sep='\t', names=["Chr", "Start", "End", "4", "5", "6", "7", "8", "9", "Gene", "11", "12"])
		df = df.drop(columns=['4', '5', '6', '7', '8', '9', '11', '12'])
	# Keep unknown option
	if config['KEEP_UNKNOWN'] == True:
		keep_unknown('Gene', '.', 'Unknown')
	else:
		df = df[df.Gene != '.']
	# Fix for some bed that remove _ex part if gene is noted genename_ex
	if  config['EXON_SEP'] == True: 
		df['Gene'] = df['Gene'].str.split('_ex').str[0]
	# Option to filter the bed to analyse only certain genes
	if config['GENE_LIST_RESTRICT']:
		df = df[df['Gene'].isin(config['GENE_LIST_RESTRICT'])]
	if config['CHR_LIST_RESTRICT']:
		df = df[-df['Chr'].isin(config['CHR_LIST_RESTRICT'])]
	df = df.drop_duplicates()
	df.to_csv(intermediatebed, sep='\t', index=False, header=False)

if config['BED_FILE'] and config['BED_PROCESS'] == 'NO':
	df = pd.read_csv(config['BED_FILE'], sep='\t', names=["Chr", "Start", "End", "Gene"], index_col=False)
	df.to_csv(intermediatebed, sep='\t', index=False, header=False)

# DECON bed formating from intermediatebed file
deconbed_file = tmpdir + "/" + serviceName + "." + date_time + '.bed'

if config['customexon'] == True:
	df = pd.read_csv(intermediatebed, sep='\t', names=["Chr", "Start", "End", "Gene", "Custom.Exon"], index_col=False)
	df = df.drop_duplicates()
	df.to_csv(deconbed_file, sep='\t', index=False, header=False) # for decon.bed no header
else:
	if config['KMER']:
		kmerisation(config['KMER'], intermediatebed, deconbed_file)
	else:
		df = pd.read_csv(intermediatebed, sep='\t', names=["Chr", "Start", "End", "Gene"], index_col=False)
		df = df.drop_duplicates()
		df.to_csv(deconbed_file, sep='\t', index=False, header=False) # for decon.bed no header

if not config['BED_FILE']:
	logsomefile(logfile, 'No bed found, DECON cannot continue, exiting', "\n")
	exit()

# Separate cram/bam processing
# for cram extract to bam into the input directory
# for bam copy or symlink the bam files to input directory
for sample in sample_list:
	processalignedfiles(inputdir, dico_run, sample, config['PROCESS_FILE'], aligner_list, config['PROCESS_CMD'], name=config['ALIGNER_NAME'])

# Create file_list of bam by gender by searching the input directory, depending on the gender_list
# A = all ; XX = Female only ; XY = Male only
# files_list_A contains the full path of all files (bam files for ex)
# Warning the dictionnary for sexe is A/M/F but the gender_list is A/XY/XX
files_list_A = searchfiles(inputdir, '/*', False)
files_list_XY = []
files_list_XX = []
for files in files_list_A:
	sample = os.path.basename(files).split(".")[0]
	try:
		if dico_run[sample]['gender'] == 'F':
			files_list_XX.append(files)
		if dico_run[sample]['gender'] == 'M':
			files_list_XY.append(files)
	except KeyError: pass

# Creating a txt list for the bam files per aligner per gender (A, M & F)
# the file_list_whatever must be prefiltered
for aligner in aligner_list:
	for gender in gender_list:
		if gender == 'A':
			bamlist = tmpdir + "/" + serviceName + "." + date_time + "." + gender + '.list.txt'
			create_list(bamlist, files_list_A, config['PROCESS_FILE'], aligner)
		if gender == 'XX':
			bamlist = tmpdir + "/" + serviceName + "." + date_time + "." + gender + '.list.txt'
			create_list(bamlist, files_list_XX, config['PROCESS_FILE'], aligner)
		if gender == 'XY':
			bamlist = tmpdir + "/" + serviceName + "." + date_time + "." + gender + '.list.txt'
			create_list(bamlist, files_list_XY, config['PROCESS_FILE'], aligner)

# Option to remove gender in the gender_list to force A, XX or XY analysis only
# the calling of Y for male subject is not done
if config['REMOVE_M'] == True:
	gender_list.remove('XY')
if config['REMOVE_F'] == True:
	gender_list.remove('XX')
if config['REMOVE_A'] == True:
	gender_list.remove('A')

# Log
logsomefile(logfile, 'Start of the analysis:', "\n", items = date_time)
logsomefile(logfile, 'Input for all files:', "\n", items_list = files_list_A)
logsomefile(logfile, 'Input files for male analysis:', "\n", items_list = files_list_XX)
logsomefile(logfile, 'Input files for female analysis:', "\n", items_list = files_list_XY)
logsomefile(logfile, 'List of samples:', "\n", items_list = sample_list)
logsomefile(logfile, 'List of gender:', "\n", items_list = gender_list)
logsomefile(logfile, 'Aligner list:', "\n", items_list = aligner_list)
logsomefile(logfile, 'Analyse run:', "\n", items = nameoftherun)
logsomefile(logfile, 'Design Bed:', "\n", items = config['BED_FILE'])
logsomefile(logfile, 'Panel Bed:', "\n", items = config['GENES_FILE'])
logsomefile(logfile, 'Transcripts list:', "\n", items = config['TRANSCRIPTS_FILE'])
logsomefile(logfile, 'List of genes files:', "\n", items = config['LIST_GENES'])

# Copy2 bed_file & genes_file & transcripts_file (for debug)
if config['DEBUG_MODE']:
	try:
		copy2(config['BED_FILE'], inputdir)
		copy2(config['GENES_FILE'], inputdir)
		copy2(config['TRANSCRIPTS_FILE'], inputdir)
	except FileNotFoundError: pass

################################################## RULES ##################################################
# The input for the rule all is the output of the pipeline
# The expand for the {sample} {aligner} is done only at this step
# Final results will follow this structure in the output directory
# /nameoftherun/SAMPLENAME/ServiceName/ individual results
# /nameoftherun/global results & global logs with ServiceName index
#
# rule all will different depending on the bed files available
# design = .bed, vcf will be vcf.design ; panel = genes, vcf will be vcf.panel ; not bed or genes = vcf will be vcf.full
# warning : bcftools merge will crash if there's only 1 sample, so we need to cp files
############################################################################################################

##### NOTE ####
# Tools that output a file adding a extension should be set with params: params.output as the output of the command without the extension, and output: output.ext exist only to connect rules
# For programs that do not have an explicit log parameter, you may always use 2> {log} to redirect standard output to a file (here, the log file) in Linux-based systems. Note that it is also supported to have multiple (named) log files being specified
# 1 is stdout, 2 is stderr 
##################

ruleorder: indexing > ReadInBams

rule all:
	"""
	Rule will create a design vcf.gz with the gene list file provided
	"""
	input:
		bai = expand(inputdir + "/" + "{sample}.{aligner}.bai", aligner=aligner_list, sample=sample_list),
		failtsv = expand(tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.Metrics.tsv", aligner=aligner_list),
		vcfgzdesign = expand(tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf.gz", aligner=aligner_list, sample=sample_list),
		alltsv = tmpdir + "/" + serviceName + "." + date_time + ".all.AnnotSV.Design.tsv",
		pdf = expand(tmpdir + "/" + serviceName + '.' + date_time + ".{aligner}.{gender}.pdf", aligner=aligner_list, gender=gender_list)

rule help:
	"""
	General help for DECON
	Launch snakemake -s  snakefile_decon -c(numberofthreads) --config DATA_DIR=absolutepathoftherundirectory (default is data) without / at the end of the path
	To launch the snakemake file, use --config to replace variables that must be properly set for the pipeline to work ie run path directory
	Every variable defined in the yaml file can be change
	Separate multiple variable with a space (ex  --config DATA_DIR=runname transProb=0.05 var1=0.05 var2=12)
	Also use option --configfile another.yaml to replace and merge existing config.yaml file variables
	Use -p to display shell commands
	Use --lt to display docstrings of rules
	Input file = bam or cram files (if bai is needed, it will be generate)
	Output file = vcf annoted with AnnotSV 3.x for each sample/bam, and a global vcf file will all the samples ; a set of vcf files by design/panel
	"""

rule indexing:
	"""
	Indexing bam files with samtools
	"""
	input:
		bam = inputdir + "/" + "{sample}.{aligner}.bam"
	output:
		bai = inputdir + "/" + "{sample}.{aligner}.bai"
	params:
		threads = config['THREADS']
	shell:
		"""
		samtools index -b -@ {params.threads} {input.bam} {output.bai}
		"""

# Keep the chr prefix to the Chr list (chr1 and not 1) if your ref genome got chr1 notation (and not 1) if not ReadInBams will crash
rule ReadInBams:
	"""
	DECoN uses a list of BAM files and a BED file of exons to calculate a coverage metric called the fragment per kilobase and million base pairs (FPKM) for each exon specified in the BED file in each sampleâ€™s BAM file
	Rscript ReadInBams.R --bams listofbams.txt --bed file.bed (unique for all bams) --fasta refgenome.fasta --out outputFolder
	Requirement : all .bam files must be indexed
	"""
	input:
		allbamlist = tmpdir + "/" + serviceName + "." + date_time + ".A.list.txt",
		bai = expand(inputdir + "/" + "{sample}.{aligner}.bai", sample=sample_list, aligner=aligner_list)
	output:
		rdata = tmpdir + "/" + serviceName + '.' + date_time + ".{aligner}.ReadInBams.RData"
	params:
		deconbed_file = tmpdir + "/" + serviceName + "." + date_time + ".bed",
		refbamlist = config['REF_BAM_LIST'],
		decondir = config['DECON_PATH'],
		refgene = config['REFGENEFA_PATH']
	log: 
		log1 = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.ReadInBams.log", log2 = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.ReadInBams.err"
	shell:
		"""
		Rscript {params.decondir}/ReadInBams.R --bams {input.allbamlist} --bed {params.deconbed_file} --fasta {params.refgene} --rdata {output.rdata} {params.refbamlist} 1> {log.log1} 2> {log.log2}
		"""

rule IdentifyFailures:
	"""
	The summary .RData file outputted can be used to flag any samples or exons where exon CNV calling may be suboptimal. 
	Both exons and samples are evaluated based on their median coverage level. 
	When coverage is low, accuracy of detection will be compromised and caution should be exercised when interpreting results. 
	Samples are also evaluated based on their correlation with other samples. 
	Samples which do not have a high correlation with other samples in the set are likely to have suboptimal detection across the entire target
	Rscript IdentifyFailures.R --Rdata file.RData --mincorr 0.98 --mincov 100 --exons listofexonsforspecificannotation --out outputfolder
	output is Metrics.tsv ; if custom exon annotation is used, the output is named Metrics_custom.tsv
	"""
	input:
		rdata = tmpdir + "/" + serviceName + '.' + date_time + ".{aligner}.ReadInBams.RData"
	params:
		mincorr = config['mincorr'],
		mincov = config['mincov'],
		analysisfailure = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.MetricsFailed",
		decondir = config['DECON_PATH']
	output:
		failtsv = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.Metrics.tsv"
	log: 
		log1 = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.Metrics.log", log2 = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.Metrics.err"
	shell:
		"""
		Rscript {params.decondir}/IdentifyFailures.R --rdata {input.rdata} --mincorr {params.mincorr} --mincov {params.mincov} --tsv {output.failtsv} 1> {log.log1} 2> {log.log2} && [[ -s {output.failtsv} ]] || touch {params.analysisfailure} ; [[ -s {output.failtsv} ]] || touch {output.failtsv}
		"""

rule makeCNVcalls:
	"""
	This step calls exon CNVs in each sample by selecting reference samples from all other samples contained in the input summary .RData file. 
	The correlation between samples and the number of samples used as a reference are thus calculated and outputted to aid interpretation of call quality
	Rscript makeCNVcalls.R --Rdata file.Rdata --transProb 0.01 (default) --exons bedwithexons.bed --out outputfolder --refbams = txt file containing a list of reference bam files
	output is CNVcall.tsv and a summary Rdata associated ; if custom exon is used, the output is CNVcall_custom.tsv
	"""
	input:
		rdata = tmpdir + "/" + serviceName + '.' + date_time + ".{aligner}.ReadInBams.RData"
	params:
		prob = config['transProb'],
		output = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.Design_results",
		analysisfailure = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.CNVCalledFailed",
		bamlist = tmpdir + "/" + serviceName + "." + date_time + ".{gender}.list.txt",
		chromosome = expand("{gender}", gender=gender_list),
		refbamlist = config['REF_BAM_LIST'],
		decondir = config['DECON_PATH']
	output:
		calltsv = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.Design_results_all.tsv",
		rdata = tmpdir + "/" + serviceName + '.' + date_time + ".{aligner}.{gender}.CNVcalls.RData"
	log: 
		log1 = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.makeCNVcalls.log", log2 = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.makeCNVcalls.err"
	shell:
		"""
		Rscript {params.decondir}/makeCNVcalls.R --rdata {input.rdata} --samples {params.bamlist} --transProb {params.prob} --chromosome {params.chromosome} {params.refbamlist} --tsv {output.calltsv} --outrdata {output.rdata} 1> {log.log1} 2> {log.log2} && [[ -s {output.calltsv} ]] || touch {params.analysisfailure} ; [[ -s {output.calltsv} ]] || touch {output.calltsv}
		"""

rule merge_makeCNVcalls:
	"""
	Merge all CNVs call results
	"""
	input:
		calltsv =  expand(tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.Design_results_all.tsv", gender=gender_list, aligner=aligner_list)
	output:
		allcalltsv = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.Design_results_all.tsv"
	shell:
		"""
		cat {input.calltsv} | sort -u | sort -r >> {output.allcalltsv}
		"""


rule variantconvert:
	input:
		allcalltxt = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.Design_results_all.tsv"
	output:
		vcfunsortall = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.allsamples.Design.unsort.vcf"
	params:
		scriptspath = config['SCRIPTS_PATH'],
		scriptsconfig = config['SCRIPTS_CONFIG']
	log:
		log2 = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.variantconvert.log"
	shell:
		"""
		variantconvert convert -i {input.allcalltxt} -o {output.vcfunsortall} -fi tsv -fo vcf -c {params.scriptsconfig}/config_decon.json 2> {log.log2} && [[ -s {output.vcfunsortall} ]] || cat {params.dummypath}/empty.vcf > {output.output.vcfunsortall}
		"""

# bcftools sort don't sort correctly
rule sortvcfall:
	input:
		vcfunsortall = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.allsamples.Design.unsort.vcf"
	output:
		vcfsortall = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.allsamples.Noannotation.vcf"
	params:
		dummypath = config['DUMMY_PATH']
	shell:
		"""
		grep \"^#\" {input.vcfunsortall} > {output.vcfsortall} && grep -v \"^#\" {input.vcfunsortall} | sort -k1,1V -k2,2g >> {output.vcfsortall} && [[ -s {output.vcfsortall} ]] || cat {params.dummypath}/empty.vcf > {output.vcfsortall}
		"""

# bgzip -c to keep input file
rule vcf2gzall:
	input:
		vcfsortall = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.allsamples.Noannotation.vcf"
	output:
		vcfgzdesign = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.allsamples.Noannotation.vcf.gz"
	params:
		dummypath = config['DUMMY_PATH']
	shell:
		"""
		bgzip -c {input.vcfsortall} > {output.vcfgzdesign} && [[ -s {output.vcfgzdesign} ]] || cat {params.dummypath}/empty.vcf.gz > {output.vcfgzdesign} ; tabix {output.vcfgzdesign}
		"""

# --force-samples will output a vcf file with all annotations but without a sample name if the sample did not exist
# so we output a dummy vcf.gz
rule splitvcf:
	"""
	Split vcf with bcftools
	-s {sample} : comma-separated list of samples to include
	-Oz : output vcf compressed
	-c1 : minimum allele count (INFO/AC) of sites to be printed
	"""
	input:
		vcfgzdesign = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.allsamples.Noannotation.vcf.gz"
	output:
		vcfgzsplit = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.Noannotation.vcf.gz"
	params:
		path = tmpdir + "/{sample}/" + serviceName + "/",
		dummypath = config['DUMMY_PATH']
	shell:
		"""
		mkdir -p {params.path} && bcftools view -c1 -Oz -s {wildcards.sample} -o {output.vcfgzsplit} {input.vcfgzdesign} && [[ -s {output.vcfgzsplit} ]] || cat {params.dummypath}/empty.vcf.gz > {output.vcfgzsplit} ; tabix {output.vcfgzsplit}
		"""

rule AnnotSV:
	"""
	AnnotSV will annotate and rank Structural Variations (SV) from a vcf file. Output will be an AnnotSV vcf file.
	-annotationMode can be : split by exons/introns or full by genes
	-txtFile : path to a file containing a list of preferred genes transcripts to be used in priority during the annotation, preferred genes transcripts names should be tab or space separated
	-genomeBuild must be specified if not hg19
	"""
	input:
		vcf = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.Noannotation.vcf.gz"
	output:
		tsvuncorr = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.AnnotSV.Design_unsort.vcf"
	log:
		txt = tmpdir + "/{sample}/" + serviceName +  "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.AnnotSV.log"
	params:
		genome = config['genomeBuild'],
		overlap = config ['overlap'],
		mode = config['annotationMode'],
		annotation = config['annotationdir']
		# HPO -hpo HP:0001156,HP:0001363,HP:0011304"
	shell:
		"""
		AnnotSV -SVinputFile {input.vcf} -outputFile {output.tsvuncorr} -annotationMode {params.mode} -annotationsDir {params.annotation} -txFile {annotation_file} -genomeBuild {params.genome} -overlap {params.overlap} -vcf 1 > {log.txt} && [[ -s {output.tsvuncorr} ]] || echo "#No data to annotate" > {output.tsvuncorr}
		"""

# bcftools sort don't sort correctly
rule sortvcf:
	"""
	Bash script to sort a vcf
	"""
	input:
		vcfunsort = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.AnnotSV.Design_unsort.vcf"
	output:
		vcfsort = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf"
	params:
		dummypath = config['DUMMY_PATH']
	shell:
		"""
		grep \"^#\" {input.vcfunsort} > {output.vcfsort} && grep -v \"^#\" {input.vcfunsort} | sort -k1,1V -k2,2g >> {output.vcfsort} && [[ -s {output.vcfsort} ]] || cat {params.dummypath}/empty.vcf | sed 's/SAMPLENAME/{wildcards.sample}/g' > {output.vcfsort}
		"""

# bcftools need vcf.gz.tbi index for the merge all
rule vcf2gz:
	"""
	Compress vcf with bgzip
	"""
	input:
		vcfsort = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf"
	output:
		vcfgzdesign = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf.gz"
	shell:
		"""
		bgzip -c {input.vcfsort} > {output.vcfgzdesign} ; tabix {output.vcfgzdesign}
		"""


rule mergedesign:
	"""
	Copy or merge with bcftools several vcfs
	"""
	input:
		vcfgzdesign = expand(tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf.gz", sample=sample_list, aligner=aligner_list)
	output:
		vcfgzalldesign = tmpdir + "/" + serviceName + "." + date_time + ".all.Design.vcf.gz"
	shell:
		"""
		bcftools merge {input.vcfgzdesign} -O z -o {output.vcfgzalldesign} && tabix {output.vcfgzalldesign}
		"""

rule convertvcf_alldesign:
	"""
	Vcf 2 tsv conversion
	"""
	input:
		vcfgzalldesign = tmpdir + "/" + serviceName + "." + date_time + ".all.Design.vcf.gz"
	output:
		tsvalldesign = tmpdir + "/" + serviceName + "." + date_time + ".all.AnnotSV.Design.tsv"
	log:
		log2 = tmpdir + "/" + serviceName + "." + date_time + ".all.AnnotSV.Design.vcf2tsv_converter.log"
	shell:
		"""
		vcf2tsvpy --keep_rejected_calls --input_vcf {input.vcfgzalldesign} --out_tsv {output.tsvalldesign}
		"""

rule plot:
	"""
	This step input the CNVcall.Rdata file to output the plot files in pdf format
	"""
	input:
		rdata = tmpdir + "/" + serviceName + '.' + date_time + ".{aligner}.{gender}.CNVcalls.RData"
	params:
		analysisfailure = tmpdir + "/" + serviceName + '.' + date_time + ".{aligner}.{gender}.pdf",
		folder = tmpdir + "/pdfs/",
		decondir = config['DECON_PATH']
	output:
		pdf = tmpdir + "/" + serviceName + '.' + date_time + ".{aligner}.{gender}.pdf"
	log: 
		log1 = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.plots.log", log2 = tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.{gender}.plots.err"
	shell:
		"""
		Rscript {params.decondir}/DECONplot.R --rdata {input.rdata} --out {params.folder} 1> {log.log1} 2> {log.log2} && [[ -s {output.pdf} ]] || touch {params.analysisfailure}
		"""

onstart:
	shell("touch " + config['OUTPUT_DIR'] + serviceName + "Running.txt")
	# Add the snakemake parameters to log
	with open(logfile, "a+") as f:
		json.dump(config, f)
		f.write("\n")

onsuccess:
	include = config['INCLUDE_RSYNC']
	exclude = config['EXCLUDE_RSYNC']
	
	shell("touch " + config['OUTPUT_DIR'] + serviceName + "Complete.txt")
	shell("rm -f " + config['OUTPUT_DIR'] + serviceName + "Running.txt")
	# Move pdfs file into each sample directory
	pdf_list = os.listdir(tmpdir+"/pdfs")
	for sample in sample_list:
		for pdf in pdf_list:
			if sample in pdf:
				shell("mv "+tmpdir+ "/pdfs/"+pdf+" " +tmpdir+"/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/")
	# rmdir the /pdfs dir
	shell("rm -rf "+tmpdir+"/pdfs")
	# Add end time of the analysis to the log file
	date_time_end = datetime.now().strftime("%Y%m%d-%H%M%S")
	with open(logfile, "a+") as f:
		f.write("End of the analysis : ")
		f.write((date_time_end) + "\n")
	# Remove old files
	for sample in sample_list:
		shell("rm -f " + config['OUTPUT_DIR'] + sample + "/" + serviceName + "/* || true") # if rm stderr, the true will avoid exiting snakemake
	# Rsync all files into final destination
	shell("rsync -azvh --include={include} --exclude {exclude} {tmpdir}/ "+config['OUTPUT_DIR']+"")
	# Copy new files
	for sample in sample_list:
		shell("cp " + config['OUTPUT_DIR'] + sample + "/" + serviceName + "/" + sample + "_" + date_time + "_" + serviceName + "/* " + config['OUTPUT_DIR'] + sample + "/" + serviceName + "/")
	if config['DEPOT_COPY'] == True:
		shell("rsync -azvh --include={include} --exclude {exclude} {tmpdir}/ {depotdir}")
		for sample in sample_list:
			shell("cp " + depotdir + sample + "/" + serviceName + "/" + sample + "_" + date_time + "_" + serviceName + "/* " + depotdir + sample + "/" + serviceName + "/")

onerror:
	include_log = config['INCLUDE_LOG_RSYNC']
	exclude_log = config['EXCLUDE_RSYNC']
	
	shell("touch " + config['OUTPUT_DIR'] + serviceName + "Failed.txt")
	#shell("rm -f " + config['OUTPUT_DIR'] + serviceName + "Running.txt")
	shell("rsync -azvh --include={include_log} --exclude {exclude_log} {tmpdir}/ "+config['OUTPUT_DIR']+"")