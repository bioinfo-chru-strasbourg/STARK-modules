##########################################################################
# Snakemakefile Version:   0.1
# Description:             Snakemake file to run DECoN module (Detection of Exon Copy Number variants)
##########################################################################

# DEV version 0.1 : 10/11/2021
# INT version 0.1 : 17/03/2022
# Authoring : Thomas LAVAUX

################## Context ##################
# launch snakemake -s  snakefile_decon -c(numberofthreads) --config run=absolutepathoftherundirectory without / at the end of the path
# to launch the snakemake file, use --config to replace variables that must be properly set for the pipeline to work ie run path directory
# every variable defined in the yaml file can be change
# separate multiple variable with a space (ex  --config run=runname transProb=0.05 var1=0.05 var2=12)
# also use option --configfile another.yaml to replace and merge existing config.yaml file variables

# use -p to display shell commands
# use --lt to display docstrings of rules
# input file = bam files (if bai is needed, it will be generate)
# output file = vcf annoted with AnnotSV 3.x for each sample/bam, and a global vcf file will all the samples ; a set of vcf files by design/panel

################## Import libraries ##################
import os
import os.path
import glob
import pandas as pd

from datetime import datetime
from shutil import copy2
from shutil import move
################## Configuration file and PATHS ##################
configfile: "/app/config/default.yaml"

### COMMON VARIABLES ###
# Analysis only validation bam/cram files
validation_only = config['VALIDATION_ONLY']

# Debug mode
debug = config['DEBUG_MODE']

# Subdir of module results
serviceName = config['serviceName']

# Datas directory
rundir = config['run']

# Repository/depository directories
repositorydir = config['repository']
depositorydir = config['depository']
outputdir = config['OUTPUT_DIR']

# Copy to depot (archive)
depotcopy = config['DEPOT_COPY']

# Group and app name to construct output
group_name = config['GROUP_NAME']
app_name = config['APP_NAME']

# Design.bed or SAMPLE.bed or SAMPLE.aligner.design.bed in STARK folder
bed_file = config['BED_FILE']
# Panel.SAMPLE.APP.manifest.genes.bed or SAMPLE.APP.manifest.genes.bed in STARK folder
genes_file = config['GENES_FILE']
# Transcripts file containing NM notation SAMPLE.transcripts in STARK folder
transcripts_file = config['TRANSCRIPTS_FILE']
# List genes contains list of gene's files
list_genes = config['LIST_GENES']

# Search options to find files to process
recursive_search = config['RECURSIVE_SEARCH']
search_argument = config['SEARCH_ARGUMENT']

# Sample to remove, set in config_scramble.yaml file
sample_exclude_list = config['EXCLUDE_SAMPLE_LIST']
filter_sample_list = config['FILTER_SAMPLE_LIST']

# Ext to index into dictionary
ext_list = config['EXT_INDEX_LIST']
# Option for processing (cp/ls/samtools)
process_cmd = config['PROCESS_CMD']
# Option for processing bam or cram
process_file = config['PROCESS_FILE']

# Option to append an aligner name to a file (case if the bam don't have an aligner name == sample.bam)
aligner_name = config['ALIGNER_NAME']

# Rsync options for copying results/log
include_file = config['INCLUDE_RSYNC']
exclude_file = config['EXCLUDE_RSYNC']
include_file_log = config['INCLUDE_LOG_RSYNC']

### Variables specific for the tools ###
# RefSeq variable
refseqgene = config['REFSEQGENE_PATH']
# Refgene variable
refgene = config['REFGENEFA_PATH']
# Scripts path
scriptspath = config['SCRIPTS_PATH']

### Variable for DECON ###
# DECON dir for R scripts
decondir = config['DECON_PATH']

### DECON Bed processing ###
# Set DECON to custom exon analysis
custom_exon = config['custom']
# Regen exon number with IntersectBed and a refseqgene reference 
customexon_regen = config['EXON_REGEN']
keep_intergenic = config['KEEP_INTERGENIC']

### FUNCTIONS ###

def finditemindico(sample_list, ext_list, dictionary, include_ext, exclude_ext):
	""" Function to search in a dictionary a file path itering by the first key (sample_list) and second key (extension_list) with an including and excluding filter """
	""" Result must be an non empty file """
	searchresult = ""
	for sample in sample_list:
		for ext in ext_list:
			try:
				items = dictionary[sample][ext]
				if include_ext in items and not exclude_ext in items:
					if os.path.exists(items) and os.path.getsize(items) != 0:
						searchresult = items
			except KeyError: pass
	return searchresult

def searchfiles(directory, search_arg, recursive_arg):
	""" Function to search all files in a directory, addind a search arguement append to the directory and a recursive_search options (True/False) """
	return sorted(filter(os.path.isfile, glob.glob(directory + search_arg, recursive=recursive_arg)))

def processalignedfiles(storedir, dictonnary, sample, ext, aligner_list, process_cmd, name=None):
	""" Function to process bam or cram files who's path are stored in a dictionary by sample/ext name"""
	""" You can either cp, ls bam files ; or use samtools to extract bam from a cram file into a storedir"""
	""" You can append a name to the files if you specify a name """
	inputfile = dictonnary[sample][ext]
	file_without_ext = os.path.splitext((os.path.basename(inputfile)))[0]
	# option to append name file with an aligner name (if file is sample.bam we need to rename it to sample.aligner.bam for the pipeline)
	if name:
		outputfile = storedir + "/" + file_without_ext + name
		aligner_list.append(name)
	else:
		outputfile = storedir + "/" + file_without_ext
	for aligner in aligner_list:
		if aligner in inputfile:
			if process_cmd == 'samtools':
				shell("samtools view -b -T "+refgene+" -o "+outputfile+".bam "+inputfile)	# samtools view -b -T ref_sequence.fa -o sample.bam sample.cram
			if process_cmd == 'cp':
				copy2(inputfile, outputfile+ext)
			if process_cmd == 'ls':
				shell("ln -sfn "+inputfile+" "+outputfile+ext)

def logsomefile(logfile, text, sep, items_list=None, items=None):
	""" Function to log variable value or list values into a log file """
	with open(logfile, 'a+') as f:
		f.write(text + sep)
		if items_list:
			for items in items_list:
				f.write(str(items) + sep)
		else:
			f.write(str(items) + sep)

# structure of bam files are sample.aligner.(validation).bam or sample.archive.cram
# samples name are taken from bam files
# [x] for x part of the filename separated by '.'
# [0] is sample ; [-1] is extension ; [1] is aligner
def extractlistfromfiles(file_list, extension, sep, position):
	""" Function for creating list from a file list, with a specific extension, a separator and the position of the string we want to extract """
	output_list = []
	for files in file_list:
		filesname = os.path.basename(files)
		if filesname.endswith(extension):
			output_list.append(filesname.split(sep)[position])
	output_list = list(set(output_list)) # we don't want duplicate in the list
	return output_list

def deconbedfromdesign(inputbed, outputbed, sepgenecolumn):
	""" Function to extract a DECON bed 4 columns from another bed, assuming that the 4th column contains a gene name with a unique separator """
	df = pd.read_csv(inputbed, sep='\t', names=["Chr", "Start", "End", "Gene"], index_col=False)
	df['Gene'] = df['Gene'].str.split(sepgenecolumn).str[0] # we extract the gene name == string before separator (ex TP73_chr1_3598878_3599018 sep is _chr to get TP73)
	df.to_csv(outputbed, sep='\t', index=False, header=False)

def aligned_list(txtlistoutput, file_list, ext, pattern):
	""" Function to create a txt file from a list of files filtered by extension and pattern """
	with open(txtlistoutput, 'a+') as f:
		for files in file_list:
			if pattern in files and files.endswith(ext):
				f.write(str(files) + "\n")

def makedictionary(dictionary, firstkey_list, secondkey_list, files_list, exclude_pattern, specific_ext, option_validation):
	""" Function to create a dictionary """
	# Init dictionary
	dictionary = {}
	for firstkey in firstkey_list:
		dictionary[firstkey] = {}
	# Populating dictionary
	for firstkey in firstkey_list:
		for secondkey in secondkey_list:
			for files in files_list:
				if os.path.basename(files).startswith(firstkey) and os.path.basename(files).endswith(secondkey):
					if option_validation == False:
						if secondkey == specific_ext and not exclude_pattern in files:
							dico_run[firstkey][secondkey] = files
						if secondkey != specific_ext and not exclude_pattern in files:
							dico_run[firstkey][secondkey] = files
					if option_validation == True:
						if secondkey == specific_ext and exclude_pattern in files:
							dico_run[firstkey][secondkey] = files
						if secondkey != specific_ext and not exclude_pattern in files:
							dico_run[firstkey][secondkey] = files

### END OF FUNCTIONS ###

# Variables initialisation
# set datetime to add to output file name
date_time = datetime.now().strftime("%Y%m%d-%H%M%S")

# Return last directory of the path run (without /)
# ex == /STARK/output/repository/GROUP/APP/run
nameoftherun = os.path.basename(rundir)

# Set directories to save tempory results and input files
inputdir = "/app/res/" + nameoftherun + "/input"
tmpdir = "/app/res/" + nameoftherun + "/tmp"

# Get group [4] and app [5] name from run (or -1 and -2 ?)
# run structure is "/STARK/output/repository/group/app/run"
# Default is UNKNOW in the yaml file
try:
	group_name = rundir.split('/')[4]
	app_name = rundir.split('/')[5]
except IndexError: pass

# Construct repository and depository dir structure (outputdir will be the default output)
depotdir = depositorydir + "/" + group_name + "/" + app_name + "/" + nameoftherun + "/"

if not outputdir:
	outputdir = repositorydir + "/" + group_name + "/" + app_name + "/" + nameoftherun + "/"
 
# Set the path of the customnumberingfile for makeCNVcalls
customnumberingfile = tmpdir + "/" + serviceName + "." +  date_time + '.CustomNumbering.txt'

# Set log path file
logfile = tmpdir + "/" + serviceName + "." + date_time + '.parameters.log'

# Set annotation file path
annotation_file = tmpdir + "/" + serviceName + "." + date_time + '.AnnotSV.txt'

# Set Decon Bed file
deconbed_file = tmpdir + "/" + serviceName + "." + date_time + '.bed'

# Create directories
os.makedirs(inputdir, exist_ok = True)
os.makedirs(tmpdir, exist_ok = True)
os.makedirs(outputdir, exist_ok = True)
if depotcopy == True:
	os.makedirs(depotdir, exist_ok = True)

# Search files in repository 
files_list = searchfiles(rundir, search_argument, recursive_search)

# Create sample and aligner list
sample_list = extractlistfromfiles(files_list, process_file, '.', 0)
aligner_list = extractlistfromfiles(files_list, process_file, '.', 1)

# Exclude samples from the exclude_list
for sample_exclude in sample_exclude_list:
	for sample in sample_list:
		if sample.startswith(sample_exclude):
			sample_list.remove(sample)

# If filter_sample_list variable is not empty, it will force the sample list
if filter_sample_list:
	sample_list = list(filter_sample_list)
# For validation analyse bam will be sample.aligner.validation.bam
if validation_only == True:
	append_aligner = '.validation'
	aligner_list = [sub + append_aligner for sub in aligner_list]

# Init dictionary
dico_run = {}
for samples in sample_list:
	dico_run[samples] = {}
# Populating dictionary
for samples in sample_list:
	for ext in ext_list:
		for files in files_list:
			if os.path.basename(files).startswith(samples) and os.path.basename(files).endswith(ext):
				if validation_only == False:
					if ext == process_file and not 'validation' in files:
						dico_run[samples][ext] = files
					if ext != process_file and not 'validation' in files:
						dico_run[samples][ext] = files
				if validation_only == True:
					if ext == process_file and 'validation' in files:
						dico_run[samples][ext] = files
					if ext != process_file and not 'validation' in files:
						dico_run[samples][ext] = files

# Separate cram/bam processing
# for cram extract to bam into the input directory
# for bam copy or symlink the bam files to input directory
for sample in sample_list:
	processalignedfiles(inputdir, dico_run, sample, process_file, aligner_list, process_cmd, name=aligner_name)

# Create the file_list by searching the depot directory
files_list_depot = searchfiles(inputdir, '/*', False)

# log
logsomefile(logfile, 'Input file:', "\n", items_list = files_list_depot)

# Creating a txt list for the bam files per aligner
for aligner in aligner_list:
	bamlist = tmpdir + "/" + serviceName + "." + date_time + "." + aligner + '.list.txt'
	aligned_list(bamlist, files_list_depot, process_file, aligner)

# Find bed file (Design)
if not bed_file:
	bed_file = finditemindico(sample_list, ext_list, dico_run, '.design.bed', '.genes.bed')

# Find genes file (Panel)
# Note : we can't use .genes files because .list.genes and .genes are not distinctable from the indexing we made
if not genes_file:
	genes_file = finditemindico(sample_list, ext_list, dico_run, '.genes.bed', '.list.genes')

# Find transcripts files (NM)
if not transcripts_file:
	transcripts_file = finditemindico(sample_list, ext_list, dico_run, '.transcripts', '.list.transcripts')

# If transcript file exist, create the annotation file for AnnotSV
if os.path.exists(transcripts_file) and os.path.getsize(transcripts_file) != 0:
	df = pd.read_csv(transcripts_file, sep='\t', names=["NM", "Gene"])
	df = df.drop(['Gene'], 1)
	NM_list = df.loc[:,'NM'].tolist()
	with open(annotation_file, 'w+') as f:
		f.write('\t'.join(NM_list))
else:
	with open(annotation_file, 'w') as f:
		f.write("No NM found")

# Find list.genes files 
if not list_genes:
	list_genes = finditemindico(sample_list, ext_list, dico_run, '.list.genes', '.list.transcripts')

# Transform list_genes into a list if list_genes exist, else use genes_file if exist
# Trick is that we use list .genes to found the .genes.bed files, so we add .bed to the .genes list
if list_genes:
	with open(list_genes) as f:
		panel_list = f.read().splitlines()
elif genes_file and not list_genes:
	panel_list = []
	panel_list.append(os.path.basename(genes_file).split(".bed",1)[0]) # we split the .bed ext because list don't have .bed but genes_file does
# cp files from panel_list to inputdir and rename them
if panel_list:
	for panel in panel_list:
		inputfile = os.path.dirname(list_genes) + "/" + panel + ".bed" # panel_list don't have the bed extension, need that for the copy
		# cut sample. and .genes from file name so files will have the same name as the truncated name list
		panel_trunc = panel.split(".", 1)[1].split(".genes",1)[0]
		outputfile = tmpdir + "/" + panel_trunc
		copy2(inputfile, outputfile)
		# Create a new list for expand, names are filenames without sample and .genes.bed ext
		panel_list_trunc = []
		panel_list_trunc.append(panel_trunc)

### DECON bed ###
# DECON need a 4 columns bed for the ReadInBams rule, if bed is not set correctly ReadInBams will crash
# Chromosome Start End Gene but no header, bed format (tsv)
# Chr must be formated chr1, Gene column must be a string, no negative numbers

### DECON Custom.Exon ###
# DECON can use a customNumbering.txt exons description for the accurate formating of pdfs graph (makeCNVcalls) and the IdentifyFailures rule, file is not mandatory
# Format is tab separated	:	'Chr'	'Start'	'End'	'Gene'	'Custom.Exon'
# exemple					:	17	4511	4889	BCRA1	26
# documentation forgot to mention Gene column

# Case if genes file bed don't have exon number
# We create a custom exon file from the design bed and intersectBed with a refseqgene database to get the exon number
# Warning the refseqgene must be generated before
if bed_file and customexon_regen == True:
	# Generating the custom.exon file from scratch (intersectBed with refgene file)
	df = pd.read_csv(bed_file, sep='\t', names=["Chr", "Start", "End"], index_col=False)
	df.to_csv(inputdir + "/" + serviceName + '.custom.bed', sep='\t', index=False, header=False)
	# Intersect bed with refgene using bedtools
	shell("intersectBed -a "+inputdir+"/"+serviceName+".custom.bed -b "+refseqgene+" -loj > "+inputdir+"/"+serviceName+".customtemp.bed")
	df = pd.read_csv(inputdir + "/" + serviceName + '.customtemp.bed', sep='\t', names=["Chr", "Start", "End", "4", "5", "6", "Tosplit", "Gene", "9"])
	df = df.drop(['4', '5', '6', '9'], 1)
	df['NM'] = df['Tosplit'].str.split(',').str[0]
	df['Custom.Exon'] = df['Tosplit'].str.split(',').str[-1]
	df['Custom.Exon'] = df['Custom.Exon'].str.replace('exon','')
	df['Chr'] = df['Chr'].str.replace('chr','')
	df = df.drop(['Tosplit'], 1)
	# Formating intergenic loc : intersect will output -1 if no NM exist, replace with "Intergenic"
	if keep_intergenic == True:
		df = df.replace('-1', 'Unknown', regex=True)
		df = df.replace('\.', '0', regex=True)
		if os.path.exists(transcripts_file) and os.path.getsize(transcripts_file) != 0: # Filter with NM_list if NM_list is not empty (avoid multiple NM/exons for the graphs)
			Intergenic_number = '0'
			NM_list.append(Intergenic_number)
			df = df[df['NM'].isin(NM_list)]
			df = df.replace('0', '', regex=True) # will replace 0 numeration with NA
	else:
		df = df[df.Gene != '-1'] # Remove row that don't have gene name (intergenic regions are noted -1) if not DECON will crash
		if os.path.exists(transcripts_file) and os.path.getsize(transcripts_file) != 0: # Filter with NM_list if NM_list is not empty (avoid multiple NM/exons for the graphs)
			df = df[df['NM'].isin(NM_list)]
	df = df.drop(['NM'], 1)
	df.to_csv(customnumberingfile, sep='\t', index=False)
	# DECON.bed from custom.exon : if Gene name is missing from bed file, it will be extract from the customnumbering file
	# Assuming that the custom exon file is tab separated	:	'Chr'	'Start'	'End'	'Gene'	'Custom.Exon'
	df = pd.read_csv(customnumberingfile, sep='\t', index_col=False)
	df['Chr'] = 'chr' + df['Chr'].astype(str) # re-add the chr prefix to Chr columns value
	# Save bed file DECON formated
	df = df.drop(['Custom.Exon'], 1)
	df.to_csv(deconbed_file, sep='\t', index=False, header=False)

# Case when we generate a DECON.bed and a custom.exon file with design and panel bed
if bed_file and genes_file and customexon_regen == False:
	# We concatenate all panels
	cat_panels_bed = inputdir+"/"+serviceName+".concatenatepanels.bed"
	cat_list_genes = inputdir+"/"+serviceName+".fullpath.genes.list"
	if list_genes:
		basepath = os. path. dirname(list_genes)
		with open(list_genes) as f:
			cat_panel_list = f.read().splitlines()
			cat_panel_list = [basepath + "/" + s for s in cat_panel_list]
		with open(cat_list_genes, 'w+') as f:
			f.write('\t'.join(cat_panel_list))
		shell(" xargs --delimiter='\\n' cat <"+cat_list_genes+" >> "+cat_panels_bed+" ")
	else:
		shell(" xargs --delimiter='\\n' cat <"+genes_file+" >> "+cat_panels_bed+" ")
	df = pd.read_csv(inputdir + "/" + serviceName + '.concatenatepanels.bed',  sep='\t')
	df = df.drop_duplicates() # remove duplicate
	cat_noduplicate_panels_bed = inputdir+"/"+serviceName+".concatenatepanelsnoduplicate.bed"
	df.to_csv(cat_noduplicate_panels_bed, sep='\t', index=False, header=False)
	# Intersect bed design with panel intersectBed -a design.bed -b panel.bed -loj > RESULT.bed = columns 1 2 3 & 10 are retained for the bed, column 13 for the exon number
	# ex = chr1	27022390	27024129	NM_006015_ARID1A_5utr-ex1_chr1_27022391_27024129	0	+	.	-1	-1	.	-1	.
	shell("intersectBed -a "+bed_file+" -b "+cat_noduplicate_panels_bed+" -loj > "+inputdir+"/"+serviceName+".customtemp.bed")
	# Generate DECON.bed
	df = pd.read_csv(inputdir + "/" + serviceName + '.customtemp.bed', sep='\t', names=["Chr", "Start", "End", "4", "5", "6", "7", "8", "9", "Gene", "11", "12", "Custom.Exon"])
	df = df.drop(['4', '5', '6', '7', '8', '9', '11', '12', "Custom.Exon"], 1)
	df = df.replace('\.', 'Unknown', regex=True)
	df.to_csv(deconbed_file, sep='\t', index=False, header=False)
	# Generate custom numbering file
	df = pd.read_csv(inputdir + "/" + serviceName + '.customtemp.bed', sep='\t', names=["Chr", "Start", "End", "4", "5", "6", "7", "8", "9", "Gene", "11", "12", "Custom.Exon"])
	df = df.drop(['4', '5', '6', '7', '8', '9', '11', '12'], 1)
	df = df[pd.to_numeric(df['Custom.Exon'], errors='coerce').notnull()] # value is NaN if errors
	df.to_csv(customnumberingfile, sep='\t', index=False)

#if bed_file and customexon_regen == False:
#	deconbedfromdesign(bed_file, deconbed_file, '_chr')

if not bed_file:
	logsomefile(logfile, 'No bed found, DECON cannot continue, exiting', "\n")
	exit()

# Log
logsomefile(logfile, 'Start of the analysis:', "\n", items = date_time)
logsomefile(logfile, 'List of samples:', "\n", items_list = sample_list)
logsomefile(logfile, 'Aligner list:', "\n", items_list = aligner_list)
logsomefile(logfile, 'Analyse run:', "\n", items = nameoftherun)
logsomefile(logfile, 'Design Bed:', "\n", items = bed_file)
logsomefile(logfile, 'Panel Bed:', "\n", items = genes_file)
logsomefile(logfile, 'Transcripts list:', "\n", items = transcripts_file)
logsomefile(logfile, 'List of genes files:', "\n", items = list_genes)

# Copy2 bed_file & genes_file & transcripts_file (for debug)
if debug:
	try:
		copy2(bed_file, inputdir)
		copy2(genes_file, inputdir)
		copy2(transcripts_file, inputdir)
	except FileNotFoundError: pass

################################################## RULES ##################################################
# The input for the rule all is the output of the pipeline
# The expand for the {sample} {aligner} is done only at this step
# Final results will follow this structure in the output directory
# /nameoftherun/SAMPLENAME/ServiceName/ individual results
# /nameoftherun/global results & global logs with ServiceName index
#
# rule all will different depending on the bed files available
# design = .bed, vcf will be vcf.design ; panel = genes, vcf will be vcf.panel ; not bed or genes = vcf will be vcf.full
# warning : bcftools merge will crash if there's only 1 sample, so we need to cp files
############################################################################################################

##### NOTE ####
# Tools that output a file adding a extension should be set with params: params.output as the output of the command without the extension, and output: output.ext exist only to connect rules
# For programs that do not have an explicit log parameter, you may always use 2> {log} to redirect standard output to a file (here, the log file) in Linux-based systems. Note that it is also supported to have multiple (named) log files being specified
# 1 is stdout, 2 is stderr 
##################

ruleorder: indexing > ReadInBams

if genes_file:
	rule all:
		"""
		Rule will create a design and panel vcf.gz if a gene list file is provided, else a design vcf.gz only
		"""
		input:
			bai = expand(inputdir + "/" + "{sample}.{aligner}.bai", aligner=aligner_list, sample=sample_list),
			failtxt = tmpdir + "/" + serviceName + "." + date_time + "_Failures.txt",
			vcfgzpanel = expand(tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Panel.{panel}.vcf.gz", aligner=aligner_list, sample=sample_list, panel=panel_list_trunc),
			vcfgzallpanel = expand(tmpdir + "/" + serviceName + "." + date_time + ".allsamples.Panel.{panel}.vcf.gz", panel=panel_list_trunc)

else:
	rule all:
		"""
		Rule will create a design and panel vcf.gz if a gene list file is provided, else a design vcf.gz only
		"""
		input:
			bai = expand(inputdir + "/" + "{sample}.{aligner}.bai", aligner=aligner_list, sample=sample_list),
			failtxt = tmpdir + "/" + serviceName + "." + date_time + "_Failures.txt",
			vcfgzdesign = expand(tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf.gz", aligner=aligner_list, sample=sample_list),
			vcfgzalldesign = tmpdir + "/" + serviceName + "." + date_time + ".allsamples.Design.vcf.gz"

rule help:
	"""
	General help for DECON
	Launch snakemake -s  snakefile_decon -c(numberofthreads) --config DATA_DIR=absolutepathoftherundirectory (default is data) without / at the end of the path
	To launch the snakemake file, use --config to replace variables that must be properly set for the pipeline to work ie run path directory
	Every variable defined in the yaml file can be change
	Separate multiple variable with a space (ex  --config DATA_DIR=runname transProb=0.05 var1=0.05 var2=12)
	Also use option --configfile another.yaml to replace and merge existing config.yaml file variables
	Use -p to display shell commands
	Use --lt to display docstrings of rules
	Input file = bam files (if bai is needed, it will be generate)
	Output file = vcf annoted with AnnotSV 3.x for each sample/bam, and a global vcf file will all the samples ; a set of vcf files by design/panel
	"""

rule indexing:
	"""
	Indexing bam files with samtools
	"""
	input:
		bam = inputdir + "/" + "{sample}.{aligner}.bam"
	output:
		bai = inputdir + "/" + "{sample}.{aligner}.bai"
	params:
		threads = config['THREADS']
	shell:
		"""
		samtools index -b -@ {params.threads} {input.bam} {output.bai}
		"""

# Keep the chr prefix to the Chr list (chr1 and not 1) if your ref genome got chr1 notation (and not 1) if not ReadInBams will crash
# ReadInBams must run R in the decondir directory to be able to load packrat
rule ReadInBams:
	"""
	DECoN uses a list of BAM files and a BED file of exons to calculate a coverage metric called the fragment per kilobase and million base pairs (FPKM) for each exon specified in the BED file in each sample’s BAM file
	Rscript ReadInBams.R --bams listofbams.txt --bed file.bed (unique for all bams) --fasta refgenome.fasta --out output.(prefix).RData
	Requirement : all .bam files must have a .bai file associated with in the same folder
	"""
	input:
		bamlist = expand(tmpdir + "/" + serviceName + "." + date_time + ".{aligner}.list.txt", aligner=aligner_list)
	output:
		rdata = tmpdir + "/" + serviceName + '.' + date_time + ".RData"
	params:
		output = tmpdir + "/" + serviceName + '.' + date_time
	log: 
		log1 = tmpdir + "/" + serviceName + "." + date_time + ".ReadInBams.log", log2 = tmpdir + "/" + serviceName + "." + date_time + ".ReadInBams.err"
	shell:
		"""
		cd {decondir} && Rscript ReadInBams.R --bams {input.bamlist} --bed {deconbed_file} --fasta {refgene} --out {params.output} 1> {log.log1} 2> {log.log2}
		"""

# makeCNVcalls must run R in the decondir directory to be able to load packrat
# We add a if then condition because --custom FALSE will not work if we provide a --exons file
rule makeCNVcalls:
	"""
	This step calls exon CNVs in each sample by selecting reference samples from all other samples contained in the input summary .RData file. The correlation between samples and the number of samples used as a reference are thus calculated and outputted to aid interpretation of call quality
	Rscript makeCNVcalls.R --Rdata summary.file --transProb 0.01 (default) --exons listofexonsforspecificannotation --custom TRUE/FALSE --out output.prefix –-plot  None, Custom or All --plotFolder Plotfoldertoset
	output is _results_all.txt and a summary Rdata associated
	"""
	input:
		rdata = tmpdir + "/" + serviceName + '.' + date_time + ".RData"
	params:
		prob = config['transProb'],
		pdf = tmpdir + "/pdfs/",
		exons = customnumberingfile,
		plot = config['plot'],
		customplot = config['custom'],
		output = tmpdir + "/" + serviceName + "." + date_time + ".Design"
	output:
		txt = tmpdir + "/" + serviceName + "." + date_time + ".Design_results_all.txt",
	log: 
		log1 = tmpdir + "/" + serviceName + "." + date_time + ".makeCNVcalls.log", log2 = tmpdir + "/" + serviceName + "." + date_time + ".makeCNVcalls.err"
	shell:
		"""
		if [ {params.customplot} ];
		then
			cd {decondir} && Rscript makeCNVcalls.R --Rdata {input.rdata} –-plot {params.plot} --exons {params.exons} --custom {params.customplot} --transProb {params.prob} --plotFolder {params.pdf} --out {params.output} 1> {log.log1} 2> {log.log2}
		else
			cd {decondir} && Rscript makeCNVcalls.R --Rdata {input.rdata} –-plot {params.plot} --transProb {params.prob} --plotFolder {params.pdf} --out {params.output} 1> {log.log1} 2> {log.log2}
		fi
		"""

# IdentifyFailures must run R in the decondir directory to be able to load packrat
# We add a if then condition because --custom FALSE will not work if we provide a --exons file
rule IdentifyFailures:
	"""
	The summary .RData file outputted can be used to flag any samples or exons where exon CNV calling may be suboptimal. Both exons and samples are evaluated based on their median coverage level. When coverage is low, accuracy of detection will be compromised and caution should be exercised when interpreting results. Samples are also evaluated based on their correlation with other samples. Samples which do not have a high correlation with other samples in the set are likely to have suboptimal detection across the entire target
	Rscript IdentifyFailures.R --Rdata file.RData --mincorr .98 --mincov 100 --exons listofexonsforspecificannotation --custom TRUE/FALSE --out output.prefix
	output is _coverage_Failures.txt ; if --custom is true file name is _custom_Failures.txt
	"""
	input:
		rdata = tmpdir + "/" + serviceName + '.' + date_time + ".RData"
	params:
		exons = customnumberingfile,
		mincorr = config['mincorr'],
		mincov = config['mincov'],
		customplot = config['custom'],
		output = tmpdir + "/" + serviceName + "." + date_time + ".Identify" # DECON.datetime.Identify_Failures.txt
	output:
		failtxt = tmpdir + "/" + serviceName + "." + date_time + "_Failures.txt"
	log: 
		log1 = tmpdir + "/" + serviceName + "." + date_time + ".Identify_Failures.log", log2 = tmpdir + "/" + serviceName + "." + date_time + ".Identify_Failures.err"
	shell:
		"""
		if [ {params.customplot} ];
		then
			cd {decondir} && Rscript IdentifyFailures.R --Rdata {input.rdata} --exons {params.exons} --custom {params.customplot} --mincorr {params.mincorr} --mincov {params.mincov} --out {params.output} 1> {log.log1} 2> {log.log2} && [[ -s {params.output}_Failures.txt ]] || echo "IdentityFailure failed or no datas are suboptimal ; check .err file for infos" > {output.failtxt}_Failures.txt
		else
			cd {decondir} && Rscript IdentifyFailures.R --Rdata {input.rdata} --mincorr {params.mincorr} --mincov {params.mincov} --out {params.output} 1> {log.log1} 2> {log.log2} && [[ -s {params.output}_Failures.txt ]] || echo "IdentityFailure failed or no datas are suboptimal ; check .err file for infos" > {params.output}_Failures.txt
		fi
		"""

rule Decon_tsv2vcf:
	input:
		txt = tmpdir + "/" + serviceName + "." + date_time + ".Design_results_all.txt"
	output:
		vcfunsort = tmpdir + "/" + serviceName + "." + date_time + ".allsamples.Design.unsort.vcf"
	log:
		log1 = tmpdir + "/" + serviceName + "." + date_time + ".DeconTsv2Vcf.log"
	shell:
		"""
		python3.9 {scriptspath}/file_converter.py -i {input.txt} -o {output.vcfunsort} -fi tsv -fo vcf -c {scriptspath}/fileconversion/config_decon.json 2> {log.log1} && [[ -s {output.vcfunsort} ]] || cat {scriptspath}/empty.vcf > {output.vcfunsort} 
		"""

# bcftools sort don't sort correctly
rule sortvcf:
	input:
		vcfunsort = tmpdir + "/" + serviceName + "." + date_time + ".allsamples.Design.unsort.vcf"
	output:
		vcfsort = tmpdir + "/" + serviceName + "." + date_time + ".allsamples.Design.vcf"
	shell:
		"""
		grep \"^#\" {input.vcfunsort} > {output.vcfsort} && grep -v \"^#\" {input.vcfunsort} | sort -k1,1V -k2,2g >> {output.vcfsort} && [[ -s {output.vcfsort} ]] || cat {scriptspath}/empty.vcf > {output.vcfsort}
		"""

# bgzip -c to keep input file
rule vcf2gz:
	input:
		vcfsort = tmpdir + "/" + serviceName + "." + date_time + ".allsamples.Design.vcf"
	output:
		vcfgzdesign = tmpdir + "/" + serviceName + "." + date_time + ".allsamples.Design.vcf.gz"
	shell:
		"""
		bgzip -c {input.vcfsort} > {output.vcfgzdesign} && [[ -s {output.vcfgzdesign} ]] || cat {scriptspath}/empty.vcf.gz > {output.vcfgzdesign} ; tabix {output.vcfgzdesign} 
		"""

rule filtervcfall:
	input:
		vcfgzdesign = tmpdir + "/" + serviceName + "." + date_time + ".allsamples.Design.vcf.gz"
	output:
		vcfgzallpanel = tmpdir + "/" + serviceName + "." + date_time + ".allsamples.Panel.{panel}.vcf.gz"
	params:
		genes = expand(tmpdir + "/{panel}" , panel=panel_list_trunc)
	shell:
		"""
		bcftools view {input.vcfgzdesign} -R {params.genes} -O z -o {output.vcfgzallpanel} && tabix {output.vcfgzallpanel} && [[ -s {output.vcfgzallpanel} ]] || cat {scriptspath}/empty.vcf.gz > {output.vcfgzallpanel}
		"""

# --force-samples will output a vcf file with all annotations but without a sample name if the sample did not exist
# so we output a dummy vcf.gz
rule splitvcf:
	"""
	Split vcf with bcftools
	-s {sample} : comma-separated list of samples to include
	-Oz : output vcf compressed
	-c1 : minimum allele count (INFO/AC) of sites to be printed
	"""
	input:
		vcfgzdesign = tmpdir + "/" + serviceName + "." + date_time + ".allsamples.Design.vcf.gz"
	output:
		vcfgzsplit = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.Noannotation.vcf.gz"
	params:
		path = tmpdir + "/{sample}/" + serviceName + "/"
	shell:
		"""
		mkdir -p {params.path} && bcftools view -c1 -Oz -s {wildcards.sample} -o {output.vcfgzsplit} {input.vcfgzdesign} && [[ -s {output.vcfgzsplit} ]] || cat {scriptspath}/empty.vcf.gz > {output.vcfgzsplit}  ; tabix {output.vcfgzsplit}
		"""

# AnnotSV need bedtools to work ie you have to specify the directory of bedtools binaries in -bedtools argument
# add -hpo {params.hpofile} if hpofile exist
rule AnnotSV:
	"""
	AnnotSV will annotate and rank Structural Variations (SV) from a vcf file. Output will be an AnnotSV tsv file.
	-annotationMode can be : split by exons/introns or full by genes
	-txtFile : path to a file containing a list of preferred genes transcripts to be used in priority during the annotation, preferred genes transcripts names should be tab or space separated
	-genomeBuild must be specified if not hg19
	"""
	input:
		vcf = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.Noannotation.vcf.gz"
	output:
		tsv = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.tsv"
	log:
		txt = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.log"
	params:
		bedtools = config['BEDTOOLS_PATH'],
		bcftools = config['BCFTOOLS_PATH'],
		genome = config['genomeBuild'],
		overlap = config ['overlap'],
		mode = config['annotationMode']
	shell:
		"""
		AnnotSV -SVinputFile {input.vcf} -outputFile {output.tsv} -bedtools {params.bedtools} -annotationMode {params.mode} -txFile {annotation_file} -genomeBuild {params.genome} -overlap {params.overlap} > {log.txt} && [[ -s {output.tsv} ]] || echo "No data to annotate" > {output.tsv}
		"""

rule AnnotSV2vcf:
	input:
		tsv = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.tsv"
	output:
		vcfanot = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf"
	log:
		log1 = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.AnnotSV2vcf.log"
	shell:
		"""
		python3.9 {scriptspath}/file_converter.py  -i {input.tsv} -o {output.vcfanot} -fi annotsv -fo vcf -c {scriptspath}/fileconversion/config_annotsv3.json 2> {log.log1} && [[ -s {output.vcfanot} ]] || echo "No data to annotate" > {output.vcfanot}
		"""

rule vcf2gzsample:
	input:
		vcfanot = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf"
	output:
		vcfgzdesign = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf.gz"
	shell:
		"""
		bcftools sort {input.vcfanot} | bgzip > {output.vcfgzdesign} && [[ -s {output.vcfgzdesign} ]] || cat {scriptspath}/empty.vcf.gz > {output.vcfgzdesign} ; tabix {output.vcfgzdesign}
		"""

rule filtervcfpanel:
	input:
		vcfgzdesign = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf.gz"
	output:
		vcfgzpanel = tmpdir + "/{sample}/" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Panel.{panel}.vcf.gz"
	params:
		genes = expand(tmpdir + "/{panel}", panel=panel_list_trunc)
	shell:
		"""
		bcftools view {input.vcfgzdesign} -R {params.genes} -O z -o {output.vcfgzpanel} && [[ -s {output.vcfgzpanel} ]] || cat {scriptspath}/empty.vcf.gz > {output.vcfgzpanel} ;  tabix {output.vcfgzpanel} 
		"""

onstart:
	shell("touch " + outputdir + "/" + serviceName + "Running.txt")
	# Add the snakemake parameters to log
	with open(logfile, "a+") as f:
		json.dump(config, f)
		f.write("\n")

onsuccess:
	shell("touch " + outputdir + "/" + serviceName + "Complete.txt")
	shell("rm " + outputdir + "/" + serviceName + "Running.txt")
	# Sort pdfs file into each sample directory
	pdf_list = os.listdir(tmpdir+"/pdfs")
	for sample in sample_list:
		for pdf in pdf_list:
			if sample in pdf:
				# Change directoy for destination
				# Structure of output is /SAMPLE/ServiceName/pdfs 
				shell("mkdir -p " + outputdir + "/" + sample + "/" + serviceName + "/pdfs")
				move(tmpdir+ "/pdfs/"+pdf, outputdir+"/" + sample + "/"  + serviceName +"/pdfs/")
	# if move rmdir the /pdfs dir
	try:
		shell("rmdir "+tmpdir+"/pdfs")
	except subprocess.CalledProcessError: pass
	# Add end time of the analysis to the log file
	date_time_end = datetime.now().strftime("%Y%m%d-%H%M%S")
	with open(logfile, "a+") as f:
		f.write("End of the analysis : ")
		f.write((date_time_end) + "\n")
	# Copy files to final destination
	shell("rsync -azvh --include={include_file} --exclude {exclude_file} {tmpdir}/ {outputdir}")
		
	if depotcopy == True:
		shell("rsync -azvh --include={include_file} --exclude {exclude_file} {tmpdir}/ {depotdir}")

onerror:
	shell("touch " + outputdir + "/" + serviceName + "Failed.txt")
	shell("rm " + outputdir + "/" + serviceName + "Running.txt")
	shell("rsync -azvh --include={include_file_log} --exclude {exclude_file} {tmpdir}/ {outputdir}")