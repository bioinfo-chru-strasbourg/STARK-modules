##########################################################################
# Snakemakefile Version:   3
# Description:             Snakemake file to run DECoN module (Detection of Exon Copy Number variants)
##########################################################################

# DEV version 0.1 : 10/11/2021
# INT version 0.1 : 17/03/2022
# PROD version 1 : 03/06/2022
# Authoring : Thomas LAVAUX

# PROD version 2 : 14/10/2022 changelog
	# add the possibility to analyse gender separately, extract gender from tag files ; gender analysis can be set with REMOVE_F & REMOVE_M & REMOVE_A options
	# add a gene list restriction to limit the analysis to a certain list of gene's name (GENE_LIST_RESTRICT)
	# rewrite DECON bed / custom numbering formating
	# remove panel vcf filtering (output was essentially empty)
	# add a chr filter option to remove chromosome (essentially chrY) (CHR_LIST_RESTRICT)
	# add a chrX Male gender analysis only to avoid chrY failure (REMOVE_M_noY)
	# rename failures and results_all from txt to tsv
	# exclude samples list case insensitive
	# copying new analysis per sample in the root sample dir & removing the old ones
	# add vcf2tsv converter ((https://github.com/sigven/vcf2tsvpy) & corresponding rules : each vcf will be convert to tsv
	# correct run path by removing ending '/' if exist
	# keep "Running.txt" file if failed, avoiding multiple analysis launch by the listener

# PROD version 3 : 19/09/2023 changelog
	# AnnotSV version 3.3.6 (include the vcf converter)
	# more options to process DECON bed, update the processalignedfiles function
	# R scripts rewrite, to speed up ReadInBam, separate plotting, call CNV with a reference bam list (see changlog in the R scripts)
	# re-arrange code for snakemake a lot

################## Context ##################
# launch snakemake -s  snakefile_decon -c(numberofthreads) --config run=absolutepathoftherundirectory without / at the end of the path
# to launch the snakemake file, use --config to replace variables that must be properly set for the pipeline to work ie run path directory
# every variable defined in the yaml file can be change
# separate multiple variable with a space (ex  --config run=runname transProb=0.05 var1=0.05 var2=12)
# also use option --configfile another.yaml to replace and merge existing config.yaml file variables

# use -p to display shell commands
# use --lt to display docstrings of rules
# input file = bam files (if bai is needed, it will be generate)
# output file = vcf annoted with AnnotSV 3.x for each sample/bam, and a global vcf file will all the samples ; a set of vcf files by design/panel

################## Import libraries ##################
import os
import os.path
import glob
import pandas as pd
import json
import csv

from datetime import datetime
from shutil import copy2

################## Configuration file ##################
configfile: "/app/config/snakefile/default.yaml"

####################### FUNCTIONS ######################
def parse_samplesheet(samplesheet_path):
	"""
	samplesheet_path: absolute path of a samplesheet file, Illumina format
	return: a dataframe containing 9 columns :
	Sample_ID,Sample_Plate,Sample_Well,I7_Index_ID,index,Manifest,GenomeFolder,Sample_Project,Description
	The description field contains tag separated by ! ; the name of the tag and the value is separated by # (ex : SEX#F!APP#DIAG.BBS_RP)
	"""
	samplesheet_data = []
	samplesheet_header = []
	with open(samplesheet_path, 'r') as f:
		v = False
		for lines in f:
			lines = lines.strip()
			if v:
				samplesheet_data.append(lines.split(','))
			if 'Sample_ID' in lines:
				v = True
				samplesheet_header.append(lines.split(','))
	df = pd.DataFrame(samplesheet_data, columns=samplesheet_header)
	sample_list = df.iloc[:, 0].tolist()
	return df

def getSampleInfos(samplesheet_path, dictionary):
	"""
	samplesheet_path: absolute path of a samplesheet file, Illumina format
	return a dictionnary with Sample_ID from samplesheet as key and 'gender': 'F or M or NULL'
	"""
	dictionary = {}
	if samplesheet_path:
		parse_samplesheet(samplesheet_path)
		for i, rows in parse_samplesheet(samplesheet_path).iterrows():
			if not any(exclude in rows['Sample_ID'] for exclude in config['EXCLUDE_SAMPLE']):
				sampleID = rows["Sample_ID"]
				dictionary[sampleID] = {}
				tags = rows['Description'].split('!')
				for tag in tags:
					if 'SEX' in tag and '_' in tag:
						tag = tag.split('_')[-1]
						dictionary[sampleID]['gender'] = tag
						break
					elif 'SEX' in tag and '#' in tag:
						tag = tag.split('#')[-1]
						dictionary[sampleID]['gender'] = tag
						break
					else:
						tag = ''
						dictionary[sampleID]['gender'] = tag
	return dictionary

def finditemindico(sample_list, ext_list, dictionary, include_ext, exclude_ext):
	""" Function to search in a dictionary a file path itering by the first key (sample_list) and second key (extension_list) with an including and excluding filter """
	""" Result must be an non empty file """
	searchresult = ""
	for sample in sample_list:
		for ext in ext_list:
			try:
				items = dictionary[sample][ext]
				if include_ext in items and not exclude_ext in items:
					if os.path.exists(items) and os.path.getsize(items) != 0:
						searchresult = items
			except KeyError: pass
	return searchresult

def searchfiles(directory, search_arg, recursive_arg):
	""" Function to search all files in a directory, adding a search arguement append to the directory and a recursive_search options (True/False) """
	return sorted(filter(os.path.isfile, glob.glob(directory + search_arg, recursive=recursive_arg)))

def logsomefile(logfile, text, sep, items_list=None, items=None):
	""" Function to log a variable value or a list of values into a log file """
	with open(logfile, 'a+') as f:
		f.write(text + sep)
		if items_list:
			for items in items_list:
				if items == '':
					f.write(str('None') + sep)
				else:
					f.write(str(items) + sep)
		else:
			if items == '':
				f.write(str('None') + sep)
			else:
				f.write(str(items) + sep)

def extractlistfromfiles(file_list, ext_list, sep, position):
	""" Function for creating list from a file list, with a specific extension, a separator and the position of the string we want to extract """
	output_list = []
	for files in file_list:
		filesname = os.path.basename(files)
		for ext in ext_list:
			if filesname.endswith(ext):
				output_list.append(filesname.split(sep)[position])
	output_list = list(set(output_list))
	return output_list

def deconbedfromdesign(inputbed, outputbed, sepgenecolumn):
	""" Function to extract a DECON bed 4 columns from another bed, assuming that the 4th column contains a gene name with a unique separator """
	df = pd.read_csv(inputbed, sep='\t', names=["Chr", "Start", "End", "Gene"], index_col=False)
	df['Gene'] = df['Gene'].str.split(sepgenecolumn).str[0] # we extract the gene name == string before separator (ex TP73_chr1_3598878_3599018 sep is _chr to get TP73)
	df.to_csv(outputbed, sep='\t', index=False, header=False)

def create_list(txtlistoutput, file_list, ext_list, pattern):
	""" Function to create a txt file from a list of files filtered by extension and pattern """
	with open(txtlistoutput, 'a+') as f:
		for files in file_list:
			for ext in ext_list:
				if pattern in files and files.endswith(ext):
					f.write(str(files) + "\n")

def intersectbed(inputbed, refbed, outputbed):
	""" Function to intersect a bed with a ref bed using bedtools """
	shell("intersectBed -a "+inputbed+" -b "+refbed+" -loj > "+outputbed+" ")

def concatenatelist(inputlist, outputfile):
	""" Function to concatenate lines tab separated in a file """
	shell(" xargs --delimiter='\\n' cat <"+inputlist+" >> "+outputfile+" ")

def keep_unknown(columnname, pattern, prefix):
	""" Function to rename gene """
	i=0
	for j, gene in df[columnname].items():
		if gene == pattern:
			df.at[j, columnname] = prefix+str(i)
		i+=1

def extract_tag(tagfile, tag, tagsep, sep):
	""" Function to extract a tag """
	output_tag = ""
	row = open(tagfile, 'r').readline().strip().split(tagsep)
	for items in row:
		if tag in items and sep in items:
			output_tag = items.split(sep)[-1]
	return(output_tag)

def kmerisation(kmerSize, bedFile, kmerBedFile):
	""" Bed kmerisation """
	count = 0
	print(str(kmerSize) + ' kmerisation of your bed ' + bedFile + ' in progress')
	with open(bedFile, 'r') as readBed:
		with open(kmerBedFile, 'w+') as writeKbed:
			for line in readBed.readlines():
				count += 1
				if line.startswith('#'):
					continue
				else:
					line = line.split()
					diff = int(line[2])-int(line[1])
					chr = line[0]
					start = line[1]
					end = line[2]
					gene = line[3]
					while diff >= kmerSize:
						newEnd = int(start) + int(kmerSize) - 1
						newLine = chr + '\t' + str(start) + '\t' + str(newEnd) + '\t' + gene
						diff = diff - kmerSize
						start = int(newEnd) + 1
						writeKbed.write(newLine + '\n')
					if diff > 0:
						newLine = chr + '\t' + str(start) + '\t' + str(end) + '\t' + gene
						writeKbed.write(newLine + '\n')
	print('Kmerisation done')

### END OF FUNCTIONS ###
serviceName = config['serviceName']
date_time = datetime.now().strftime("%Y%m%d-%H%M%S")
runName = os.path.basename(os.path.normpath(config['run']))
inputDir = "/app/res/"+runName+"/"+ date_time+"/input"
tmpDir = "/app/res/"+runName+"/"+date_time+"/tmp"
try:
	config['GROUP_NAME'] = os.path.normpath(config['run']).split('/')[4]
	config['APP_NAME'] = os.path.normpath(config['run']).split('/')[5]
except IndexError: pass
depotdir = config['depository'] + "/" + config['GROUP_NAME'] + "/" + config['APP_NAME']  + "/" + runName + "/"
if not config['OUTPUT_DIR']:
	config['OUTPUT_DIR'] = config['run']
logfile = tmpDir+"/"+serviceName+"."+date_time+'.parameters.log'
annotation_file = tmpDir+"/"+serviceName+"."+date_time+'.AnnotSV.txt'
os.makedirs(inputDir, exist_ok = True)
os.makedirs(tmpDir, exist_ok = True)
os.makedirs(config['OUTPUT_DIR'], exist_ok = True)
if config['DEPOT_COPY'] == True:
	os.makedirs(depotdir, exist_ok = True)

# Search files in repository 
files_list = searchfiles(os.path.normpath(config['run']), config['SEARCH_ARGUMENT'],  config['RECURSIVE_SEARCH'])

# Create sample and aligner list
sample_list = extractlistfromfiles(files_list, config['PROCESS_FILE'], '.', 0)
aligner_list = extractlistfromfiles(files_list, config['PROCESS_FILE'], '.', 1)

# Exclude samples from the exclude_list , case insensitive
for sample_exclude in config['EXCLUDE_SAMPLE']:
	for sample in sample_list:
		if sample.upper().startswith(sample_exclude.upper()):
			sample_list.remove(sample)

# If filter_sample_list variable is not empty, it will force the sample list
if config['FILTER_SAMPLE']:
	sample_list = list(config['FILTER_SAMPLE'])

# For validation analyse bam will be sample.aligner.validation.bam, so we append .validation to all the aligner strings
if config['VALIDATION_ONLY'] == True:
	append_aligner = '.validation'
	aligner_list = [sub + append_aligner for sub in aligner_list]

runDict = {}
for samples in sample_list:
	runDict[samples] = {}
for samples in sample_list:
	for ext in config['EXT_INDEX_LIST']:
		for files in files_list:
			if os.path.basename(files).split(".")[0] == samples and os.path.basename(files).endswith(ext):
				if config['VALIDATION_ONLY'] == False:
					if ext == config['PROCESS_FILE'] and not 'validation' in files:
						runDict[samples][ext] = files
					if ext != config['PROCESS_FILE'] and not 'validation' in files:
						runDict[samples][ext] = files
				if config['VALIDATION_ONLY'] == True:
					if ext == config['PROCESS_FILE'] and 'validation' in files:
						runDict[samples][ext] = files
					if ext != config['PROCESS_FILE'] and not 'validation' in files:
						runDict[samples][ext] = files

# Set a filelist with all the files tag ; file format is sample.tag
tagfile_list = []
for sample in sample_list:
	try:
		if runDict[sample]['.tag']:
			tagfile_list.append(runDict[sample]['.tag'])
	except KeyError: pass

# Extract the tags from the list
# tag ex SEX#M!POOL#POOL_HFV72AFX3_M_10#POOL_HFV72AFX3_F_11!
if tagfile_list:
	for sample in sample_list:
		for tagfile in tagfile_list:
			output_tag = extract_tag(tagfile,'SEX', '!', '#')
			sample = os.path.basename(tagfile).split(".")[0]
			runDict[sample]['gender'] = output_tag

# Extract the gender_list from dictionary with the key gender
gender_list = ['A']
try:
	for sample in sample_list:
			if runDict[sample]['gender'] == 'F':
				gender_list.append('XX')
			if runDict[sample]['gender'] == 'M':
				gender_list.append('XY')
except KeyError: pass

# Removing duplicate
gender_list = list(set(gender_list))

# Find bed file (Design)
if not config['BED_FILE']:
	config['BED_FILE'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], runDict, '.design.bed', '.genes.bed')
# Find genes file (Panel)
# Note : we can't use .genes files because .list.genes and .genes are not distinctable from the indexing we made
if not config['GENES_FILE']:
	config['GENES_FILE'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], runDict, '.genes.bed', '.list.genes')
# Find transcripts files (NM)
if not config['TRANSCRIPTS_FILE']:
	config['TRANSCRIPTS_FILE'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], runDict, '.transcripts', '.list.transcripts')
# If transcript file exist, create the annotation file for AnnotSV
if os.path.exists(config['TRANSCRIPTS_FILE']) and os.path.getsize(config['TRANSCRIPTS_FILE']) != 0:
	df = pd.read_csv(config['TRANSCRIPTS_FILE'], sep='\t', names=["NM", "Gene"])
	df = df.drop(columns=['Gene'])
	NM_list = df.loc[:,'NM'].tolist()
	with open(annotation_file, 'w+') as f:
		f.write('\t'.join(NM_list))
else:
	with open(annotation_file, 'w') as f:
		f.write("No NM found")
# Find list.genes files 
if not config['LIST_GENES']:
	config['LIST_GENES'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], runDict, '.list.genes', '.list.transcripts')

### DECON bed ###
# DECON need a 4 columns bed for the ReadInBams rule, if bed is not set correctly ReadInBams will crash
# Chromosome Start End Gene but no header, bed format (tsv)
# Chr must be formated chr1, Gene column must be a string, no negative numbers
# ex	:	chr17	4511	4889	BCRA1
# DECON can also use a bed with the exons number for an accurate formating of pdfs graph (makeCNVcalls) and the IdentifyFailures metrics
# ex	:	chr17	4511	4889	BCRA1	26

inputbed = inputDir+"/"+serviceName+".input.bed"
cat_panels_bed = inputDir+"/"+serviceName+".concatenatepanels.bed"
cat_list_genes = inputDir+"/"+serviceName+".fullpath.genes.list"
cat_noduplicate_panels_bed = inputDir+"/"+serviceName+".concatenatepanelsnoduplicate.bed"
outputbed = inputDir + "/" + serviceName + ".output.bed"
intermediatebed = inputDir + "/" + serviceName + '.intermediate.bed'

# Case if genes file bed don't have gene name & exon number
# We always create a custom exon file from the design bed and intersectBed with a refseqgene database to get the exon number
# Warning the refseqgene database must be generated before
if config['BED_FILE'] and config['BED_PROCESS'] == 'REGEN':
	# Generating the custom.exon file from scratch (intersectBed with refseqgene file)
	df = pd.read_csv(config['BED_FILE'], sep='\t', names=["Chr", "Start", "End"], index_col=False)
	df.to_csv(inputbed, sep='\t', index=False, header=False)
	####### Intersect bed with refseqgene using bedtools #######
	intersectbed(inputbed, config['REFSEQGENE_PATH'], outputbed)
	df = pd.read_csv(outputbed, sep='\t', names=["Chr", "Start", "End", "4", "5", "6", "Tosplit", "Gene", "9"])
	df['Custom.Exon'] = df['Tosplit'].str.split(',').str[-1]
	df['Custom.Exon'] = df['Custom.Exon'].str.replace('exon','')
	# Filter with NM_list if NM_list is not empty (avoid multiple NM/exons for the graphs)
	# Specific for config['BED_PROCESS'] == 'REGEN'
	if os.path.exists(config['TRANSCRIPTS_FILE']) and os.path.getsize(config['TRANSCRIPTS_FILE']) != 0:
		df['NM'] = df['Tosplit'].str.split(',').str[0]
		df = df[df['NM'].isin(NM_list)]
		df = df.drop(columns=['NM'])
	# Formating unknown gene name: intersect will output -1 if no gene name exist, replace with "Unknown"
	if config['KEEP_UNKNOWN'] == True:
		keep_unknown('Gene', '-1', 'Unknown')
	else:
		df = df[df.Gene != '-1']
	if config['GENE_LIST_RESTRICT']:
		df = df[df['Gene'].isin(config['GENE_LIST_RESTRICT'])]
	if config['CHR_LIST_RESTRICT']:
		df = df[-df['Chr'].isin(config['CHR_LIST_RESTRICT'])]
	df = df.drop(columns=['4', '5', '6', 'Tosplit', '9'])
	df = df.drop_duplicates()
	df.to_csv(intermediatebed, sep='\t', index=False, header=False)

# Old bed compatibility
# Intersect bed design with panel intersectBed -a design.bed -b panel.bed -loj > RESULT.bed = columns 1 2 3 and the gene column from the panel is 10
# chr1	65300206	65300354	JAK1_chr1_65300207_65300354	0	+	chr1	65300240	65300344	JAK1_ex25	0	- (gene is JAK1_ex25)
# chr1	3598877	3599018	TP73_chr1_3598878_3599018	0	+	.	-1	-1	.	-1	. (gene is .)
# New bed 13 cols
# Intersect bed design with panel intersectBed -a design.bed -b panel.bed -loj > RESULT.bed = columns 1 2 3 & 10 are retained for the bed, column 13 for the exon number
# ex = chr1	27022390	27024129	NM_006015_ARID1A_5utr-ex1_chr1_27022391_27024129	0	+	.	-1	-1	.	-1	.
if config['BED_FILE'] and config['GENES_FILE'] and config['BED_PROCESS'] == 'STANDARD':
	if config['LIST_GENES']:
		basepath = os. path. dirname(config['LIST_GENES'])
		with open(config['LIST_GENES']) as f:
			cat_panel_list = f.read().splitlines()
			cat_panel_list = [basepath + "/" + s for s in cat_panel_list]
		with open(cat_list_genes, 'w+') as f:
			f.write('\t'.join(cat_panel_list))
		shell(" xargs --delimiter='\\t' cat <"+cat_list_genes+" >> "+cat_panels_bed+" ")
	else:
		shell(" xargs --delimiter='\\t' cat <"+config['GENES_FILE']+" >> "+cat_panels_bed+" ")
	df = pd.read_csv(cat_panels_bed,  sep='\t')
	df = df.drop_duplicates()
	df.to_csv(cat_noduplicate_panels_bed, sep='\t', index=False, header=False)
	####### Intersect bed with genes files using bedtools #######
	intersectbed(config['BED_FILE'], cat_noduplicate_panels_bed, outputbed)
	if config['OLD_BED'] == False:
		df = pd.read_csv(outputbed, sep='\t', names=["Chr", "Start", "End", "4", "5", "6", "7", "8", "9", "Gene", "11", "12", "13", "14", "15", "16", "17", "18", "Custom.Exon"])
		df = df.drop(columns=['4', '5', '6', '7', '8', '9', '11', '12','13', '14', '15', '16', '17', '18'])
	else:
		df = pd.read_csv(outputbed, sep='\t', names=["Chr", "Start", "End", "4", "5", "6", "7", "8", "9", "Gene", "11", "12"])
		df = df.drop(columns=['4', '5', '6', '7', '8', '9', '11', '12'])
	# Keep unknown option
	if config['KEEP_UNKNOWN'] == True:
		keep_unknown('Gene', '.', 'Unknown')
	else:
		df = df[df.Gene != '.']
	# Fix for some bed that remove _ex part if gene is noted genename_ex
	if  config['EXON_SEP'] == True: 
		df['Gene'] = df['Gene'].str.split('_ex').str[0]
	# Option to filter the bed to analyse only certain genes
	if config['GENE_LIST_RESTRICT']:
		df = df[df['Gene'].isin(config['GENE_LIST_RESTRICT'])]
	if config['CHR_LIST_RESTRICT']:
		df = df[-df['Chr'].isin(config['CHR_LIST_RESTRICT'])]
	df = df.drop_duplicates()
	df.to_csv(intermediatebed, sep='\t', index=False, header=False)

if config['BED_FILE'] and config['BED_PROCESS'] == 'NO':
	df = pd.read_csv(config['BED_FILE'], sep='\t', names=["Chr", "Start", "End", "Gene"], index_col=False)
	df.to_csv(intermediatebed, sep='\t', index=False, header=False)

# DECON bed formating from intermediatebed file
deconbed_file = tmpDir + "/" + serviceName + "." + date_time + '.bed'

if config['customexon'] == True:
	df = pd.read_csv(intermediatebed, sep='\t', names=["Chr", "Start", "End", "Gene", "Custom.Exon"], index_col=False)
	df = df.drop_duplicates()
	df.to_csv(deconbed_file, sep='\t', index=False, header=False) # for decon.bed no header
else:
	if config['KMER']:
		kmerisation(config['KMER'], intermediatebed, deconbed_file)
	else:
		df = pd.read_csv(intermediatebed, sep='\t', names=["Chr", "Start", "End", "Gene"], index_col=False)
		df = df.drop_duplicates()
		df.to_csv(deconbed_file, sep='\t', index=False, header=False) # for decon.bed no header

if not config['BED_FILE']:
	logsomefile(logfile, 'No bed found, DECON cannot continue, exiting', "\n")
	exit()

# Create file_list of bam by gender with the runDict, depending on the gender_list
# A = all ; XX = Female only ; XY = Male only
# files_list_A contains the full path of all files (bam files for ex)
# Warning the dictionnary for sexe is A/M/F but the gender_list is A/XY/XX
files_list_A = []
for sample in sample_list:
	try:
		files_list_A.append(runDict[sample]['.bam'])
	except KeyError: pass

files_list_XY = []
files_list_XX = []
for files in files_list_A:
	sample = os.path.basename(files).split(".")[0]
	try:
		if runDict[sample]['gender'] == 'F':
			files_list_XX.append(files)
		if runDict[sample]['gender'] == 'M':
			files_list_XY.append(files)
	except KeyError: pass

# Creating a txt list for the bam files per aligner per gender (A, M & F) ; the file_list_whatever must be prefiltered
for aligner in aligner_list:
	for gender in gender_list:
		if gender == 'A':
			bamlist = tmpDir + "/" + serviceName + "." + date_time + "." + gender + '.list.txt'
			create_list(bamlist, files_list_A, config['PROCESS_FILE'], aligner)
		if gender == 'XX':
			bamlist = tmpDir + "/" + serviceName + "." + date_time + "." + gender + '.list.txt'
			create_list(bamlist, files_list_XX, config['PROCESS_FILE'], aligner)
		if gender == 'XY':
			bamlist = tmpDir + "/" + serviceName + "." + date_time + "." + gender + '.list.txt'
			create_list(bamlist, files_list_XY, config['PROCESS_FILE'], aligner)

# Option to remove gender in the gender_list to force A, XX or XY analysis only ; the calling of Y for male subject is not done
if config['REMOVE_M'] == True:
	gender_list.remove('XY')
if config['REMOVE_F'] == True:
	gender_list.remove('XX')
if config['REMOVE_A'] == True:
	gender_list.remove('A')

# Log
logsomefile(logfile, 'Start of the analysis:', "\n", items = date_time)
logsomefile(logfile, 'Input for all files:', "\n", items_list = files_list_A)
logsomefile(logfile, 'Input files for male analysis:', "\n", items_list = files_list_XX)
logsomefile(logfile, 'Input files for female analysis:', "\n", items_list = files_list_XY)
logsomefile(logfile, 'List of samples:', "\n", items_list = sample_list)
logsomefile(logfile, 'List of gender:', "\n", items_list = gender_list)
logsomefile(logfile, 'Aligner list:', "\n", items_list = aligner_list)
logsomefile(logfile, 'Analyse run:', "\n", items = runName)
logsomefile(logfile, 'Design Bed:', "\n", items = config['BED_FILE'])
logsomefile(logfile, 'Panel Bed:', "\n", items = config['GENES_FILE'])
logsomefile(logfile, 'Transcripts list:', "\n", items = config['TRANSCRIPTS_FILE'])
logsomefile(logfile, 'List of genes files:', "\n", items = config['LIST_GENES'])

# Copy2 bed_file & genes_file & transcripts_file for debug
if config['DEBUG_MODE']:
	try:
		copy2(config['BED_FILE'], inputDir)
		copy2(config['GENES_FILE'], inputDir)
		copy2(config['TRANSCRIPTS_FILE'], inputDir)
	except FileNotFoundError: pass

################################################## RULES ##################################################
# rule all will different depending on the presence of a bed files
# design = .bed, vcf will be vcf.design ; panel = genes, vcf will be vcf.panel ; not bed or genes = vcf will be vcf.full
# warning : bcftools merge will crash if there's only 1 sample, so we need to cp files
############################################################################################################

# check the number of sample for copy or merge vcf rule
sample_count = len(sample_list) 

ruleorder: copy_bam > cramtobam > indexing

rule all:
	"""
	Rule will create a design vcf.gz with the gene list file provided
	"""
	input:
		expand(inputDir + "/" + "{sample}.{aligner}.bai", aligner=aligner_list, sample=sample_list),
		expand(tmpDir + "/" + serviceName + "." + date_time + ".{aligner}.Metrics.tsv", aligner=aligner_list),
		expand(tmpDir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.Design.vcf.gz", aligner=aligner_list, sample=sample_list),
		tmpDir + "/" + serviceName + "." + date_time + ".all.AnnotSV.Design.tsv",
		expand(tmpDir + "/" + serviceName + '.' + date_time + ".{aligner}.{gender}.plotSuccess", aligner=aligner_list, gender=gender_list)

rule help:
	"""
	General help for DECON
	Launch snakemake -s  snakefile_decon -c(numberofthreads) --config DATA_DIR=absolutepathoftherundirectory (default is data) without / at the end of the path
	To launch the snakemake file, use --config to replace variables that must be properly set for the pipeline to work ie run path directory
	Every variable defined in the yaml file can be change
	Separate multiple variable with a space (ex  --config DATA_DIR=runname transProb=0.05 var1=0.05 var2=12)
	Also use option --configfile another.yaml to replace and merge existing config.yaml file variables
	Use -p to display shell commands
	Use --lt to display docstrings of rules
	Input file = bam or cram files (if bai is needed, it will be generate)
	Output file = vcf annoted with AnnotSV 3.x for each sample/bam, and a global vcf file will all the samples ; a set of vcf files by design/panel
	"""

rule copy_bam:
	"""
	Copy input files
	"""
	output:
		temp(inputDir+"/{sample}.{aligner}.bam")
	params:
		process = config['PROCESS_CMD'],
		download_link = lambda wildcards: runDict[wildcards.sample]['.bam']
	shell:
		"""
		if [ "{params.process}" = "ls" ];
		then
		ln -sfn {params.download_link} {output}
		else
		rsync -azvh {params.download_link} {output}
		fi
		"""


rule copy_cram:
	"""
	Copy input files
	"""
	output:
		temp(inputDir+"/{sample}.{aligner}.cram")
	params:
		process = config['PROCESS_CMD'],
		download_link = lambda wildcards: runDict[wildcards.sample]['.cram']
	shell:
		"""
		if [ "{params.process}" = "ls" ];
		then
		ln -sfn {params.download_link} {output}
		else
		rsync -azvh {params.download_link} {output}
		fi
		"""


rule cramtobam:
	"""
	Extract bam from a cram file
	"""
	input:
		rules.copy_cram.output
	output:
		temp(inputDir+"/{sample}.{aligner}.bam")
	params:
		refgenome = config['REFGENEFA_PATH']
	shell:
		"""
		samtools view -b -T {params.refgenome} -o {output} {input}
		"""

rule indexing:
	"""
	Indexing bam files with samtools
	"""
	input:
		inputDir+"/{sample}.{aligner}.bam"
	output:
		temp(inputDir+"/{sample}.{aligner}.bai")
	params:
		threads = config['THREADS']
	shell:
		"""
		samtools index -b -@ {params.threads} {input} {output}
		"""

# Keep the chr prefix to the Chr list (chr1 and not 1) if your ref genome got chr1 notation (and not 1) if not ReadInBams will crash
rule ReadInBams:
	"""
	DECoN uses a list of BAM files and a BED file of exons to calculate a coverage metric called the fragment per kilobase and million base pairs (FPKM) for each exon specified in the BED file in each sample’s BAM file
	Rscript ReadInBams.R --bams listofbams.txt --bed file.bed (unique for all bams) --fasta refgenome.fasta --out outputFolder
	Requirement : all .bam files must be indexed
	"""
	input:
		allbamlist = tmpDir+"/"+serviceName+"."+date_time+".A.list.txt",
		bai = expand(inputDir+"/{sample}.{aligner}.bai",sample=sample_list,aligner=aligner_list)
	output:
		tmpDir+"/"+serviceName+'.'+date_time+".{aligner}.ReadInBams.RData"
	params:
		deconbed_file = tmpDir+"/"+serviceName+"."+date_time+".bed",
		refbamlist = config['REF_BAM_LIST'],
		decondir = config['DECON_PATH'],
		refgene = config['REFGENEFA_PATH'],
		mcores = config['MCORES']
	log: 
		log1 = tmpDir+"/"+serviceName+"."+date_time+".{aligner}.ReadInBams.log", log2 = tmpDir+"/"+serviceName+"."+date_time+".{aligner}.ReadInBams.err"
	shell:
		"""
		Rscript {params.decondir}/ReadInBams.R --maxcores {params.mcores} --bams {input.allbamlist} --bed {params.deconbed_file} --fasta {params.refgene} --rdata {output} {params.refbamlist} 1> {log.log1} 2> {log.log2}
		"""


rule IdentifyFailures:
	"""
	The summary .RData file outputted can be used to flag any samples or exons where exon CNV calling may be suboptimal. 
	Both exons and samples are evaluated based on their median coverage level. 
	When coverage is low, accuracy of detection will be compromised and caution should be exercised when interpreting results. 
	Samples are also evaluated based on their correlation with other samples. 
	Samples which do not have a high correlation with other samples in the set are likely to have suboptimal detection across the entire target
	Rscript IdentifyFailures.R --Rdata file.RData --mincorr 0.98 --mincov 100 --exons listofexonsforspecificannotation --out outputfolder
	output is Metrics.tsv ; if custom exon annotation is used, the output is named Metrics_custom.tsv
	"""
	input:
		rules.ReadInBams.output
	params:
		mincorr = config['mincorr'],
		mincov = config['mincov'],
		analysisfailure = tmpDir+"/"+serviceName+"."+date_time+".{aligner}.MetricsFailed",
		decondir = config['DECON_PATH']
	output:
		tmpDir+"/"+serviceName+"."+date_time+".{aligner}.Metrics.tsv"
	log: 
		log1 = tmpDir+"/"+serviceName+"."+date_time+".{aligner}.Metrics.log", log2 = tmpDir+"/"+serviceName+"."+date_time+".{aligner}.Metrics.err"
	shell:
		"""
		Rscript {params.decondir}/IdentifyFailures.R --rdata {input} --mincorr {params.mincorr} --mincov {params.mincov} --tsv {output} 1> {log.log1} 2> {log.log2} && [[ -s {output} ]] || touch {params.analysisfailure} ; [[ -s {output} ]] || touch {output}
		"""

rule makeCNVcalls:
	"""
	This step calls exon CNVs in each sample by selecting reference samples from all other samples contained in the input summary .RData file. 
	The correlation between samples and the number of samples used as a reference are thus calculated and outputted to aid interpretation of call quality
	Rscript makeCNVcalls.R --Rdata file.Rdata --transProb 0.01 (default) --exons bedwithexons.bed --out outputfolder --refbams = txt file containing a list of reference bam files
	output is CNVcall.tsv and a summary Rdata associated ; if custom exon is used, the output is CNVcall_custom.tsv
	"""
	input:
		rules.ReadInBams.output
	params:
		prob = config['transProb'],
		output = tmpDir+"/"+serviceName+"."+date_time+".{aligner}.{gender}.Design_results",
		analysisfailure = tmpDir+"/"+serviceName+"."+date_time+".{aligner}.{gender}.CNVCalledFailed",
		bamlist = tmpDir+"/"+serviceName+"."+date_time+".{gender}.list.txt",
		chromosome = expand("{gender}", gender=gender_list),
		refbamlist = config['REF_BAM_LIST'],
		decondir = config['DECON_PATH']
	output:
		calltsv = tmpDir+"/"+serviceName+"."+date_time+".{aligner}.{gender}.Design_results_all.tsv",
		rdata = tmpDir+"/"+serviceName+"."+date_time+".{aligner}.{gender}.CNVcalls.RData"
	log: 
		log1 = tmpDir+"/"+serviceName+"."+date_time+".{aligner}.{gender}.makeCNVcalls.log", log2 = tmpDir+"/"+serviceName+"."+date_time+".{aligner}.{gender}.makeCNVcalls.err"
	shell:
		"""
		Rscript {params.decondir}/makeCNVcalls.R --rdata {input} --samples {params.bamlist} --transProb {params.prob} --chromosome {params.chromosome} {params.refbamlist} --tsv {output.calltsv} --outrdata {output.rdata} 1> {log.log1} 2> {log.log2} && [[ -s {output.calltsv} ]] || touch {params.analysisfailure} ; [[ -s {output.calltsv} ]] || touch {output.calltsv}
		"""

rule merge_makeCNVcalls:
	"""
	Merge all CNVs call results
	"""
	input:
		expand(tmpDir+"/"+serviceName+"."+date_time+".{aligner}.{gender}.Design_results_all.tsv", gender=gender_list, aligner=aligner_list)
	output:
		tmpDir+"/"+serviceName+"."+date_time+".{aligner}.Design_results_all.tsv"
	shell:
		"""
		cat {input} | sort -u | sort -r >> {output}
		"""

rule variantconvert:
	input:
		rules.merge_makeCNVcalls.output
	output:
		temp(tmpDir+"/"+serviceName+"."+date_time+".{aligner}.allsamples.Design.unsort.vcf")
	params:
		scriptsconfig = config['SCRIPTS_CONFIG'],
		dummypath = config['DUMMY_PATH']
	log:
		tmpDir+"/"+serviceName+"."+date_time+".{aligner}.variantconvert.log"
	shell:
		"""
		variantconvert convert -i {input} -o {output} -fi tsv -fo vcf -c {params.scriptsconfig}/config_decon.json 2> {log} && [[ -s {output} ]] || cat {params.dummypath}/empty.vcf > {output}
		"""

rule sortvcfall:
	input:
		rules.variantconvert.output
	output:
		temp(tmpDir+"/"+serviceName+"."+date_time+".{aligner}.allsamples.Noannotation.vcf")
	params:
		dummypath = config['DUMMY_PATH']
	shell:
		"""
		grep \"^#\" {input} > {output} && grep -v \"^#\" {input} | sort -k1,1V -k2,2g >> {output} && [[ -s {output} ]] || cat {params.dummypath}/empty.vcf > {output}
		"""

# bgzip -c to keep input file
rule vcf2gzall:
	input:
		rules.sortvcfall.output
	output:
		tmpDir+"/"+serviceName+"."+date_time+".{aligner}.allsamples.Noannotation.vcf.gz"
	params:
		dummypath = config['DUMMY_PATH']
	shell:
		"""
		bgzip -c {input} > {output} && [[ -s {output} ]] || cat {params.dummypath}/empty.vcf.gz > {output} ; tabix {output}
		"""

# --force-samples will output a vcf file with all annotations but without a sample name if the sample did not exist ; so we output a dummy vcf.gz
rule splitvcf:
	"""
	Split vcf with bcftools
	-s {sample} : comma-separated list of samples to include
	-Oz : output vcf compressed
	-c1 : minimum allele count (INFO/AC) of sites to be printed
	"""
	input:
		rules.vcf2gzall.output
	output:
		tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.Design.Noannotation.vcf.gz"
	params:
		path = tmpDir+"/{sample}/"+serviceName+"/",
		dummypath = config['DUMMY_PATH']
	shell:
		"""
		mkdir -p {params.path} && bcftools view -c1 -Oz -s {wildcards.sample} -o {output} {input} && [[ -s {output} ]] || cat {params.dummypath}/empty.vcf.gz > {output} ; tabix {output}
		"""

rule AnnotSV:
	"""
	AnnotSV will annotate and rank Structural Variations (SV) from a vcf file. Output will be an AnnotSV vcf file.
	-annotationMode can be : split by exons/introns or full by genes
	-txtFile : path to a file containing a list of preferred genes transcripts to be used in priority during the annotation, preferred genes transcripts names should be tab or space separated
	-genomeBuild must be specified if not hg19
	"""
	input:
		rules.splitvcf.output
	output:
		tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.AnnotSV.Design_unsort.vcf"
	log:
		tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.AnnotSV.log"
	params:
		output = tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.AnnotSV.Design_unsort",
		genome = config['genomeBuild'],
		overlap = config ['overlap'],
		mode = config['annotationMode'],
		annotation = config['annotationdir']
		# HPO -hpo HP:0001156,HP:0001363,HP:0011304"
	shell:
		"""
		AnnotSV -SVinputFile {input} -outputFile {params.output} -annotationMode {params.mode} -annotationsDir {params.annotation} -txFile {annotation_file} -genomeBuild {params.genome} -overlap {params.overlap} -vcf 1 > {log} && [[ -s {output} ]] || echo "#No data to annotate" > {output}
		"""

rule sortvcf:
	"""
	Bash script to sort a vcf
	"""
	input:
		rules.AnnotSV.output
	output:
		tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.Design.vcf"
	params:
		dummypath = config['DUMMY_PATH']
	shell:
		"""
		grep \"^#\" {input} > {output} && grep -v \"^#\" {input} | sort -k1,1V -k2,2g >> {output} && [[ -s {output} ]] || cat {params.dummypath}/empty.vcf | sed 's/SAMPLENAME/{wildcards.sample}/g' > {output}
		"""

# bcftools need vcf.gz.tbi index for the merge all
rule vcf2gz:
	"""
	Compress vcf with bgzip
	"""
	input:
		rules.sortvcf.output
	output:
		tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.Design.vcf.gz"
	shell:
		"""
		bgzip -c {input} > {output} ; tabix {output}
		"""

rule mergedesign:
	"""
	Copy or merge with bcftools several vcfs
	"""
	input:
		expand(tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.Design.vcf.gz", sample=sample_list, aligner=aligner_list)
	output:
		tmpDir+"/"+serviceName+"."+date_time+".all.Design.vcf.gz"
	shell:
		"""
		if [ {sample_count} -eq 1 ]
		then
			cp {input} {output} && tabix {output}
		else
			bcftools merge {input} -O z -o {output} && tabix {output}
		fi
		"""

rule convertvcf_alldesign:
	"""
	Vcf 2 tsv conversion
	"""
	input:
		rules.mergedesign.output
	output:
		tmpDir+"/"+serviceName+"."+date_time+".all.AnnotSV.Design.tsv"
	log:
		log2 = tmpDir+"/"+serviceName+"."+date_time+".all.AnnotSV.Design.vcf2tsv_converter.log"
	shell:
		"""
		vcf2tsvpy --keep_rejected_calls --input_vcf {input} --out_tsv {output}
		"""

rule plot:
	"""
	This step input the CNVcall.Rdata file to output the plot files in pdf format
	"""
	input:
		rules.makeCNVcalls.output.rdata
	params:
		analysisfailure = tmpDir+"/"+serviceName+"."+date_time+".{aligner}.{gender}.plotFailed",
		folder = tmpDir+"/pdfs/",
		decondir = config['DECON_PATH']
	output:
		tmpDir+"/"+serviceName+"."+date_time+".{aligner}.{gender}.plotSuccess"
	log: 
		log1 = tmpDir+"/"+serviceName+"."+date_time+".{aligner}.{gender}.plots.log", log2 = tmpDir+"/"+serviceName+"."+date_time+".{aligner}.{gender}.plots.err"
	shell:
		"""
		mkdir {params.folder} && Rscript {params.decondir}/DECONplot.R --rdata {input} --out {params.folder} 1> {log.log1} 2> {log.log2} && touch {output} &&[[ -s {output} ]] || touch {params.analysisfailure}
		"""

onstart:
	shell("touch "+config['OUTPUT_DIR']+serviceName+"Running.txt")
	with open(logfile, "a+") as f:
		json.dump(config, f)
		f.write("\n")

onsuccess:
	include = config['INCLUDE_RSYNC']
		
	shell("touch "+config['OUTPUT_DIR']+"/"+serviceName+"Complete.txt")
	shell("rm -f "+config['OUTPUT_DIR']+"/"+serviceName+"Running.txt")

	pdf_list = os.listdir(tmpDir+"/pdfs")
	for sample in sample_list:
		for pdf in pdf_list:
			if sample in pdf:
				shell("mv "+tmpDir+ "/pdfs/"+pdf+" " +tmpDir+"/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/")
	shell("rm -rf "+tmpDir+"/pdfs")
	date_time_end = datetime.now().strftime("%Y%m%d-%H%M%S")
	with open(logfile, "a+") as f:
		f.write("End of the analysis : ")
		f.write((date_time_end) + "\n")
	for sample in sample_list:
		shell("rm -f "+config['OUTPUT_DIR']+"/"+sample+"/"+serviceName+"/* || true") # if rm stderr, the true will avoid exiting snakemake
	shell("rsync -azvh --include={include} --exclude='*' {tmpDir}/ "+config['OUTPUT_DIR']+"")
	for sample in sample_list:
		shell("cp "+config['OUTPUT_DIR']+"/"+sample+"/"+serviceName+"/"+sample+"_"+date_time+"_"+serviceName+"/* "+config['OUTPUT_DIR']+sample+"/"+serviceName+"/ || true")
	if config['DEPOT_COPY'] == True:
		shell("rsync -azvh --include={include} --exclude='*' {tmpDir}/ {depotdir}")
		for sample in sample_list:
			shell("cp "+depotdir+sample+"/"+serviceName+"/"+sample+"_"+date_time+"_"+serviceName+"/* "+depotdir+sample+"/"+serviceName+"/ || true")

onerror:
	include_log = config['INCLUDE_LOG_RSYNC']
	
	shell("touch "+config['OUTPUT_DIR']+"/"+serviceName+"Failed.txt")
	#shell("rm -f "+config['OUTPUT_DIR']+serviceName+"Running.txt")
	shell("rsync -azvh --include={include_log} --exclude='*' {tmpDir}/ "+config['OUTPUT_DIR']+"")