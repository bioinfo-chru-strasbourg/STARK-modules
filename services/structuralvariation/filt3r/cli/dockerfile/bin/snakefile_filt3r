##########################################################################
# Snakemakefile Version:   0.1
# Description:             Snakemake file to run filt3r module
##########################################################################

################## Context ##################
# launch snakemake -s  snakefile_filt3r -c(numberofthreads) --config run=absolutepathoftherundirectory
# to launch the snakemake file, use --config to replace variables that must be properly set for the pipeline to work ie run path directory
# every variable defined in the yaml file can be change
# separate multiple variable with a space (ex  --config run=runname var1=0.05 var2=12)
# also use option --configfile another.yaml to replace and merge existing config.yaml file variables

# use -p to display shell commands
# use --lt to display docstrings of rules

# input file = fastq files
# output file = vcf
################## Import libraries ##################

########## Note ########################################################################################

########################################################################################################

import os
import os.path
import glob
import pandas as pd
import json

from datetime import datetime
from shutil import copy2


################## Configuration file and PATHS ##################
configfile: "/app/config/default.yaml"


### COMMON VARIABLES ###
# Analysis only validation bam/cram files
validation_only = config['VALIDATION_ONLY']

# Debug mode
debug = config['DEBUG_MODE']

# Subdir of module results
serviceName = config['serviceName']

# Datas directory
rundir = os.path.normpath(config['run'])

# Repository/depository directories
repositorydir = config['repository']
depositorydir = config['depository']
outputdir = config['OUTPUT_DIR']

# Copy to depot (archive)
depotcopy = config['DEPOT_COPY']

# Group and app name to construct output
group_name = config['GROUP_NAME']
app_name = config['APP_NAME']

# Design.bed or SAMPLE.bed or SAMPLE.aligner.design.bed in STARK folder
bed_file = config['BED_FILE']
# Panel.SAMPLE.APP.manifest.genes.bed or SAMPLE.APP.manifest.genes.bed in STARK folder
genes_file = config['GENES_FILE']
# Transcripts file containing NM notation SAMPLE.transcripts in STARK folder
transcripts_file = config['TRANSCRIPTS_FILE']
# List genes contains list of gene's files
list_genes = config['LIST_GENES']

# Search options to find files to process
recursive_search = config['RECURSIVE_SEARCH']
search_argument = config['SEARCH_ARGUMENT']

# Sample to remove, set in config_scramble.yaml file
sample_exclude_list = config['EXCLUDE_SAMPLE']
filter_sample_list = config['FILTER_SAMPLE']

# Ext to index into dictionary
ext_list = config['EXT_INDEX_LIST']
# Option for processing (cp/ls/samtools)
process_cmd = config['PROCESS_CMD']
# Option for processing bam or cram
process_file = config['PROCESS_FILE']

# Option to append an aligner name to a file (case if the bam don't have an aligner name == sample.bam)
aligner_name = config['ALIGNER_NAME']

# Rsync options for copying results/log
include_file = config['INCLUDE_RSYNC']
exclude_file = config['EXCLUDE_RSYNC']
include_file_log = config['INCLUDE_LOG_RSYNC']
exclude_file_log = config['EXCLUDE_LOG_RSYNC']

### Variables specific for the tools ###
# Refgene variable
refgene = config['REFGENOMEFA_PATH']
# Scripts path
scriptspath = config['SCRIPTS_PATH']

## SALMON
fastq_type = config['SALMON_FASTQ_TYPE']

### FUNCTIONS ###

def finditemindico(sample_list, ext_list, dictionary, include_ext, exclude_ext):
	""" Function to search in a dictionary a file path itering by the first key (sample_list) and second key (extension_list) with an including and excluding filter """
	""" Result must be an non empty file """
	searchresult = ""
	for sample in sample_list:
		for ext in ext_list:
			try:
				items = dictionary[sample][ext]
				if include_ext in items and not exclude_ext in items:
					if os.path.exists(items) and os.path.getsize(items) != 0:
						searchresult = items
			except KeyError: pass
	return searchresult

def searchfiles(directory, search_arg, recursive_arg):
	""" Function to search all files in a directory, adding a search arguement append to the directory and a recursive_search options (True/False) """
	return sorted(filter(os.path.isfile, glob.glob(directory + search_arg, recursive=recursive_arg)))

# TODO include all the files treatement to snakemake rules
# so we should only create files list for the input rules
def processalignedfiles(storedir, dictionary, sample, ext_list, aligner_list, process_cmd, name=None):
	""" Function to process files who's path are stored in a dictionary by sample/ext name"""
	""" You can either cp, rsync, ls files ; or use samtools to extract bam from a cram file into a storedir"""
	""" You can append a name to the end of the files if you specify a name """
	for ext in ext_list:
		inputfiles = dictionary[sample][ext]
		file_without_ext = os.path.basename(inputfiles.replace(ext,''))
		# option to append name file with an aligner name (if file is sample.bam we need to rename it to sample.aligner.bam for the pipeline)
		if name:
			outputfile = storedir + "/" + file_without_ext  + "." + name
			aligner_list.append(name)
		else:
			outputfile = storedir + "/" + file_without_ext
		for aligner in aligner_list:
			if aligner in inputfiles:
				if process_cmd == 'cram':
					print("Uncompress cram files with samtools")
					print(inputfiles)
					shell("samtools view -b -T "+refgene+" -o "+outputfile+".bam "+inputfiles)	# samtools view -b -T ref_sequence.fa -o sample.bam sample.cram
				if process_cmd == 'cp':
					print("Copying files to analyse with copy2")
					print(inputfiles)
					copy2(inputfiles, outputfile+ext)
				if process_cmd == 'rsync':
					print("Copying files to analyse with rsync")
					print(inputfiles)
					shell("rsync -azvh "+inputfiles+" "+storedir) # shell("rsync -azvh --include={include_file} --exclude {exclude_file} {tmpdir}/ {outputdir}")
				if process_cmd == 'ls':
					print("Creating symobling links for the files to analyse with ln")
					print(inputfiles)
					shell("ln -sfn "+inputfiles+" "+outputfile+ext)
				if process_cmd == 'filterbam':
					print("Filter bam files with samtools with the bed provided")
					print(inputfiles)
					shell("samtools view -b -T "+refgene+" -L "+bed_file+" -o "+outputfile+".bam "+inputfiles)	# samtools view -b -h -L bedfile.bed -o newbam.bam originalbam.bam

def logsomefile(logfile, text, sep, items_list=None, items=None):
	""" Function to log a variable value or a list of values into a log file """
	with open(logfile, 'a+') as f:
		f.write(text + sep)
		if items_list:
			for items in items_list:
				if items == '':
					f.write(str('None') + sep)
				else:
					f.write(str(items) + sep)
		else:
			if items == '':
				f.write(str('None') + sep)
			else:
				f.write(str(items) + sep)

# structure of bam files are sample.aligner.(validation).bam or sample.archive.cram
# samples name are taken from bam files
# [x] for x part of the filename separated by '.'
# [0] is sample ; [-1] is extension ; [1] is aligner
def extractlistfromfiles(file_list, ext_list, sep, position):
	""" Function for creating list from a file list, with a specific extension, a separator and the position of the string we want to extract """
	output_list = []
	for files in file_list:
		filesname = os.path.basename(files)
		for ext in ext_list:
			if filesname.endswith(ext):
				output_list.append(filesname.split(sep)[position])
	output_list = list(set(output_list))
	return output_list

def create_list(txtlistoutput, file_list, ext_list, pattern):
	""" Function to create a txt file from a list of files filtered by extension and pattern """
	with open(txtlistoutput, 'a+') as f:
		for files in file_list:
			for ext in ext_list:
				if pattern in files and files.endswith(ext):
					f.write(str(files) + "\n")

def extract_tag(tagfile, tag, tagsep, sep):
	""" Function to extract a tag """
	output_tag = ""
	row = open(tagfile, 'r').readline().strip().split(tagsep)
	for items in row:
		if tag in items and sep in items:
			output_tag = items.split(sep)[-1]
	return(output_tag)

### END OF FUNCTIONS ###

# Variables initialisation
# set datetime to add to output file name
date_time = datetime.now().strftime("%Y%m%d-%H%M%S")

# Return last directory of the path run (without /)
# ex == /STARK/output/repository/GROUP/APP/run
nameoftherun = os.path.basename(rundir)

# Set directories to save tempory results and input files
inputdir = "/app/res/" + nameoftherun +  "/input/"+ date_time
tmpdir = "/app/res/" + nameoftherun + "/tmp"

# Get group [4] and app [5] name from run (or -1 and -2 ?)
# run structure is "/STARK/output/repository/group/app/run"
# Default is UNKNOW in the yaml file
try:
	group_name = rundir.split('/')[4]
	app_name = rundir.split('/')[5]
except IndexError: pass

# Construct repository and depository dir structure (outputdir will be the default output)
depotdir = depositorydir + "/" + group_name + "/" + app_name + "/" + nameoftherun + "/"

if not outputdir:
	outputdir = repositorydir + "/" + group_name + "/" + app_name + "/" + nameoftherun + "/"

# Set the path of the customnumberingfile for makeCNVcalls
customnumberingfile = tmpdir + "/" + serviceName + "." +  date_time + '.CustomNumbering.txt'

# Set log path file
logfile = tmpdir + "/" + serviceName + "." + date_time + '.parameters.log'

# Set annotation file path
annotation_file = tmpdir + "/" + serviceName + "." + date_time + '.AnnotSV.txt'

# Set Decon Bed file
deconbed_file = tmpdir + "/" + serviceName + "." + date_time + '.bed'

# Create directories
os.makedirs(inputdir, exist_ok = True)
os.makedirs(tmpdir, exist_ok = True)
os.makedirs(outputdir, exist_ok = True)
if depotcopy == True:
	os.makedirs(depotdir, exist_ok = True)

# Search files in the rundir directory
files_list = searchfiles(rundir, search_argument, recursive_search)

# Create sample and aligner list
sample_list = extractlistfromfiles(files_list, process_file, '.', 0)
aligner_list = extractlistfromfiles(files_list, process_file, '.', 1)

# Exclude samples from the exclude_list
# Case insensitive
for sample_exclude in sample_exclude_list:
	for sample in sample_list:
		if sample.upper().startswith(sample_exclude.upper()):
			sample_list.remove(sample)

# If filter_sample_list variable is not empty, it will force the sample list
if filter_sample_list:
	sample_list = list(filter_sample_list)

# For validation analyse bam will be sample.aligner.validation.bam
if validation_only == True:
	append_aligner = '.validation'
	aligner_list = [sub + append_aligner for sub in aligner_list]

# Init dictionary
dico_run = {}
for samples in sample_list:
	dico_run[samples] = {}
# Populating dictionary
for samples in sample_list:
	for ext in ext_list:
		for files in files_list:
			if os.path.basename(files).split(".")[0] == samples and os.path.basename(files).endswith(ext):
				if validation_only == False:
					for filext in process_file:
						if ext == filext and not 'validation' in files:
							dico_run[samples][ext] = files
						if ext != process_file:
							dico_run[samples][ext] = files
				if validation_only == True:
					for filext in process_file:
						if ext == filext and 'validation' in files:
							dico_run[samples][ext] = files
						if ext != process_file:
							dico_run[samples][ext] = files

# Separate cram/bam processing
# for cram extract to bam into the input directory
# for bam copy or symlink the bam files to input directory
for sample in sample_list:
	processalignedfiles(inputdir, dico_run, sample, process_file, aligner_list, process_cmd, name=aligner_name)

# Create the file_list by searching the depot directory
files_list_depot = searchfiles(inputdir, '/*', False)

# log
logsomefile(logfile, 'Input file:', "\n", items_list = files_list_depot)

# Creating a txt list for the bam files per aligner
for aligner in aligner_list:
	bamlist = tmpdir + "/" + serviceName + "." + date_time + "." + aligner + '.list.txt'
	create_list(bamlist, files_list_depot, process_file, aligner)

# Find bed file (Design)
if not bed_file:
	bed_file = finditemindico(sample_list, ext_list, dico_run, '.design.bed', '.genes.bed')

# Find genes file (Panel)
if not genes_file:
	genes_file = finditemindico(sample_list, ext_list, dico_run, '.genes', '.list.genes')

# Find transcripts files (NM)
if not transcripts_file:
	transcripts_file = finditemindico(sample_list, ext_list, dico_run, '.transcripts', '.list.transcripts')

# If transcript file exist, create the annotation file for AnnotSV
if os.path.exists(transcripts_file) and os.path.getsize(transcripts_file) != 0:
	df = pd.read_csv(transcripts_file, sep='\t', names=["NM", "Gene"])
	df = df.drop(['Gene'], 1)
	NM_list = df.loc[:,'NM'].tolist()
	with open(annotation_file, 'w+') as f:
		f.write('\t'.join(NM_list))
else:
	with open(annotation_file, 'w') as f:
		f.write("No NM found")

# Find list.genes files 
if not list_genes:
	list_genes = finditemindico(sample_list, ext_list, dico_run, '.list.genes', '.list.transcripts')

# Transform list_genes into a list if list_genes exist, else use genes_file if exist
panel_list = []
panel_list_trunc = []
if list_genes:
	with open(list_genes) as f:
		panel_list = f.read().splitlines()
elif genes_file and not list_genes:
	panel_list.append(os.path.basename(genes_file).split(".bed",1)[0]) # we split the .bed ext because list don't have .bed but genes_file does
# cp files from panel_list to inputdir and rename them
if panel_list:
	for panel in panel_list:
		inputfile = os.path.dirname(list_genes) + "/" + panel + ".bed" # panel_list don't have the bed extension, need that for the copy
		# cut sample. and .genes from file name so files will have the same name as the truncated name list
		panel_trunc = panel.split(".", 1)[1].split(".genes",1)[0]
		outputfile = tmpdir + "/" + panel_trunc
		copy2(inputfile, outputfile)
		# Create a new list for expand, names are filenames without sample and .genes.bed ext
		panel_list_trunc.append(panel_trunc)

# Log
logsomefile(logfile, 'Start of the analysis:', "\n", items = date_time)
logsomefile(logfile, 'List of samples:', "\n", items_list = sample_list)
logsomefile(logfile, 'Analyse run:', "\n", items = nameoftherun)
if aligner_list:
	logsomefile(logfile, 'Aligner list:', "\n", items_list = aligner_list)
if bed_file:
	logsomefile(logfile, 'Design Bed:', "\n", items = bed_file)
if genes_file:
	logsomefile(logfile, 'Panel Bed:', "\n", items = genes_file)
if transcripts_file:
	logsomefile(logfile, 'Transcripts list:', "\n", items = transcripts_file)
if list_genes:
	logsomefile(logfile, 'List of genes files:', "\n", items = list_genes)

# Copy2 bed_file & genes_file & transcripts_file (for debug)
if debug:
	try:
		copy2(bed_file, inputdir)
		copy2(genes_file, inputdir)
		copy2(transcripts_file, inputdir)
	except FileNotFoundError: pass

################################################## RULES ##################################################
# The input for the rule all is the output of the pipeline
# The expand for the {sample} {aligner} is done only at this step
# Final results will follow this structure in the output directory 
# /nameoftherun/SAMPLENAME/ServiceName / individual results
# /nameoftherun/global results & global logs with ServiceName index
#
############################################################################################################

##### NOTE ####
# Tools that output a file adding a extension should be set with params: params.output as the output of the command without the extension, and output: output.ext exist only to connect rules
# For programs that do not have an explicit log parameter, you may always use 2> {log} to redirect standard output to a file (here, the log file) in Linux-based systems. Note that it is also supported to have multiple (named) log files being specified
# 1 is stdout, 2 is stderr 
################################################################################


################################################## RULES ##################################################

# check the number of sample for copy or merge vcf rule
sample_count = len(sample_list) 

rule all:
	input:
		vcfgz = expand(tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.vcf.gz", sample=sample_list, aligner=aligner_list),
		vcfgzallfull = tmpdir + "/" + serviceName + "." + date_time + ".all.vcf.gz"

rule samtools_fastq:
	"""
	Extract fastq from a bam or a cram file
	"""
	input:
		bam = expand(tmpdir + "/" + {sample}.{aligner}.bam, sample=sample_list)
	output:
		fastqR1 = inputdir + "/{sample}.R1.fastq.gz",
		fastqR2 = inputdir + "/{sample}.R2.fastq.gz"
	params:
	shell:
		"""
		samtools fastq -1 {output.fastqR1} -2 {output.fastqR2} {input.bam}
		"""

# --out Output filename. If the filename is empty the output will be written in a file whose name is the same as the input with an appended extension '.results.json'
# --vcf Output an additional VCF file. The output name will be the same as the one for --out, but the .json extension will be replaced with .vcf
rule filt3r:
	"""
	FiLT3r is a software that detects internal duplications in raw sequencing data
	"""
	input:
		fastqR1 = inputdir + "/{sample}.R1.fastq.gz",
		fastqR2 = inputdir + "/{sample}.R2.fastq.gz"
	output:
		json = expand(tmpdir + "/{sample}.filt3r", sample=sample_list)
	params:
		kmer = config[KMER], 
		itdrefseq = config[ITDREF]
	shell:
		"""
		/filt3r/filt3r -k {params.kmer} --sequences {input.fastqR1},{input.fastqR2} --ref {params.itdrefseq} --vcf;
		"""


rule correctvcf:
	"""
	Correction of vcf output, add sample name and genotype to be consistent with the vcf format specification.
	"""
	input:
		vcf = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.uncorr.vcf"
	output:
		vcf = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.vcf"
	params:
	shell:
		"""
		(grep "^##" {input.vcf} && echo '##FORMAT=<ID=GT,Number=1,Type=String,Description="Genotype">' && grep "^#CHROM" {input.vcf} | awk -v SAMPLE={wildcards.sample} '{{print $0"\tFORMAT\t"SAMPLE}}' && grep "^#" -v {input.vcf} | awk '{{print $0"\tGT\t0/1"}}') > {output.vcf}
		"""

# bcftools need vcf.gz.tbi index for the merge all
rule vcf2gz:
	"""
	Compress vcf with bgzip
	"""
	input:
		vcfsort = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.vcf"
	output:
		vcfgz = tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.vcf.gz"
	shell:
		"""
		bgzip -c {input.vcfsort} > {output.vcfgz} ; tabix {output.vcfgz}
		"""

rule cpvcffull:
	"""
	Copy or merge with bcftools several vcfs
	"""
	input:
		vcfgz = expand(tmpdir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.vcf.gz", aligner=aligner_list, sample=sample_list)
	output:
		vcfgzallfull = tmpdir + "/" + serviceName + "." + date_time + ".all.vcf.gz"
	shell:
		"""
		if [ {sample_count} -eq 1 ]
		then
			cp {input.vcfgz} {output.vcfgzallfull} && tabix {output.vcfgzallfull}
		else
			bcftools merge {input.vcfgz} -O z -o {output.vcfgzallfull} && tabix {output.vcfgzallfull}
		fi
		"""

onstart:
	shell("touch " + outputdir + serviceName + "Running.txt")
	# Add the snakemake parameters to log
	with open(logfile, "a+") as f:
		json.dump(config, f)
		f.write("\n")

onsuccess:
	shell("touch " + outputdir + serviceName + "Complete.txt")
	shell("rm -f " + outputdir + serviceName + "Running.txt")
	# Add end time of the analysis to the log file
	date_time_end = datetime.now().strftime("%Y%m%d-%H%M%S")
	with open(logfile, "a+") as f:
		f.write("End of the analysis : ")
		f.write((date_time_end) + "\n")
	# Remove old files
	for sample in sample_list:
		shell("rm -f " + outputdir + sample + "/" + serviceName + "/* || true") # if rm stderr, the true will avoid exiting snakemake
	# Rsync all files into final destination
	shell("rsync -azvh --include={include_file} --exclude {exclude_file} {tmpdir}/ {outputdir}")
	# Copy new files
	for sample in sample_list:
		shell("cp " + outputdir + sample + "/" + serviceName + "/" + sample + "_" + date_time + "_" + serviceName + "/* " + outputdir + sample + "/" + serviceName + "/ || true")
	if depotcopy == True:
		shell("rsync -azvh --include={include_file} --exclude {exclude_file} {tmpdir}/ {depotdir}")
		for sample in sample_list:
			shell("cp " + depotdir + sample + "/" + serviceName + "/" + sample + "_" + date_time + "_" + serviceName + "/* " + depotdir + sample + "/" + serviceName + "/ || true")

onerror:
	shell("touch " + outputdir + serviceName + "Failed.txt")
	#shell("rm -f " + outputdir + serviceName + "Running.txt")
	shell("rsync -azvh --include={include_file_log} --exclude {exclude_file_log} {tmpdir}/ {outputdir}")