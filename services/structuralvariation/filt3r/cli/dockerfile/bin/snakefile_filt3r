##########################################################################
# Snakemakefile Version:   1.0
# Description:             Snakemake file to run filt3r module
##########################################################################

################## Context ##################
# launch snakemake -s  snakefile_filt3r -c(numberofthreads) --config run=absolutepathoftherundirectory
# to launch the snakemake file, use --config to replace variables that must be properly set for the pipeline to work ie run path directory
# every variable defined in the yaml file can be change
# separate multiple variable with a space (ex  --config run=runname var1=0.05 var2=12)
# also use option --configfile another.yaml to replace and merge existing config.yaml file variables

# use -p to display shell commands
# use --lt to display docstrings of rules

# input file = fastq files
# output file = vcf
################## Import libraries ##################

########## Note ########################################################################################

########################################################################################################

import os
import os.path
import glob
import pandas as pd
import json

from os.path import join as osj
from datetime import datetime
from shutil import copy2


################## Configuration file ##################
configfile: "/app/config/default.yaml"

####################### FUNCTIONS #####################

def parse_samplesheet(samplesheet_path):
	"""
	samplesheet_path: absolute path of a samplesheet file, Illumina format
	return: a dataframe containing 9 columns :
	Sample_ID,Sample_Plate,Sample_Well,I7_Index_ID,index,Manifest,GenomeFolder,Sample_Project,Description
	The description field contains tag separated by ! ; the name of the tag and the value is separated by # (ex : SEX#F!APP#DIAG.BBS_RP)
	"""
	samplesheet_data = []
	samplesheet_header = []
	with open(samplesheet_path, 'r') as f:
		v = False
		for lines in f:
			lines = lines.strip()
			if v:
				samplesheet_data.append(lines.split(','))
			if 'Sample_ID' in lines:
				v = True
				samplesheet_header.append(lines.split(','))
	df = pd.DataFrame(samplesheet_data, columns=samplesheet_header)
	sample_list = df.iloc[:, 0].tolist()
	return df

def getSampleInfos(samplesheet_path, dictionary):
	"""
	samplesheet_path: absolute path of a samplesheet file, Illumina format
	return a dictionnary with Sample_ID from samplesheet as key and 'gender': 'F or M or NULL'
	"""
	dictionary = {}
	if samplesheet_path:
		parse_samplesheet(samplesheet_path)
		for i, rows in parse_samplesheet(samplesheet_path).iterrows():
			if not any(exclude in rows['Sample_ID'] for exclude in config['EXCLUDE_SAMPLE']):
				sampleID = rows["Sample_ID"]
				dictionary[sampleID] = {}
				tags = rows['Description'].split('!')
				for tag in tags:
					if 'SEX' in tag and '_' in tag:
						tag = tag.split('_')[-1]
						dictionary[sampleID]['gender'] = tag
						break
					elif 'SEX' in tag and '#' in tag:
						tag = tag.split('#')[-1]
						dictionary[sampleID]['gender'] = tag
						break
					else:
						tag = ''
						dictionary[sampleID]['gender'] = tag
	return dictionary

def finditemindico(sample_list, ext_list, dictionary, include_ext, exclude_ext):
	""" Function to search in a dictionary a file path itering by the first key (sample_list) and second key (extension_list) with an including and excluding filter """
	""" Result must be an non empty file """
	searchresult = ""
	for sample in sample_list:
		for ext in ext_list:
			try:
				items = dictionary[sample][ext]
				if include_ext in items and not exclude_ext in items:
					if os.path.exists(items) and os.path.getsize(items) != 0:
						searchresult = items
			except KeyError: pass
	return searchresult

def searchfiles(directory, search_arg, recursive_arg):
	""" Function to search all files in a directory, adding a search arguement append to the directory and a recursive_search options (True/False) """
	return sorted(filter(os.path.isfile, glob.glob(directory + search_arg, recursive=recursive_arg)))

def logsomefile(logfile, text, sep, items_list=None, items=None):
	""" Function to log a variable value or a list of values into a log file """
	with open(logfile, 'a+') as f:
		f.write(text + sep)
		if items_list:
			for items in items_list:
				if items == '':
					f.write(str('None') + sep)
				else:
					f.write(str(items) + sep)
		else:
			if items == '':
				f.write(str('None') + sep)
			else:
				f.write(str(items) + sep)

# structure of bam files are sample.aligner.(validation).bam or sample.archive.cram
# samples name are taken from bam files
# [x] for x part of the filename separated by '.'
# [0] is sample ; [-1] is extension ; [1] is aligner
def extractlistfromfiles(file_list, ext_list, sep, position):
	""" Function for creating list from a file list, with a specific extension, a separator and the position of the string we want to extract """
	output_list = []
	for files in file_list:
		filesname = os.path.basename(files)
		for ext in ext_list:
			if filesname.endswith(ext):
				output_list.append(filesname.split(sep)[position])
	output_list = list(set(output_list))
	return output_list

def create_list(txtlistoutput, file_list, ext_list, pattern):
	""" Function to create a txt file from a list of files filtered by extension and pattern """
	with open(txtlistoutput, 'a+') as f:
		for files in file_list:
			for ext in ext_list:
				if pattern in files and files.endswith(ext):
					f.write(str(files) + "\n")

def extract_tag(tagfile, tag, tagsep, sep):
	""" Function to extract a tag from a tagfile """
	output_tag = ""
	row = open(tagfile, 'r').readline().strip().split(tagsep)
	for items in row:
		if tag in items and sep in items:
			output_tag = items.split(sep)[-1]
	return(output_tag)

### END OF FUNCTIONS ###
serviceName = config['serviceName']
date_time = datetime.now().strftime("%Y%m%d-%H%M%S")
runName = os.path.basename(os.path.normpath(config['run']))

inputDir = "/app/res/" + runName + "/" + date_time + "/input"
tmpDir = "/app/res/" + runName + "/" + date_time + "/tmp"
try:
	config['GROUP_NAME'] = os.path.normpath(config['run']).split('/')[4]
	config['APP_NAME'] = os.path.normpath(config['run']).split('/')[5]
except IndexError: pass
depotdir = config['depository'] + "/" + config['GROUP_NAME'] + "/" + config['APP_NAME'] + "/" + runName + "/"
if not config['OUTPUT_DIR']:
	config['OUTPUT_DIR'] = config['run']
logfile = tmpDir + "/" + serviceName + "." + date_time + '.parameters.log'

# Create directories
os.makedirs(inputDir, exist_ok = True)
os.makedirs(tmpDir, exist_ok = True)
os.makedirs(config['OUTPUT_DIR'], exist_ok = True)
if config['DEPOT_COPY'] == True:
	os.makedirs(depotdir, exist_ok = True)

# Search files in the run directory
files_list = searchfiles(os.path.normpath(config['run']), config['SEARCH_ARGUMENT'], config['RECURSIVE_SEARCH'])
# Create sample and aligner list
sample_list = extractlistfromfiles(files_list, config['PROCESS_FILE'], '.', 0)
aligner_list = extractlistfromfiles(files_list, config['PROCESS_FILE'], '.', 1)

# Exclude samples from the exclude_list, case insensitive
for sample_exclude in config['EXCLUDE_SAMPLE']:
	for sample in sample_list:
		if sample.upper().startswith(sample_exclude.upper()):
			sample_list.remove(sample)

# If config['FILTER_SAMPLE'] variable is not empty, it will force the sample list
if config['FILTER_SAMPLE']:
	sample_list = list(config['FILTER_SAMPLE'])

# For validation analyse bam will be sample.aligner.validation.bam
if config['VALIDATION_ONLY'] == True:
	append_aligner = '.validation'
	aligner_list = [sub + append_aligner for sub in aligner_list]

# Create run dictionary
runDict = {}
for samples in sample_list:
	runDict[samples] = {}
for samples in sample_list:
	for ext in config['EXT_INDEX_LIST']:
		for files in files_list:
			if os.path.basename(files).split(".")[0] == samples and os.path.basename(files).endswith(ext):
				if config['VALIDATION_ONLY'] == False:
					for filext in config['PROCESS_FILE']:
						if ext == filext and not 'validation' in files:
							runDict[samples][ext] = files
						if ext != config['PROCESS_FILE']:
							runDict[samples][ext] = files
				if config['VALIDATION_ONLY'] == True:
					for filext in config['PROCESS_FILE']:
						if ext == filext and 'validation' in files:
							runDict[samples][ext] = files
						if ext != config['PROCESS_FILE']:
							runDict[samples][ext] = files

# Find bed file (Design)
if not config['BED_FILE']:
	config['BED_FILE'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], runDict, '.design.bed', '.genes.bed')
# Find genes file (Panel)
if not config['GENES_FILE']:
	config['GENES_FILE'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], runDict, '.genes', '.list.genes')
# Find transcripts files (NM)
if not config['TRANSCRIPTS_FILE']:
	config['TRANSCRIPTS_FILE'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], runDict, '.transcripts', '.list.transcripts')
# Find list.genes files 
if not config['LIST_GENES']:
	config['LIST_GENES'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], runDict, '.list.genes', '.list.transcripts')

# Transform list_genes into a list if list_genes exist, else use genes_file if exist
panel_list = []
panel_list_trunc = []
if config['LIST_GENES']:
	with open(config['LIST_GENES']) as f:
		panel_list = f.read().splitlines()
elif config['GENES_FILE'] and not config['LIST_GENES']:
	panel_list.append(os.path.basename(config['GENES_FILE']).split(".bed",1)[0]) # we split the .bed ext because list don't have .bed but genes_file does
if panel_list:
	for panel in panel_list:
		inputfile = os.path.dirname(config['LIST_GENES']) + "/" + panel + ".bed" # panel_list don't have the bed extension, need that for the copy
		# cut sample. and .genes from file name so files will have the same name as the truncated name list
		panel_trunc = panel.split(".", 1)[1].split(".genes",1)[0]
		outputfile = tmpDir + "/" + panel_trunc
		copy2(inputfile, outputfile)
		# Create a new list for expand, names are filenames without sample and .genes.bed ext
		panel_list_trunc.append(panel_trunc)

logsomefile(logfile, 'Start of the analysis:', "\n", items = date_time)
logsomefile(logfile, 'List of samples:', "\n", items_list = sample_list)
logsomefile(logfile, 'Analyse run:', "\n", items = runName)
if aligner_list:
	logsomefile(logfile, 'Aligner list:', "\n", items_list = aligner_list)
if config['BED_FILE']:
	logsomefile(logfile, 'Design Bed:', "\n", items = config['BED_FILE'])
if config['GENES_FILE']:
	logsomefile(logfile, 'Panel Bed:', "\n", items = config['GENES_FILE'])
if config['TRANSCRIPTS_FILE']:
	logsomefile(logfile, 'Transcripts list:', "\n", items = config['TRANSCRIPTS_FILE'])
if config['LIST_GENES']:
	logsomefile(logfile, 'List of genes files:', "\n", items = config['LIST_GENES'])

# Copy2 bed_file & genes_file & transcripts_file for debug
if config['DEBUG_MODE']:
	try:
		copy2(config['BED_FILE'], inputDir)
		copy2(config['GENES_FILE'], inputDir)
		copy2(config['TRANSCRIPTS_FILE'], inputDir)
	except FileNotFoundError: pass

################################################## RULES ##################################################
# The input for the rule all is the output of the pipeline
# The expand for the {sample} {aligner} is done only at this step
# Final results will follow this structure in the output directory 
# /runName/SAMPLENAME/ServiceName / individual results
# /runName/global results & global logs with ServiceName index
############################################################################################################

##### NOTE #################################################################################################
# Tools that output a file adding a extension should be set with params: params.output as the output of the command without the extension, and output: output.ext exist only to connect rules
# For programs that do not have an explicit log parameter, you may always use 2> {log} to redirect standard output to a file (here, the log file) in Linux-based systems. Note that it is also supported to have multiple (named) log files being specified
# 1 is stdout, 2 is stderr 
############################################################################################################

# check the number of sample for merge vcf rule
sample_count = len(sample_list) 

if config['PROCESS_FILE'] == [".R1.fastq.gz", ".R2.fastq.gz"]:
	ruleorder: copy_fastq > samtools_fastq
else:
	ruleorder: copy_bam > cramtobam

rule all:
	input:
		expand(tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.uncorr.vcf",sample=sample_list,aligner=aligner_list),
		tmpDir+"/"+serviceName+"."+date_time+".all.vcf.gz"

rule copy_fastq:
	"""
	Copy input files
	"""
	output:
		fastqR1=inputDir+"/{sample}.{aligner}.R1.fastq.gz",
		fastqR2=inputDir+"/{sample}.{aligner}.R2.fastq.gz"
	params:
		process = config['PROCESS_CMD'],
		download_link1 = lambda wildcards: runDict[wildcards.sample]['.R1.fastq.gz'],
		download_link2 = lambda wildcards: runDict[wildcards.sample]['.R2.fastq.gz']
	shell:
		"""
		if [ "{params.process}" = "ls" ];
		then
		ln -sfn {params.download_link1} {output.fastqR1} && ln -sfn {params.download_link2} {output.fastqR2}
		else
		rsync -azvh {params.download_link1} {output.fastqR1} && rsync -azvh {params.download_link2} {output.fastqR2}
		fi
		"""

rule copy_bam:
	"""
	Copy input files
	"""
	output:
		inputDir+"/{sample}.{aligner}.bam"
	params:
		process = config['PROCESS_CMD'],
		download_link = lambda wildcards: runDict[wildcards.sample]['.bam']
	shell:
		"""
		if [ "{params.process}" = "ls" ];
		then
		ln -sfn {params.download_link} {output}
		else
		rsync -azvh {params.download_link} {output}
		fi
		"""


rule copy_cram:
	"""
	Copy input files
	"""
	output:
		inputDir+"/{sample}.{aligner}.cram"
	params:
		process = config['PROCESS_CMD'],
		download_link = lambda wildcards: runDict[wildcards.sample]['.cram']
	shell:
		"""
		if [ "{params.process}" = "ls" ];
		then
		ln -sfn {params.download_link} {output}
		else
		rsync -azvh {params.download_link} {output}
		fi
		"""


rule cramtobam:
	"""
	Extract bam from a cram file
	"""
	input:
		rules.copy_cram.output
	output:
		inputDir+"/{sample}.{aligner}.bam"
	params:
		refgenome = config['REFGENEFA_PATH']
	shell:
		"""
		samtools view -b -T {params.refgenome} -o {output} {input}
		"""

rule samtools_fastq:
	"""
	Extract fastq from a bam or a cram file
	"""
	input:
		inputDir+"/{sample}.{aligner}.bam"
	output:
		fastqR1=inputDir+"/{sample}.{aligner}.R1.fastq.gz",
		fastqR2=inputDir+"/{sample}.{aligner}.R2.fastq.gz"
	shell:
		"""
		samtools fastq -1 {output.fastqR1} -2 {output.fastqR2} {input}
		"""


rule filt3r:
	"""
	FiLT3r is a software that detects internal duplications in raw sequencing data
	"""
	input:
		fastqR1=inputDir+"/{sample}.{aligner}.R1.fastq.gz",
		fastqR2=inputDir+"/{sample}.{aligner}.R2.fastq.gz"
	output:
		tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.uncorr.vcf"
	params:
		kmer = config['KMER'], 
		itdrefseq = config['ITDREF'],
		filterpath = config['FILTER_PATH'],
		shortindels = config['SHORT_INDELS'],
		output=tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.uncorr"
	shell:
		"""
		{params.filterpath}/filt3r -k {params.kmer} --sequences {input.fastqR1},{input.fastqR2} --ref {params.itdrefseq} --out {params.output} --ignore-short-indels {params.shortindels} --vcf;
		"""

# sed 's/old-text/new-text/g' input > output
rule vcf_infofix:
	"""
	Fix VAF type (Float)
	"""
	input:
		rules.filt3r.output
	output:
		tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.fixGT.vcf"
	shell:
		"""
		sed 's/##INFO=<ID=VAF,Number=.,Type=String,Description="Variant Allele Frequency (ratio)">/##INFO=<ID=VAF,Number=.,Type=Float,Description="Variant Allele Frequency (ratio)">/g' {input} > {output}
		"""

rule vcf_fixgenotype:
	"""
	Fix sample
	"""
	input:
		rules.vcf_infofix.output
	output:
		tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.fixINFO.vcf"
	shell:
		"""
		(grep "^##" {input} && echo '##FORMAT=<ID=GT,Number=1,Type=String,Description="Genotype">' && grep "^#CHROM" {input} | awk -v SAMPLE={wildcards.sample} '{{print $0"\tFORMAT\t"SAMPLE}}' && grep "^#" -v {input} | awk '{{print $0"\tGT\t0/1"}}') > {output}
		"""


rule bcftools_filter:
	"""
	Filter with bcftools
	"""
	input:
		rules.vcf_fixgenotype.output
	output:
		tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.unfilter.vcf"
	params:
		bcffilter = config['BCFTOOLS_FILTER']
	shell:
		"""
		bcftools view {params.bcffilter} {input} -o {output} 
		"""



rule sortvcf:
	"""
	Bash script to sort a vcf
	"""
	input:
		rules.bcftools_filter.output
	output:
		tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.unsort.vcf"
	params:
		dummypath = config['DUMMY_PATH']
	shell:
		"""
		grep \"^#\" {input} > {output} && grep -v \"^#\" {input} | sort -k1,1V -k2,2g >> {output} && [[ -s {output} ]] || cat {params.dummypath}/empty.vcf | sed 's/SAMPLENAME/{wildcards.sample}/g' > {output}
		"""		


# bcftools need vcf.gz.tbi index for the merge all
rule vcf2gz:
	"""
	Compress vcf with bgzip
	"""
	input:
		rules.sortvcf.output
	output:
		tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.vcf.gz"
	shell:
		"""
		bgzip -c {input} > {output} ; tabix {output}
		"""

rule mergevcf:
	"""
	Copy or merge with bcftools several vcfs
	"""
	input:
		expand(tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.vcf.gz",sample=sample_list,aligner=aligner_list)
	output:
		tmpDir+"/"+serviceName+"."+date_time+".all.vcf.gz"
	shell:
		"""
		if [ {sample_count} -eq 1 ]
		then
			cp {input} {output} && tabix {output}
		else
			bcftools merge {input} -O z -o {output} && tabix {output}
		fi
		"""
onstart:
	shell("touch "+config['OUTPUT_DIR']+serviceName+"Running.txt")
	# Add the snakemake parameters to log
	with open(logfile, "a+") as f:
		json.dump(config, f)
		f.write("\n")
		
onsuccess:
	include=config['INCLUDE_RSYNC']
	shell("touch "+config['OUTPUT_DIR']+serviceName+"Complete.txt")
	shell("rm -f "+config['OUTPUT_DIR']+serviceName+"Running.txt")
	date_time_end = datetime.now().strftime("%Y%m%d-%H%M%S")
	with open(logfile, "a+") as f:
		f.write("End of the analysis : ")
		f.write((date_time_end) + "\n")
	# Remove old files TODO add a test if files in folder to avoid stderr
	for sample in sample_list:
		shell("rm -f "+config['OUTPUT_DIR']+sample+"/"+serviceName+"/* || true") # if rm stderr, the true will avoid exiting snakemake
	shell("rsync -azvh --include={include} {tmpDir}/ "+config['OUTPUT_DIR']+"")
	# Copy new files
	for sample in sample_list:
		shell("cp "+config['OUTPUT_DIR']+sample+"/"+serviceName+"/"+sample+"_"+date_time+"_"+serviceName+"/* "+config['OUTPUT_DIR']+sample+"/"+serviceName+"/ || true")
	
onerror:
	include_log=config['INCLUDE_LOG_RSYNC']
	shell("touch "+config['OUTPUT_DIR']+serviceName+"Failed.txt")
	#shell("rm -f " + config['OUTPUT_DIR'] + serviceName + "Running.txt")
	shell("rsync -azvh --include={include_log} {tmpDir}/ "+config['OUTPUT_DIR']+"")