##########################################################################
# Snakemakefile Version:   1.0
# Description:             Snakemake file to run flt3itdext module
##########################################################################

################## Context ##################
# launch snakemake -s  snakefile_filt3r -c(numberofthreads) --config run=absolutepathoftherundirectory
# to launch the snakemake file, use --config to replace variables that must be properly set for the pipeline to work ie run path directory
# every variable defined in the yaml file can be change
# separate multiple variable with a space (ex  --config run=runname var1=0.05 var2=12)
# also use option --configfile another.yaml to replace and merge existing config.yaml file variables

# use -p to display shell commands
# use --lt to display docstrings of rules

# input file = fastq files
# output file = vcf
################## Import libraries ##################

########## Note ########################################################################################

########################################################################################################

import os
import os.path
import glob
import pandas as pd
import json

from datetime import datetime
from shutil import copy2


################## Configuration file ##################
configfile: "/app/config/default.yaml"

####################### FUNCTIONS #####################
def parse_samplesheet(samplesheet_path):
	"""
	samplesheet_path: absolute path of a samplesheet file, Illumina format
	return: a dataframe containing 9 columns :
	Sample_ID,Sample_Plate,Sample_Well,I7_Index_ID,index,Manifest,GenomeFolder,Sample_Project,Description
	The description field contains tag separated by ! ; the name of the tag and the value is separated by # (ex : SEX#F!APP#DIAG.BBS_RP)
	"""
	samplesheet_data = []
	samplesheet_header = []
	with open(samplesheet_path, 'r') as f:
		v = False
		for lines in f:
			lines = lines.strip()
			if v:
				samplesheet_data.append(lines.split(','))
			if 'Sample_ID' in lines:
				v = True
				samplesheet_header.append(lines.split(','))
	df = pd.DataFrame(samplesheet_data, columns=samplesheet_header)
	sample_list = df.iloc[:, 0].tolist()
	return df

def getSampleInfos(samplesheet_path, dictionary):
	"""
	samplesheet_path: absolute path of a samplesheet file, Illumina format
	return a dictionnary with Sample_ID from samplesheet as key and 'gender': 'F or M or NULL'
	"""
	dictionary = {}
	if samplesheet_path:
		parse_samplesheet(samplesheet_path)
		for i, rows in parse_samplesheet(samplesheet_path).iterrows():
			if not any(exclude in rows['Sample_ID'] for exclude in config['EXCLUDE_SAMPLE']):
				sampleID = rows["Sample_ID"]
				dictionary[sampleID] = {}
				tags = rows['Description'].split('!')
				for tag in tags:
					if 'SEX' in tag and '_' in tag:
						tag = tag.split('_')[-1]
						dictionary[sampleID]['gender'] = tag
						break
					elif 'SEX' in tag and '#' in tag:
						tag = tag.split('#')[-1]
						dictionary[sampleID]['gender'] = tag
						break
					else:
						tag = ''
						dictionary[sampleID]['gender'] = tag
	return dictionary

def finditemindico(sample_list, ext_list, dictionary, include_ext, exclude_ext):
	""" Function to search in a dictionary a file path itering by the first key (sample_list) and second key (extension_list) with an including and excluding filter """
	""" Result must be an non empty file """
	searchresult = ""
	for sample in sample_list:
		for ext in ext_list:
			try:
				items = dictionary[sample][ext]
				if include_ext in items and not exclude_ext in items:
					if os.path.exists(items) and os.path.getsize(items) != 0:
						searchresult = items
			except KeyError: pass
	return searchresult

def searchfiles(directory, search_arg, recursive_arg):
	""" Function to search all files in a directory, adding a search arguement append to the directory and a recursive_search options (True/False) """
	return sorted(filter(os.path.isfile, glob.glob(directory + search_arg, recursive=recursive_arg)))

def logsomefile(logfile, text, sep, items_list=None, items=None):
	""" Function to log a variable value or a list of values into a log file """
	with open(logfile, 'a+') as f:
		f.write(text + sep)
		if items_list:
			for items in items_list:
				if items == '':
					f.write(str('None') + sep)
				else:
					f.write(str(items) + sep)
		else:
			if items == '':
				f.write(str('None') + sep)
			else:
				f.write(str(items) + sep)

def extractlistfromfiles(file_list, ext_list, sep, position):
	""" Function for creating list from a file list, with a specific extension, a separator and the position of the string we want to extract """
	output_list = []
	for files in file_list:
		filesname = os.path.basename(files)
		for ext in ext_list:
			if filesname.endswith(ext):
				output_list.append(filesname.split(sep)[position])
	output_list = list(set(output_list))
	return output_list

def create_list(txtlistoutput, file_list, ext_list, pattern):
	""" Function to create a txt file from a list of files filtered by extension and pattern """
	with open(txtlistoutput, 'a+') as f:
		for files in file_list:
			for ext in ext_list:
				if pattern in files and files.endswith(ext):
					f.write(str(files) + "\n")

def extract_tag(tagfile, tag, tagsep, sep):
	""" Function to extract a tag """
	output_tag = ""
	row = open(tagfile, 'r').readline().strip().split(tagsep)
	for items in row:
		if tag in items and sep in items:
			output_tag = items.split(sep)[-1]
	return(output_tag)

### END OF FUNCTIONS ###
serviceName = config['serviceName']
date_time = datetime.now().strftime("%Y%m%d-%H%M%S")
runName = os.path.basename(os.path.normpath(config['run']))
inputDir = "/app/res/" + runName + "/" + date_time + "/input"
tmpDir = "/app/res/" + runName + "/" + date_time + "/tmp"
try:
	config['GROUP_NAME'] = os.path.normpath(config['run']).split('/')[4]
	config['APP_NAME'] = os.path.normpath(config['run']).split('/')[5]
except IndexError: pass
depotdir = config['depository'] + "/" + config['GROUP_NAME'] + "/" + config['APP_NAME'] + "/" + runName + "/"
if not config['OUTPUT_DIR']:
	config['OUTPUT_DIR'] = config['run']
logfile = tmpDir + "/" + serviceName + "." + date_time + '.parameters.log'

# Create directories
os.makedirs(inputDir, exist_ok = True)
os.makedirs(tmpDir, exist_ok = True)
os.makedirs(config['OUTPUT_DIR'], exist_ok = True)
if config['DEPOT_COPY'] == True:
	os.makedirs(depotdir, exist_ok = True)

# Search files in the rundir directory
files_list = searchfiles(os.path.normpath(config['run']), config['SEARCH_ARGUMENT'], config['RECURSIVE_SEARCH'])

# Create sample and aligner list
sample_list = extractlistfromfiles(files_list, config['PROCESS_FILE'], '.', 0)
aligner_list = extractlistfromfiles(files_list, config['PROCESS_FILE'], '.', 1)

# Exclude samples from the exclude_list case insensitive
for sample_exclude in config['EXCLUDE_SAMPLE']:
	for sample in sample_list:
		if sample.upper().startswith(sample_exclude.upper()):
			sample_list.remove(sample)

# If config['FILTER_SAMPLE'] variable is not empty, it will force the sample list
if config['FILTER_SAMPLE']:
	sample_list = list(config['FILTER_SAMPLE'])

# For validation analyse bam will be sample.aligner.validation.bam
if config['VALIDATION_ONLY'] == True:
	append_aligner = '.validation'
	aligner_list = [sub + append_aligner for sub in aligner_list]

# Create run dictionary
runDict = {}
for samples in sample_list:
	runDict[samples] = {}
for samples in sample_list:
	for ext in config['EXT_INDEX_LIST']:
		for files in files_list:
			if os.path.basename(files).split(".")[0] == samples and os.path.basename(files).endswith(ext):
				if config['VALIDATION_ONLY'] == False:
					for filext in config['PROCESS_FILE']:
						if ext == filext and not 'validation' in files:
							runDict[samples][ext] = files
						if ext != config['PROCESS_FILE']:
							runDict[samples][ext] = files
				if config['VALIDATION_ONLY'] == True:
					for filext in config['PROCESS_FILE']:
						if ext == filext and 'validation' in files:
							runDict[samples][ext] = files
						if ext != config['PROCESS_FILE']:
							runDict[samples][ext] = files

# Find bed file (Design)
if not config['BED_FILE']:
	config['BED_FILE'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], runDict, '.design.bed', '.genes.bed')
# Find genes file (Panel)
if not config['GENES_FILE']:
	config['GENES_FILE'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], runDict, '.genes', '.list.genes')
# Find transcripts files (NM)
if not config['TRANSCRIPTS_FILE']:
	config['TRANSCRIPTS_FILE'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], runDict, '.transcripts', '.list.transcripts')
# Find list.genes files 
if not config['LIST_GENES']:
	config['LIST_GENES'] = finditemindico(sample_list, config['EXT_INDEX_LIST'], runDict, '.list.genes', '.list.transcripts')

# Transform list_genes into a list if list_genes exist, else use genes_file if exist
panel_list = []
panel_list_trunc = []
if config['LIST_GENES']:
	with open(config['LIST_GENES']) as f:
		panel_list = f.read().splitlines()
elif config['GENES_FILE'] and not config['LIST_GENES']:
	panel_list.append(os.path.basename(config['GENES_FILE']).split(".bed",1)[0]) # we split the .bed ext because list don't have .bed but genes_file does
# cp files from panel_list to inputDir and rename them
if panel_list:
	for panel in panel_list:
		inputfile = os.path.dirname(config['LIST_GENES']) + "/" + panel + ".bed" # panel_list don't have the bed extension, need that for the copy
		# cut sample. and .genes from file name so files will have the same name as the truncated name list
		panel_trunc = panel.split(".", 1)[1].split(".genes",1)[0]
		outputfile = tmpDir + "/" + panel_trunc
		copy2(inputfile, outputfile)
		# Create a new list for expand, names are filenames without sample and .genes.bed ext
		panel_list_trunc.append(panel_trunc)

logsomefile(logfile, 'Start of the analysis:', "\n", items = date_time)
logsomefile(logfile, 'List of samples:', "\n", items_list = sample_list)
logsomefile(logfile, 'Analyse run:', "\n", items = runName)
if aligner_list:
	logsomefile(logfile, 'Aligner list:', "\n", items_list = aligner_list)
if config['BED_FILE']:
	logsomefile(logfile, 'Design Bed:', "\n", items = config['BED_FILE'])
if config['GENES_FILE']:
	logsomefile(logfile, 'Panel Bed:', "\n", items = config['GENES_FILE'])
if config['TRANSCRIPTS_FILE']:
	logsomefile(logfile, 'Transcripts list:', "\n", items = config['TRANSCRIPTS_FILE'])
if config['LIST_GENES']:
	logsomefile(logfile, 'List of genes files:', "\n", items = config['LIST_GENES'])

# Copy2 bed_file & genes_file & transcripts_file for debug
if config['DEBUG_MODE']:
	try:
		copy2(config['BED_FILE'], inputDir)
		copy2(config['GENES_FILE'], inputDir)
		copy2(config['TRANSCRIPTS_FILE'], inputDir)
	except FileNotFoundError: pass

################################################## RULES ##################################################

# check the number of sample for copy or merge vcf rule
sample_count = len(sample_list) 

ruleorder: copy_bam > cramtobam

rule all:
	input:
		expand(tmpDir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.vcf.gz",aligner=aligner_list, sample=sample_list),
		tmpDir + "/" + serviceName + "." + date_time + ".all.vcf.gz"


rule copy_bam:
	"""
	Copy input files
	"""
	output:
		temp(inputDir+"/{sample}.{aligner}.bam")
	params:
		process = config['PROCESS_CMD'],
		download_link = lambda wildcards: runDict[wildcards.sample]['.bam']
	shell:
		"""
		if [ "{params.process}" = "ls" ];
		then
		ln -sfn {params.download_link} {output}
		else
		rsync -azvh {params.download_link} {output}
		fi
		"""


rule copy_cram:
	"""
	Copy input files
	"""
	output:
		temp(inputDir+"/{sample}.{aligner}.cram")
	params:
		process = config['PROCESS_CMD'],
		download_link = lambda wildcards: runDict[wildcards.sample]['.cram']
	shell:
		"""
		if [ "{params.process}" = "ls" ];
		then
		ln -sfn {params.download_link} {output}
		else
		rsync -azvh {params.download_link} {output}
		fi
		"""


rule cramtobam:
	"""
	Extract bam from a cram file
	"""
	input:
		rules.copy_cram.output
	output:
		temp(inputDir+"/{sample}.{aligner}.bam")
	params:
		refgenome = config['REFGENEFA_PATH']
	shell:
		"""
		samtools view -b -T {params.refgenome} -o {output} {input}
		"""


rule indexing:
	"""
	Indexing bam files with samtools
	"""
	input:
		inputDir+"/{sample}.{aligner}.bam"
	output:
		temp(inputDir+"/{sample}.{aligner}.bai")
	params:
		threads = config['THREADS']
	shell:
		"""
		samtools index -b -@ {params.threads} {input} {output}
		"""

# ouput file is sample.aligner_FLT3_ITD.vcf & sample.aligner_FLT3_ITD_summary.txt (same results as vcf but in txt format)
rule flt3_itd_ext:
	"""
	Flt3_itd_ext is a software that detects internal duplications in bam files
	"""
	input:
		bam = inputDir+"/{sample}.{aligner}.bam",
		bai = rules.indexing.output
	output:
		tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.vcf"
	params:
		output = tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/",
		outputfile = tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/{sample}.{aligner}.vcf",
		assembly = config['ASSEMBLY'],
		index = config['REF_ITD_PATH'],
		ngstype = config['NGS_TYPE'],
		mr = config ['MINREADS']
	shell:
		"""
		perl /app/bin/FLT3_ITD_ext.pl -b {input.bam} -mr {params.mr} -g {params.assembly} --ngstype {params.ngstype} -i {params.index} -o {params.output}; mv {params.outputfile} {output}
		"""

#(grep "^##" {input} && echo '##FORMAT=<ID=GT,Number=1,Type=String,Description="Genotype">' && grep "^#CHROM" {input} | awk -v SAMPLE={wildcards.sample} '{{print $0"\tFORMAT\t"SAMPLE}}' && grep "^#" -v {input} | awk '{{print $0"\tGT\t0/1"}}') > {output}
rule correctvcf:
	"""
	Correction of vcf output, add genotype to be consistent with the vcf format specification.
	"""
	input:
		rules.flt3_itd_ext.output
	output:
		temp(tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.fixGT.vcf")
	shell:
		"""
		cat {input} > {output}
		"""

rule bcftools_filter:
	"""
	Filter with bcftools
	"""
	input:
		rules.correctvcf.output
	output:
		temp(tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.filter.vcf")
	params:
		bcffilter = config['BCFTOOLS_FILTER']
	shell:
		"""
		bcftools view {params.bcffilter} {input} -o {output} 
		"""

rule sortvcf:
	"""
	Bash script to sort a vcf
	"""
	input:
		rules.bcftools_filter.output
	output:
		temp(tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.sort.vcf")
	params:
		dummypath = config['DUMMY_PATH']
	shell:
		"""
		grep \"^#\" {input} > {output} && grep -v \"^#\" {input} | sort -k1,1V -k2,2g >> {output} && [[ -s {output} ]] || cat {params.dummypath}/empty.vcf | sed 's/SAMPLENAME/{wildcards.sample}/g' > {output}
		"""		

# bcftools need vcf.gz.tbi index for the merge all
rule vcf2gz:
	"""
	Compress vcf with bgzip
	"""
	input:
		rules.sortvcf.output
	output:
		 tmpDir+"/{sample}/"+serviceName+"/{sample}_"+date_time+"_"+serviceName+"/"+serviceName+"."+date_time+".{sample}.{aligner}.vcf.gz"
	shell:
		"""
		bgzip -c {input} > {output} ; tabix {output}
		"""

rule cpvcffull:
	"""
	Copy or merge with bcftools several vcfs
	"""
	input:
		expand(tmpDir + "/{sample}/" + serviceName + "/{sample}_" + date_time + "_" + serviceName + "/" + serviceName + "." + date_time + ".{sample}.{aligner}.vcf.gz", sample=sample_list, aligner=aligner_list)
	output:
		tmpDir + "/" + serviceName + "." + date_time + ".all.vcf.gz"
	shell:
		"""
		if [ {sample_count} -eq 1 ]
		then
			cp {input} {output} && tabix {output}
		else
			bcftools merge {input} -O z -o {output} && tabix {output}
		fi
		"""


onstart:
	shell("touch "+config['OUTPUT_DIR']+"/"+serviceName+"Running.txt")
	# Add the snakemake parameters to log
	with open(logfile, "a+") as f:
		json.dump(config, f)
		f.write("\n")

onsuccess:
	include = config['INCLUDE_RSYNC']
		
	shell("touch "+config['OUTPUT_DIR']+"/"+serviceName+"Complete.txt")
	shell("rm -f "+config['OUTPUT_DIR']+"/"+serviceName+"Running.txt")
	
	date_time_end = datetime.now().strftime("%Y%m%d-%H%M%S")
	with open(logfile, "a+") as f:
		f.write("End of the analysis : ")
		f.write((date_time_end) + "\n")
	for sample in sample_list:
		shell("rm -f "+config['OUTPUT_DIR']+"/"+sample+"/"+serviceName+"/* || true") # if rm stderr, the true will avoid exiting snakemake
	shell("rsync -azvh --include={include} --exclude='*' {tmpDir}/ "+config['OUTPUT_DIR']+"")
	for sample in sample_list:
		shell("cp "+config['OUTPUT_DIR']+"/"+sample+"/"+serviceName+"/"+sample+"_"+date_time+"_"+serviceName+"/* "+config['OUTPUT_DIR']+sample+"/"+serviceName+"/ || true")
	if config['DEPOT_COPY'] == True:
		shell("rsync -azvh --include={include} --exclude='*' {tmpDir}/ {depotdir}")
		for sample in sample_list:
			shell("cp "+depotdir+sample+"/"+serviceName+"/"+sample+"_"+date_time+"_"+serviceName+"/* "+depotdir+sample+"/"+serviceName+"/ || true")

onerror:
	include_log = config['INCLUDE_LOG_RSYNC']
	
	shell("touch "+config['OUTPUT_DIR']+"/"+serviceName+"Failed.txt")
	#shell("rm -f "+config['OUTPUT_DIR']+serviceName+"Running.txt")
	shell("rsync -azvh --include={include_log} --exclude='*' {tmpDir}/ "+config['OUTPUT_DIR']+"")