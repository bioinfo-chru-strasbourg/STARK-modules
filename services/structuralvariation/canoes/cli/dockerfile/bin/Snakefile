###############################
###   Import libraries   ######
###############################


import os
import pandas as pd
import subprocess
import time
import re
import shutil
import time
from datetime import datetime, timedelta
import argparse
import json
import csv
from os.path import join as osj
from collections import defaultdict

###########################################
#####   Configuration file and PATHS   ####
###########################################

'''
Basic usage:
snakemake --snakefile /app/bin/Snakefile --configfile /app/config/default.yaml --config runPath=<AbosluteRunPath> -j 4
j: number of jobs to run in parallele
'''

###################
###   Handler   ###
###################

'''
Configfile: contains all default config to run CANOES analysis, on start create a json file containing config used for this analysis
'''

configfile: "/app/config/default.yaml"
date = datetime.now().strftime("%Y%m%d-%H%M%S")

onstart:
	print("#[INFO] Start analysis, write config in log or service folder ...")
	if not os.path.exists(config["outputPath"]):
		os.mkdir(config["outputPath"])
	anaconfig_name = "CANOES-"+date+"-NAME-"+os.path.basename(config["runPath"])+".config.json"
	if config["service"]:
		anaconfig = osj(config["service"], anaconfig_name)
	else:
		anaconfig = osj(config["outputPath"], "logs", anaconfig_name)
		if not os.path.exists(os.path.dirname(anaconfig)):
			os.mkdir(os.path.dirname(anaconfig))
	print("#[INFO] config "+anaconfig)
	with open(anaconfig, "w+") as f:
		json.dump(config, f)
onsuccess:
	print("#[INFO] Analysis reach then end")
onerror:
	if "repositoryPath" in config:
		print("ERROR snakemake execution launch handling error")
		copylog()
	else:
		print("ERROR snakemake")

########################
###   Python Func   ####
########################

def systemcall(command):
	'''
	*passing command to the shell*
	*return list containing stdout lines*
	command - shell command (string)
	'''
	p = subprocess.Popen([command], stdout=subprocess.PIPE, shell=True)
	return p.stdout.read().decode('utf8').strip().split('\n')

def bedFromrun(runPath, out):
	'''
	runPath: absolute path of run to launch analysis, 
	out: temporary name of bed file found in run located in outputPath by default /app/res/analysis_tmp.bed
	return: Absolute path of bed file found in run, <sample>.bed
	'''
	if os.path.exists(runPath):
		for samples in os.listdir(runPath):
			if os.path.isdir(osj(runPath, samples)):
				bed = systemcall('find '+osj(runPath, samples)+' -maxdepth 2 -name '+samples+'".bed"')[0]
				if bed:
					shutil.copy2(bed, out)
					print("#[INFO] Copy "+bed+" into "+os.path.dirname(output))
					return bed
		bedset = systemcall('find '+runPath+' -maxdepth 2 -name "*.bed"')[0]
		if bedset:
			shutil.copy2(bedset, out)
			print("#[INFO] Copy "+bedset+" into "+os.path.dirname(output))
			return bedset
		else:
			print("ERROR No Bed in "+runPath+" EXIT")
			exit()
	else:
		print("ERROR "+runPath+" does not exists EXIT")
		exit()

def parse_samplesheet(samplesheet_path):
	'''
	samplesheet_path: absolute path of a samplesheet file, Illumina format
	return: a dataframe with only data field, it contains sample ID index manifests and so on
	'''
	samplesheet_data = []
	samplesheet_header = []
	with open(samplesheet_path, 'r') as f:
		v = False
		for lines in f:
			lines = lines.strip()
			if v:
				samplesheet_data.append(lines.split(','))
			if 'Sample_ID' in lines:
				v = True
				samplesheet_header.append(lines.split(','))
	df = pd.DataFrame(samplesheet_data, columns=samplesheet_header)
	#sample ID
	sample_list = df.iloc[:, 0].tolist()

	#return dataframe with full SS's samples informations
	return df

def getSampleInfos(samplesheet, runPath):
	'''
	samplesheet: absolute path of a samplesheet file, Illumina format
	runPath: absolute path of run to launch canoes analysis
	return: dict with sampleID from samplesheet as key, values is a dict containing 'bam': <absolutepath of bam>, 'sex': 'F or M or nothing'
	'''
	infos = {}
	if samplesheet:
		parse_samplesheet(samplesheet)
		for i, rows in parse_samplesheet(samplesheet).iterrows():
			if not any(exclude in rows['Sample_ID'] for exclude in config['exclude_sample']):
				sampleID = rows["Sample_ID"]
				infos[sampleID] = {}
				bam = systemcall("find "+osj(runPath, sampleID)+" -maxdepth 2 -name '*.bam' ! -name '*validation*'")[0]
				infos[sampleID]['bam'] = bam
				tags = rows['Description'].split('!')
				#print("#[INFO] tag ", tags)
				for tag in tags:
					if 'SEX' in tag and '_' in tag:
						sex = tag.split('_')[-1]
						infos[sampleID]['sex'] = sex
						break
					elif 'SEX' in tag and '#' in tag:
						sex = tag.split('#')[-1]
						infos[sampleID]['sex'] = sex
						break
					else:
						sex = ''
						infos[sampleID]['sex'] = sex
						break
					
	print("INFO OK",infos)
	return infos

def getSampleInfosFromRun(runPath):
	'''
	runPath: absolute path of run to launch canoes analysis
	return: dict with sampleID from samplesheet as key, values is a dict containing 'bam': <absolutepath of bam>, 'sex': 'F or M or nothing'
	'''
	infos = {}
	for sampleID in os.listdir(runPath):
		if os.path.isdir(osj(runPath, sampleID)):
			if not any(exclude in sampleID for exclude in config["exclude_sample"]):
				infos[sampleID] = {}
				bam = systemcall("find "+osj(runPath, sampleID)+" -maxdepth 2 -name '*.bam' ! -name '*validation*'")[0]
				infos[sampleID]['bam'] = bam
				#print("#[INFO] SampleID ", sampleID)
				#print(runPath)
				tag = systemcall("find "+osj(runPath, sampleID)+" -maxdepth 2 -name '"+sampleID+".tag'")[0]
				if tag:
					with open(tag, 'r') as tags:
						for tag in tags:
							row = tag.strip().split('!')
							#print("#[INFO] row tag ", row)
							for tag in row:
								if 'SEX' in tag and '_' in tag:
									sex = tag.split('_')[-1] 
									infos[sampleID]['sex'] = sex
									break
								elif 'SEX' in tag and '#' in tag:
									sex = tag.split('#')[-1] 
									infos[sampleID]['sex'] = sex
									break
								else:
									sex = ''
									infos[sampleID]['sex'] = sex
									break
				else:
					#print("WARNING "+sampleID+" does not have a tag file")
					infos[sampleID]['sex'] = ''
	return infos

def getSexfromPed(): ##TODO informations from sample dir and Tag or pedigree
	return

def checkWarnings(output_tsv):
	'''
	This function allowed user to have potentionnal warning comming from CANOES analysis or if sex have not been provided by the user, it will also contain the sample used for analysis (regarding sex) and how many sample were used
	output_tsv: absolute path of temporary TSV containing SV variants from CANOES analysis annotated with AnnotSV 3.1
	return: 
		sex_all: dict of list (sex as keys) containing sample regarding sex  ex: {'M': ['ASG184957',....], 'A', ['ASG184957', .., n-sample], 'F':['ADN2104279',...]}
		sex_warnings: dict keys SEX#F SEX#A SEX#M, and values are text file containing warning ex: SEX#F': '##Sexual chromosomes - Samples used for CANOES Analysis with, it will be used to split those warninig inside each tsv files 
	'''
	print("#[INFO] sexlist: ",sexlist)
	sex_all = defaultdict(list)
	for samp, dicovalues in infos.items():
		print("#[INFO] ", (samp, dicovalues))
		if dicovalues['sex']:
			sex_all[dicovalues['sex']].append(samp)
		sex_all['A'].append(samp)
	sex_warning = {}
	print(sex_all)
	for sex in sexlist:
		print("#[INFO] SEX: ",sex)
		if os.path.exists(osj(output, "CANOES", "script_create."+sex+".R")):
			print("#[INFO] Script R ", osj(output, "CANOES", "script_create."+sex+".R"))
			#with open(output+"/{sample}/CANOES/{sample}.canoes.annotsv.tsv", "w+") as out:

			grep = systemcall("grep -A1 'Error\|Warning' "+osj(output, "logs", "canoes."+sex+".log"))[0]
			print(grep)
			if "Error" in grep or "Warning" in grep:
				if sex == 'A' and len(sexlist) > 2:
					sex_warning["SEX#"+sex] = "##Autosomal chromosomes - /!\/!\/!\ A WARNING message appears, please check your results CAREFULLY: "+grep+"\n"
				elif sex == 'A' and len(sexlist) == 1:
					sex_warning["SEX#"+sex] = "##Autosomal chromosomes - /!\/!\/!\ A WARNING message appears, please check your results CAREFULLY: "+grep+"\n##BECAREFULL: /!\/!\/!\ CANOES launched without sexual chromosomes\n"
				elif sex == 'F' or sex == 'M':
					sex_warning["SEX#"+sex] = "##Sexual chromosomes - /!\/!\/!\ A WARNING message appears, please check your results CAREFULLY: "+grep+"\n"
			else:
				if sex == 'A' and len(sexlist) > 1:
					sex_warning["SEX#"+sex] = "##Autosomal chromosomes: Samples used for CANOES Analysis: "
				elif sex == 'A' and len(sexlist) == 1:
					sex_warning["SEX#"+sex] = "##BECAREFULL: /!\/!\/!\ CANOES launched without sexual chromosomes\n##Autosomal chromosomes: Samples used for CANOES Analysis: "
				elif sex == 'F' or sex == 'M':
					sex_warning["SEX#"+sex] = "##Sexual chromosomes - Samples used for CANOES Analysis with "
		else:
			print("ERROR no script R files generated")
			break
	print("#[INFO] sex all ", sex_all)
	print("#[INFO] sex warnings ", sex_warning)
	with open(output_tsv, "w+") as out:
		sample = os.path.basename(output_tsv).split('.')[0]
		if sample != "all":
			if infos[sample]['sex'] and infos[sample]['sex'] in sexlist:
				sex = "SEX#"+infos[sample]['sex']
				sexl = infos[sample]['sex']
				print("#[INFO] Sample ", sample)
				out.write("##Sex for sample "+sample+" is : "+infos[sample]['sex']+"\n")
				if not 'WARNING' in sex_warning[sex]:
					out.write(sex_warning[sex] +sex+" "+",".join(sex_all[sexl])+"\n")
				else:
					out.write(sex_warning[sex])
			elif not infos[sample]['sex']:
				out.write("##Sex is UNKNOWN for sample "+sample+" \n")
				if not 'WARNING' in sex_warning["SEX#A"]:
					out.write(sex_warning["SEX#A"] + str(len(sex_all["A"]))+" | "+ ','.join(sex_all["A"])+" Analysis Ended Well !\n")
				else:
					out.write(sex_warning["SEX#A"]+"\n")
				
		elif sample == "all":
			if "SEX#A" in sex_warning:
				if not 'WARNING' in sex_warning["SEX#A"]:
					out.write(sex_warning["SEX#A"] + str(len(sex_all["A"]))+" | "+ ','.join(sex_all["A"])+" Analysis Ended Well !\n")
				else:
					out.write("##Autosomal chromosomes: Samples used for CANOES Analysis: "+ ','.join(sex_all["A"])+" Analysis Ended Well !\n")
	return sex_all, sex_warning
	
def splitannotations(tsv, output):
	'''
	splitannotations the annotation by annotsv is run only one time then SV event will be split in order to have one tsv file by sample
	tsv: absolute path of temporary tsv file containint analysis for all sample with annotations 'all.canoes.annotsv.tmp.tsv'
	output: absolute path of tsv for one sample
	'''
	with open(tsv) as f:
		row = f.readline().strip()
		if row == "No data to annotate":
			for ts in output:
				with open(ts, "w+") as t:
					t.write("No data to annotate")
		else:
			df = pd.read_csv(tsv, sep='\t', header=0)
			#print(df.head())
			dico = {}
			samples_unique = df['Samples_ID'].unique()
			for samp in samples_unique:
				dico[samp] = [] 
				for i, rows in df.iterrows():
					if rows['Samples_ID'] == samp:
						dico[samp].append(rows)
			print("#[INFO] Dico ", dico)
			for s in output:
				sample_name = os.path.basename(s).split('.')[0]
				with open(s, "w+") as out:
						if sample_name in dico:
							out.write('\t'.join(df.columns))
							df_sample = pd.DataFrame(dico[sample_name], columns=df.columns)
							df_sample.to_csv(s, header=True, sep="\t", index=False, mode="a+")

def checkSizeBatch(dictinfos):
	'''
	#TODO dev
	if there are less than 3 sample for a sex analysis, it will search in group folder(STARK repository) to add more sample
	'''
	notenough = []
	sexf = 0
	sexm = 0
	for samples in dictinfos.values():
		if samples['sex'] == 'M':
			sexm += 1
		elif samples['sex'] == 'F':
			sexf += 1
	if sexf <= 3 and sexf != 0:
		notenough.append('M')
	elif sexm <= 3 and sexm != 0:
		notenough.append('F')
	#print("#[INFO] sex M "+str(sexm)+", sex F "+str(sexf)+" notenough ",notenough)
	return notenough

def addOtherSamples(sexless_list, sexlist, infos, run, exclude_tag):
	'''
	#TODO dev
	if there are less than 3 sample for a sex analysis, it will search in group folder(STARK repository) to add more sample
	'''
	new_sample_list = []
	#Path of project of the run 
	project = os.path.dirname(run)
	runID = os.path.basename(run)
	#if run is not in STARK repository
	if os.path.exists(project):
		#check for run newer than 30 days
		searchrun = systemcall("find "+project+" -maxdepth 1 -mindepth 1 -type d -mtime -30 ! -name "+runID)
		if not searchrun:
			#check for run newer than 60 days
			searchrun = systemcall("find "+project+" -maxdepth 1 -mindepth 1 -type d -mtime -60 ! -name "+runID)
			#if any other run than the one analyzed
			if not searchrun:
				print("#WARNING no others run for this project sex without enough sample sex will bed discarded from analysis")
				for v in sexless_list:
					sexlist.remove(v)
				return infos, sexlist, new_sample_list
			else:
				runlist = searchrun
		else:
			runlist = searchrun
		#if batch analyzed have not enough sample for maybe both sexM and sexF
		for x in sexless_list:
			infos_supp = {}
			print("#[INFO] len infos_supp", len(infos_supp))
			print("#[INFO] RUNLIST ",runlist)
			#for each run found in group project associated to the run analyzed
			for runs in (runlist):
				#if the run is in STARK repository format
				if os.path.exists(osj(runs, "STARKCopyComplete.txt")):
					print("#[INFO] runs",runs)
					#for each samples in run folder
					for sampleID in os.listdir(runs):
						#keep only folder which are folder 
						if os.path.isdir(osj(runs, sampleID)):
							print("#[INFO] runsampleID", sampleID)
							#looking for tag file which have sex informations 
							tags = systemcall("find "+osj(runs, sampleID)+" -maxdepth 2 -name "+sampleID+".tag")[0]
							print("#[INFO] tagfile", tags)
							if tags:
								row = open(tags, 'r').readline().strip().split('!')
								#print("#[INFO) rowtag", row)
								for tag in row:
									if 'SEX' in tag and '_' in tag:
										sex = tag.split('_')[-1] 
									elif 'SEX' in tag and '#' in tag:
										sex = tag.split('#')[-1]
									#if tag is in exclude tag set sex to unknown and continue over other sample
									elif any(exclude in tag for exclude in exclude_tag):
										sex = ''
									else:
										sex = ''
									break
								if sex == x:
									print("INFO SEX", sex)
									#find bam file in samples 
									bam = systemcall("find "+osj(runs, sampleID)+" -maxdepth 2 -name '*.bam' ! -name '*validation*'")[0]
									print("#[INFO] BAMfile ", bam)
									if sex and bam and len(infos_supp) < 4:
										infos_supp[sampleID] = {}
										infos_supp[sampleID]['sex'] = sex
										infos_supp[sampleID]['bam'] = bam
										new_sample_list.append(sampleID)
									else:
										print("##############[INFO] dictsupp", infos_supp)
										print("##########################################")
										infos.update(infos_supp)
										return infos, sexlist, new_sample_list
		else:
			print("#WARNING no others run for this project, sex without enough sample will be discarded from analysis")
			for v in sexless_list:
				sexlist.remove(v)
			return infos, sexlist, new_sample_list

def createplotstats(coverage):
	'''
	from all.canoes.coverage.tsv it will generate necessary stats to plots CANOES analysis bar and boxplot
	return dico with stats
	'''
	with open(coverage, 'r') as file:
		dico = {'header': [], 'depth': []}
		for line in file:
			if line.startswith('chrom'):
				line = line.strip().split('\t')
				add = ['Chr:Start-End']
				add.extend(line[3:])
				dico['header'].extend(add)
			elif line.startswith('chrX') or line.startswith('chrY'):
				continue
			else:
				metrics = []
				line = line.strip().split('\t')
				calcs = line[3:]
				#From list of string to list of int
				calcs = list(map(int, calcs))
				for depth in calcs:
					try:
						metrics.append(str(round(float(depth / (int(line[2]) - int(line[1]))), 4)).replace(',', '.'))
					except ZeroDivisionError:
						metrics.append(0)
				newline = line[0]+':'+line[1]+'-'+line[2]+'\t'+'\t'.join(metrics)
				dico['depth'].append(newline)
	return dico

def modifyfilename(filename, typo):
	'''
	from filename which could be file or folder specify in typo( could be optimized by os library)
	return filename modified with date and good formalism
	'''
	if typo == 'file':
		print('INFO file ', filename)
		#basename
		bs = os.path.basename(filename)
		#extension
		extension = bs.split('.')[-1]
		#basename without extension
		sample = bs.split('.')[0]
		out = osj(os.path.dirname(filename), sample+'.CANOES.'+date+'.variants.Design.'+extension)
		print("INFO OUT", out)
		return out
	elif typo == 'folder':
		print('INFO dir ', filename)
		folder = os.path.basename(filename)
		dirname = os.path.dirname(filename)
		out = osj(dirname, folder+'.'+date)
		print("INFO OUT", out)
		return out

def compressindex(vcf):
	'''
	vcf: absolute paht of vcf file (conf samtools vcf specifications)
	return vcf compressed by bgzip and indexed by tabix
	'''
	systemcall("bgzip -c "+vcf+" > "+vcf+".gz && tabix -p vcf "+vcf+".gz")
	return vcf+".gz"

def filteronpanel(vcf, sample):
	'''
	#TODO
	still in dev to split SV events regarding panel file(.genes conf STARK speficiation)
	'''
	filename_path, extension = os.path.splitext(vcf)
	filename = os.path.basename(filename_path)
	
	#vcf_gz = compress(designout)
	panel_file = systemcall("find "+config['runPath']+" -maxdepth 3 -name '*.list.genes' -print -quit")[0]
	#/home1/data/STARK/output/repository/HUSDIAGGEN/GOMV1_GERMLINE/211220_M01658_0629_000000000-K2BJC/SGT2101924/STARK/SGT2101924.list.genes
	print(panel_file)
	panel_list = []
	with open(panel_file) as f:
		for lines in f:
			panel_list.append(lines.strip())
	print(panel_list)
	vcf_gz = compressindex(vcf)
	for panel in panel_list:
		#Compress and index vcf_gz
		
		#PAnel GOMVV1....
		panel_name = '.'.join(panel.split('.')[1:])
		#CANOES.20220111-164708.variants.Panel.GOMV1_GERMLINE.manifest.genes.vcf.gz
		panelout = sample+"."+date+".variants.Panel."+panel_name+".vcf.gz"
		systemcall("bcftools view -R "+osj(os.path.dirname(panel_file), panel)+" "+vcf_gz+" -O z -o "+osj(config['outputPath'], sample, 'CANOES', panelout))
	return osj(config['outputPath'], sample, 'CANOES', panelout)

def copylog():
	'''
	in case of SNAKEMAKE crash copy log folder in each samples
	'''
	if os.path.exists(osj(config['outputPath'], "logs")):
		print("WARNING Error in Snakemake execution Copy logs in "+config['outputPath']+" samples")
		for sample in infos:
			if os.path.isdir(osj(config['repositoryPath'], sample)):
				print("INFO ",sample)
				if not os.path.exists(osj(config['repositoryPath'], sample, 'CANOES')):
					os.mkdir(osj(config['repositoryPath'], sample, 'CANOES'))
					shutil.copytree()
				else:
					listfile = [folder for folder in os.listdir(osj(config['repositoryPath'], sample, "CANOES")) if os.path.isdir(osj(config['repositoryPath'], sample, 		"CANOES", 	folder)) and 'logs' in folder]
					print("#[INFO] Listfie ", listfile)
					old = sample+'.'+listfile[0].split('.')[-1]
					print("#[INFO] Create "+osj(config['repositoryPath'], sample, "CANOES", old))
					os.mkdir(osj(config['repositoryPath'], sample, "CANOES", old))

					for content in os.listdir(osj(config['repositoryPath'], sample, "CANOES" )):
						if os.path.isfile(osj(config['repositoryPath'], sample, "CANOES", content)):
							shutil.move(osj(config['repositoryPath'], sample, "CANOES", content), osj(config['repositoryPath'], sample, "CANOES", old))
						else:
							shutil.move(osj(config['repositoryPath'], sample, "CANOES", content), osj(config['repositoryPath'], sample, "CANOES", old))
					
	else:
		print("WARNING anything will be copied, logs folder does not exists, EXIT")

def createbedigv():
	#TODO
	return

##############################
####   Global Variables   ####
##############################

#if not Samplesheet in configfile get sample and sex informations directly in the runPath
if config["samplesheetPath"]:   #TODO infos from samplesheet optionnal
	print(config["samplesheetPath"])
	infos = getSampleInfos(config["samplesheetPath"], config['runPath'])
else:
	infos = getSampleInfosFromRun(config['runPath'])
#print("#[INFO] infos ", infos)
samplelist = [items for items in infos]

sexlist = list(set([sample["sex"] for sample in infos.values()]))

if not config['analysis']:
	if not sexlist:
		sexlist = ['A']
	else:
		sexlist.append('A')
else:
	sexlist = config['analysis']
#correct potentially wrong sex
for elem in sexlist:
	if elem not in config['valid_sex']:
		sexlist.remove(elem)

output = config['outputPath']
log = osj(config['outputPath'], "logs")

###############
##   Rules
###############

### Ruleorder
ruleorder:
		merge_CNVs > warnings_message_all > warnings_message > AnnotSV > plot_coverage 

#Copy sample and log in each sample if set, it will handle old or crash analysis
if config['repositoryPath']:
	rule all:
		input:
			samp = expand(output+"/{sample}/CANOES/{sample}.canoes.annotsv.vcf", sample=samplelist),
			tsv = expand(output+"/{sample}/CANOES/{sample}.canoes.annotsv.tsv", sample=samplelist),
			#tsv_tmp = expand(output+"/{sample}/CANOES/{sample}.canoes.annotsv.tmp.tsv", sample=samplelist)
			tsv_all = output+"/CANOES/all.canoes.annotsv.tsv",
			vcf_all = output+"/CANOES/all.canoes.annotsv.vcf"
		output:
			output+"/CANOES/CANOESComplete.txt"
		log:
			logfile = log+"/copyrepository.log"
		run:
			with open(log.logfile, 'w+') as lg:
				with open(output[0], "w+") as f:
					f.write("# ["+(datetime.now() + timedelta(hours = 2)).strftime("%d/%m/%Y %H:%M:%S")+"] "+config['runPath']+" analyzed with CANOES\n")

				#vcf per sample
				for sample in samplelist:
					#if os.path.basename(sampl).split('.')[0] in new_sample_list:
					#	lg.write("#[INFO] sample added from other run "+sampl)
					#	continue

					###TSV
					tsv = sample+".canoes.annotsv.tsv"
					all_tsv = "all.canoes.annotsv.tsv"

					###VCF
					vcf = sample+".canoes.annotsv.vcf"
					all_vcf = "all.canoes.annotsv.vcf"

					print("#[INFO] Sample copy"+sample)
					lg.write("#[INFO] Sample copy"+sample)

					#if repository exists and is folder
					if os.path.exists(osj(config['repositoryPath'], sample)) and os.path.isdir(osj(config['repositoryPath'], sample)):
						#If its the first CANOES analysis
						if not os.path.exists(osj(config['repositoryPath'], sample, "CANOES")):
							os.mkdir(osj(config['repositoryPath'], sample, "CANOES"))
						
						#Gestion Archivage, seconde ore more CANOES analysis
						else:
							#name canoes old analysis folder
							listfile = [folder for folder in os.listdir(osj(config['repositoryPath'], sample, "CANOES")) if os.path.isdir(osj(config['repositoryPath'], sample, "CANOES", folder)) and 'logs' in folder]
							print("#[INFO] Listfile ", listfile)
							#old = listfile[0].split('.')[-1]
							old = sample+'.'+listfile[0].split('.')[-1]


							print("#[INFO] Create "+osj(config['repositoryPath'], sample, "CANOES", old))
							os.mkdir(osj(config['repositoryPath'], sample, "CANOES", old))
							for content in os.listdir(osj(config['repositoryPath'], sample, "CANOES" )):
								if content != old:
									if os.path.isfile(osj(config['repositoryPath'], sample, "CANOES", content)):
										shutil.move(osj(config['repositoryPath'], sample, "CANOES", content), osj(config['repositoryPath'], sample, "CANOES", old))
									elif os.path.isdir(osj(config['repositoryPath'], sample, "CANOES", content)) and content.startswith('logs'):
										shutil.move(osj(config['repositoryPath'], sample, "CANOES", content), osj(config['repositoryPath'], sample, "CANOES", old))
							#move old file
							print("#[INFO] Cleaning "+config['repositoryPath'])

							systemcall("find "+osj(config['repositoryPath'], sample)+" -mindepth 1 -maxdepth 1 -name '*.CANOES.*' -exec rm -f {} \;")
							systemcall("find "+config['repositoryPath']+" -maxdepth 2 -name 'CANOES.*' -exec rm -f {} \;")

						#vcf and put in sample root
						sampl_out = modifyfilename(osj(config['repositoryPath'], sample, "CANOES", vcf), 'file')
						shutil.copy2(osj(config['outputPath'], sample, "CANOES", vcf), sampl_out)
						shutil.copy2(sampl_out, osj(config['repositoryPath'], sample))
						

						#vcf all
						vcf_all_out = modifyfilename(osj(config['repositoryPath'], sample, "CANOES", all_vcf), 'file')
						shutil.copy2(osj(config['outputPath'], "CANOES", all_vcf), vcf_all_out)

						#tsv and put in sample root
						tsv_out = modifyfilename(osj(config['repositoryPath'], sample, "CANOES", tsv), 'file')
						shutil.copy2(osj(config['outputPath'], sample, "CANOES", tsv), tsv_out)
						shutil.copy2(sampl_out, osj(config['repositoryPath'], sample))

						#tsv_all
						tsv_all_out = modifyfilename(osj(config['repositoryPath'], sample, "CANOES", all_tsv), 'file')
						shutil.copy2(osj(config['outputPath'], "CANOES", all_tsv), tsv_all_out)

						#log folder
						log_out = modifyfilename(osj(config['repositoryPath'], sample, "CANOES", "logs" ), 'folder')
						shutil.copytree(osj(config['outputPath'], "logs"), log_out)
					else:
						print("WARNING "+sample+" in "+config['repositoryPath']+" does not exist or is not a directory")
						break
				
				#copy vcf all at root of the run
				out_vcf = 'CANOES.'+date+'.variants.Design.vcf'
				shutil.copy2(input.vcf_all, osj(config['repositoryPath'], out_vcf))
				
				#Depository copying
				if config['depositoryPath']:
					for s in samplelist:
						print("#[INFO] Rsync sample "+s+" in depository "+config['depositoryPath'])
						print('rsync -ar '+osj(config['repositoryPath'], s, "CANOES")+' '+osj(config['depositoryPath'], s+'/'))
						systemcall('rsync -ar '+osj(config['repositoryPath'], s, "CANOES")+' '+osj(config['depositoryPath'], s+'/'))
				
				#finish files
				if os.path.exists(osj(config['repositoryPath'], "CANOESRunning.txt")):
					os.remove(osj(config['repositoryPath'], "CANOESRunning.txt"))
				shutil.copy2(output[0], osj(config['repositoryPath']))
else:
	rule all:
		input:
			output+"/CANOES/all.canoes.annotsv.vcf",
			expand(output+"/{sample}/CANOES/{sample}.canoes.annotsv.vcf", sample=samplelist)
		output:
			output+"/CANOES/CANOESComplete.txt"
		run:
			with open(output[0], "w+") as f:
				f.write("# ["+(datetime.now() + timedelta(hours = 2)).strftime("%d/%m/%Y %H:%M:%S")+"] "+config['runPath']+" analyzed with CANOES\n")

#If bed is not in run specify bedPath: <absolute path of bedfile>
if config["bedPath"]:
	rule get_bed:
		input:
			config["bedPath"]
		output:
			output+"/CANOES/analysis_tmp.bed"
		shell:
			"""
				cp {input} {output}
			"""

#get bedfile from run
else:
	rule get_bed_fromrun:
		input:
			run = config["runPath"]
		output:
			output+"/CANOES/analysis_tmp.bed"
		run:
			bedFromrun(input[0], output[0])

if config["splitbed"]:
	'''
	Splitbed option, cut bed regions by kmer size define by user ex 'splitbed': '50'    |  chr1	2336210	2336360  into  chr1 2336210 2336260\nchr1 2336260 2336310\nchr1	2336310	2336350
	'''
	rule split_bed:
		params:
			src = config["src"],
			splitvalue = config["splitbed"]
		input:
			output+"/CANOES/analysis_tmp.bed"
		output:
			output+"/CANOES/analysis.bed"
		log:
			log+"/splitbed.log"
		shell: 
			"""
				python {params.src}/kmer.py -i {input} -k {params.splitvalue} -o {output} &> {log} 
			"""
else:
	#No split bed
	rule rename_bed:
		input:
			output+"/CANOES/analysis_tmp.bed"
		output:
			output+"/CANOES/analysis.bed"
		shell: 
			"""
				cp {input} {output} 
			"""

rule merge_CNVs:
	'''
	Merge SV event in one file to process only one annotation from annoSV 3.1
	'''
	input:
		expand(output+"/{sample}/CANOES/{sample}.canoes.annotsv.tsv", sample=samplelist),
	output:
		output+"/CANOES/all.canoes.annotsv.tmp.tsv"
	run:
		print("#[INFO] infos ", infos)
		print("#[INFO] SampleList ",samplelist)
		print("#[INFO] sexlist used ", sexlist)
		with open(output[0], "a+") as o:
			print_header = True
			for file in input:
				ok_header = False
				with open(file, "r") as i:
					for line in i:
						if not ok_header:
							#Depending of AnnotSV version
							if line.startswith("AnnotSV ID") or line.startswith("AnnotSV_ID"):
								ok_header = True
								if print_header:
									print_header = False
									o.write(line)
						else:
							o.write(line)

rule bam_to_multicov:
	'''
	From bam file generate a cov file, multiprocess in python
	'''
	params:
		src = config["src"]
	input: 
		bam = lambda wildcards: infos[wildcards.sample]['bam'],
		bedfile = output+"/CANOES/analysis.bed"
	log:
		log+"/{sample}.multicov.log"
	benchmark:
		log+"/{sample}.multicov.benchmark.txt"
	output:
		output+"/{sample}/CANOES/{sample}.multicov.tsv"
	shell:
		"""
			echo '#[INFO] {input.bam}' && python {params.src}/pybedtools.py --bam {input.bam} --bed {input.bedfile} -q 20 -o {output} 2> {log}
		"""
rule gc_percent:
	'''
	Calculate GC percent only once depending on Bed file 
	'''
	params:
		gatk = config["gatk"],
		java = config["java"]
	input:
		bed = output+"/CANOES/analysis.bed",
		genome = config["genomePath"]
	log:
		log+"/GCpercent_to_change.log"
	output:
		temp(output+"/CANOES/GCpercent_to_change.tsv"),
	shell:
		"""
			{params.java} -Xmx2000m -jar {params.gatk} AnnotateIntervals -L {input.bed} -R {input.genome} -imr OVERLAPPING_ONLY -O {output} &> {log} 
		"""

rule correct_gc_percent_file:
	'''
	fix formating of GC file
	'''
	input:
		output+"/CANOES/GCpercent_to_change.tsv",
	log:
		log+"/GCpercent.log"
	output:
		temp(output+"/CANOES/GCpercent.tsv")
	shell:
		"""
			cat {input} | awk 'NR>27{{print $0}}' > {output}
		"""

rule sex_gc_percent:
	'''
	fix formating of GC file for sex analysis
	'''
	input:
		output+"/CANOES/GCpercent.tsv"
	output:
		temp(output+"/CANOES/GCpercent.{sex}.tsv")
	log:
		log+"/GCpercent.{sex}.log"
	shell:
		"""
			if [[ "{wildcards.sex}" == "A" ]]; then
				grep -vE '^chrX|^chrY' {input} > {output};
			else
				cp {input} {output};
			fi;
		"""
rule merge_multicov_to_reads:
	'''
	fix formating
	'''
	input:
		expand(output+"/{sample}/CANOES/{sample}.multicov.tsv", sample=samplelist)
	output:
		output+"/CANOES/all.canoes.coverage.tsv"
	log:
		log+"/all.canoes.coverage.log"
	run:
		data = {}
		header = ["chrom","start","end"]
		for multicovfile in input:
			sample_name = multicovfile.split("/")[-1].split(".")[0]
			header.append(sample_name)
			with open(multicovfile,"r") as i:
				for line in i:
					field = line.strip().split("\t")
					index = "\t".join(field[0:3])
					value = field[-1]
					if index not in data:
						data[index] = {}
					data[index][sample_name] = value

		with open(output[0],"w") as o:
			o.write("\t".join(header)+"\n")
			for index in data:
				sample_value = [index]
				for sample_name in header[3:]:
					sample_value.append(data[index][sample_name])
				o.write("\t".join(sample_value)+"\n")

rule plot_coverage:
	'''
	generate plot stats file conf createplotstats docstring
	'''
	input:
		output+"/CANOES/all.canoes.coverage.tsv"
	output:
		output+"/CANOES/all.canoes.coverage.stats.tsv"
	run:
		values = createplotstats(input[0])
		with open(output[0], 'w+') as o:
			#header
			o.write('\t'.join(values['header'])+'\n')
			for val in values['depth']:
				o.write(val+'\n')
		print("#[INFO] Ending parse "+input[0]+" write output in "+output[0])
		#for samp in output.sampleall:
		#Rscript
		#with open(osj(output, samp, "CANOES", samp+".script.plots.R", "w+")) as sc:

rule write_plot_scripts:
	'''
	write plot script used by R with sample information
	'''
	input:
		output+"/CANOES/all.canoes.coverage.stats.tsv"
	output:
		sampleall = output+"/{sample}/CANOES/{sample}.script.plots.R"
	params:
		plotscript = config['plotscript']
	run:
		with open(output.sampleall, "w+") as sc:
			print("INFO", output.sampleall)
			sample = os.path.basename(output.sampleall).split('.')[0]
			sc.write("setwd(\""+os.path.dirname(output.sampleall)+"\")\n")
			sc.write("source(\""+params.plotscript+"\")\n")
			sc.write("PlotCoverage(\""+sample+"\",\""+input[0]+"\","+"\""+osj(config['outputPath'], sample, "CANOES", sample+".canoes.boxplot.png\")\n"))
			sc.write("PlotCoverage2(\""+sample+"\",\""+input[0]+"\","+"\""+osj(config['outputPath'], sample, "CANOES", sample+".canoes.barplot.png\")\n"))

rule create_plot:
	'''
	generate plot file by R
	'''
	input:
		output+"/{sample}/CANOES/{sample}.script.plots.R"
	output:
		output+"/{sample}/CANOES/{sample}.canoes.boxplot.png",
		output+"/{sample}/CANOES/{sample}.canoes.barplot.png"
	params:
		Rscript = config["Rscript"]
	log:
		log+"/{sample}.plots_coverage.log"
	shell:
		"""
			{params.Rscript} {input} &> {log} && touch {output}
		"""

rule sex_analysis_reads:
	'''
	coverage for sample
	'''
	params:
		data = infos
	input:
		output+"/CANOES/all.canoes.coverage.tsv"
	output:
		output+"/CANOES/all.canoes.{sex}.coverage.tsv"
	run:
		sample_sex = []
		with open(output[0], "w") as f:
			for sample in params.data:
				if wildcards.sex == "A":
					sample_sex.append(sample)
				else:
					print("################")
					print("#[INFO] sex is ", wildcards.sex)
					if params.data[sample]["sex"] == wildcards.sex:
						sample_sex.append(sample)
			print("#[INFO] lis sex ", sample_sex)
			fieldnames = ['chrom','start','end'] + sample_sex
			f.write("\t".join(fieldnames)+"\n")
			with open(input[0], "r") as tsvfile:
				reader = csv.DictReader(tsvfile, delimiter='\t')
				for row in reader:
					line = [ row[x] for x in fieldnames ]
					if wildcards.sex == "A":
						if line[0] == "chrX" or line[0] == "chrY":
							continue
					f.write("\t".join(line)+"\n")


rule convert_chr:
	input:
		"{w}"
	output:
		"{w}.conv"
	shell:
		"""
			sed -e "s/^chrX/chr23/" {input} | sed -e "s/^chrY/chr24/" > {output}
		"""

rule without_header:
	input:
		"{w}"
	output:
		"{w}.no_header"
	shell:
		"""
			sed '1d' {input} > {output}
		""" 

rule write_R_script:
	'''
	write R script to launch canoes analysis
	'''
	params:
		canoes = config["canoes"],
		output = output
	input:
		read   = output+"/CANOES/all.canoes.{sex}.coverage.tsv.conv.no_header",
		header = output+"/CANOES/all.canoes.{sex}.coverage.tsv.conv",
		gc     = output+"/CANOES/GCpercent.{sex}.tsv.conv"
	output:
		output+"/CANOES/script_create.{sex}.R"
	run:
		header = open(input.header, 'r').readline().strip().split()[3:]
		with open(output[0], "w") as f:
			f.write("setwd(\""+ params.output +"\")\n")
			f.write("gc <- read.table(\""+ input.gc +"\")$V2\n")
			f.write("canoes.reads <- read.table(\""+ input.read +"\")\n")
			f.write("sample.names <- c("+ ','.join(f'"{s}"' for s in header) +")\n")
			f.write("names(canoes.reads) <- c(\"chromosome\", \"start\", \"end\", sample.names)\n")
			f.write("target <- seq(1, nrow(canoes.reads))\n")
			f.write("canoes.reads <- cbind(target, gc, canoes.reads)\n")
			f.write("source(\""+ params.canoes +"\")\n")
			f.write("xcnv.list <- vector('list', length(sample.names))\n")
			f.write("for (i in 1:length(sample.names)){\n")
			f.write("xcnv.list[[i]] <- CallCNVs(sample.names[i], canoes.reads ,"+str(config['pval'])+", "+str(config['tnum'])+", "+str(config['dist'])+", "+str(config['numref'])+")\n")
			f.write("}\n")
			f.write("xcnvs <- do.call('rbind', xcnv.list)\n")
			f.write("write.csv(xcnvs, \""+ params.output +"/CANOES/results."+ wildcards.sex +".csv\", row.names=FALSE)\n")

rule canoes_calling:
	'''
	Launch canoes analysis
	'''
	params:
		R       = config["R"],
	input:
		read    = output+"/CANOES/all.canoes.{sex}.coverage.tsv.conv.no_header",
		gc      = output+"/CANOES/GCpercent.{sex}.tsv.conv",
		script  = output+"/CANOES/script_create.{sex}.R"
	output:
		results = output+"/CANOES/canoes.{sex}.tsv",
		tmp     = temp(output+"/CANOES/results.{sex}.csv"),
	log:
		log+"/canoes.{sex}.log"
	shell:
		"""
			{params.R} CMD BATCH {input.script} {log};
			[[ -s {output.tmp} ]] && cat {output.tmp} | sed 's/"//g' | sed 's/,/\t/g' | sed '1d' | sort -u > {output.results} || echo "no CNV found for sex: {wildcards.sex}" > {output.results};
		"""

rule merge_sex:
	'''
	merge all canoes results in once
	'''
		input:
			expand(output+"/CANOES/canoes.{sex}.tsv", sex=sexlist)
		output:
			temp(output+"/CANOES/canoes.tsv")
		shell:
			"""
				cat {input} | sort -u >> {output}
			"""

rule canoes_to_bed:
	'''
	from tsv output of CANOES analysis generate a bed file
	'''
	input:
		output+"/CANOES/canoes.tsv"
	output:
		output+"/CANOES/canoes.bed"
	run:
		with open(output[0], "w") as o:
			o.write("\t".join([ "#Chrom", "Start", "End", "SV type", "Samples_ID" ])+"\n")
			with open(input[0], "r") as i:
				for line in i:
					if line.startswith("no CNV found"):
						o.write(line)
						break
					elif line.startswith("error in the analysis"):
						o.write(line)
						break
					else:
						field = line.strip().split("\t")
						chrom = field[2].split(":")[0]
						if chrom == "23":
							chrom = "X"
						if chrom == "24":
							chrom = "Y"
						start = field[2].split(":")[1].split("-")[0]
						end = field[2].split(":")[1].split("-")[1]
						sample = field[0]
						cnv = field[1]
						o.write("\t".join([ chrom, start, end, cnv, sample ])+"\n")

rule warnings_message:
	input:
		output+"/{sample}/CANOES/{sample}.canoes.boxplot.png",
		output+"/{sample}/CANOES/{sample}.canoes.barplot.png",
		output+"/{sample}/CANOES/{sample}.canoes.annotsv.tmp.tsv"
	output:
		output+"/{sample}/CANOES/{sample}.canoes.annotsv.header.tsv"
	run:
		checkWarnings(output[0])

rule warnings_message_all:
	input:
		output+"/CANOES/all.canoes.annotsv.tmp.tsv"
	output:
		output+"/CANOES/all.canoes.annotsv.header.tsv"
	run:
		checkWarnings(output[0])


rule bedtovcf:
	'''
	from bed from canoes_to_bed rule generate a vcf file, 
	'''
	input:
		output+"/CANOES/canoes.bed"
	output:
		output+"/CANOES/canoes.vcf"
	log:
		log+"/bedtovcf.log"
	params:
		convertype = config["convertype"],
		bed2vcfjson = config["bed2vcfjson"],
		convertscript = config["convertscript"]
	shell:
		"""
			docker run --rm -v /home1:/home1 -v /home1/data/STARK/data:/STARK/data tsvconvert:latest {params.convertscript} -i {input} -o {output} -fo {params.convertype} -fi tsv -c {params.bed2vcfjson} 2>{log} && [[ -s {output} ]] && sed -i '/^#/! s/^/chr/' {output} || echo "No data to annotate" > {output}
		"""

rule AnnotSV:
	'''
	 AnnotSV need bedtools to work ie you have to specify the directory of bedtools binaries in -bedtools argument
	 Annotation process by annotSV 3.1, generate a TSV file
	 '''
	input:
		output+"/CANOES/canoes.vcf"
	output:
		output+"/CANOES/all.canoes.annotsv.tmp.tsv"
	log:
		log+"/all.canoes.annotsv.log"
	params:
		bedtools = config['bedtools'],
		annotsv = config['annotsv'],
		genomebuild = config['genomebuild']
		#-svtBEDcol 4 -samplesidBEDcol 5 
	shell:
		"""
		{params.annotsv}/bin/AnnotSV -SVinputFile {input} -outputFile {output} -bedtools {params.bedtools} -genomeBuild {params.genomebuild} &>{log} && 
		if grep -q 'ERROR' {log}
		then
			exit 1;
		elif [[ -s {output} ]]
		then
			echo "No data to annotate" > {output}
		else
			echo "#[INFO] AnnotSV OK" > {output}
		fi
		"""

rule split_annotations:
	input:
		output+"/CANOES/all.canoes.annotsv.tmp.tsv"
	output:
		expand(output+"/{sample}/CANOES/{sample}.canoes.annotsv.tmp.tsv", sample=samplelist)
	run:
		splitannotations(input[0], output)

rule merge_warnings_variants:
	input: 
		var = output+"/{sample}/CANOES/{sample}.canoes.annotsv.tmp.tsv",
		header = output+"/{sample}/CANOES/{sample}.canoes.annotsv.header.tsv"
	output:
		output+"/{sample}/CANOES/{sample}.canoes.annotsv.tsv"
	shell:
		"""
			cat {input.header} > {output} && cat {input.var} >> {output}
		"""

rule merge_warnings_variants_all:
	input: 
		var = output+"/CANOES/all.canoes.annotsv.tmp.tsv",
		header = output+"/CANOES/all.canoes.annotsv.header.tsv"
	output:
		output+"/CANOES/all.canoes.annotsv.tsv"
	shell:
		"""
			cat {input.header} > {output} && cat {input.var} >> {output}
		"""

rule AnnotSV2vcf:
	'''
	Conversion from TSV file (output of CANOES analysis) to VCF with samtools specifications
	'''
	input:
		tsvtmp = output+"/{sample}/CANOES/{sample}.canoes.annotsv.tmp.tsv",
		tsv = output+"/{sample}/CANOES/{sample}.canoes.annotsv.tsv"
	output:
		output+"/{sample}/CANOES/{sample}.canoes.annotsv.vcf"
	params:
		#ref = config["genomePath"],
		convertype = config["convertype"],
		annotsvconf = config["annotsvconf"],
		convertscript = config["convertscript"]
		#versionannotsv = config["versionannotsv"]
	log:
		output+"/logs/{sample}.fileconversion.log"
	shell:
		"""
		docker run --rm -v /home1:/home1 -v /home1/data/STARK/data:/STARK/data tsvconvert:latest {params.convertscript} -i {input.tsvtmp} -o {output} -fo {params.convertype} -fi annotsv -c {params.annotsvconf} 2>{log} && [[ -s {output} ]] && sed -i '/^#/! s/^/chr/' {output} || echo "No data to annotate" > {output}
		"""

rule AnnotSV2vcf_all:
	input:
		tsvtmp = output+"/CANOES/all.canoes.annotsv.tmp.tsv",
		tsv = output+"/CANOES/all.canoes.annotsv.tsv"
	output:
		output+"/CANOES/all.canoes.annotsv.vcf"
	params:
		#ref = config["genomePath"],
		convertype = config["convertype"],
		annotsvconf = config["annotsvconf"],
		convertscript = config["convertscript"]
		#versionannotsv = config["versionannotsv"]
	log:
		output+"/logs/all.fileconversion.log"
	shell:
		"""
		docker run --rm -v /home1:/home1 -v /home1/data/STARK/data:/STARK/data tsvconvert:latest {params.convertscript} -i {input.tsvtmp} -o {output} -fo {params.convertype} -fi annotsv -c {params.annotsvconf} 2>{log} && [[ -s {output} ]] && sed -i '/^#/! s/^/chr/' {output} || echo "No data to annotate" > {output}
		"""
