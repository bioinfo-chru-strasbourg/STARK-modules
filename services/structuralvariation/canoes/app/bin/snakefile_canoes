################## Import libraries ##################
import os
import glob
import pandas as pd
import json
import csv
from shutil import copy2
from datetime import datetime
from itertools import product
from collections import defaultdict
from jinja2 import Environment, FileSystemLoader

from os.path import join as osj

################## Configuration file ##################
configfile: "/app/config/snakefile/canoes_default.yaml"
####################### FUNCTIONS ######################

def extract_columns_per_sample(input_file, output_file):
	# Read the input file into a pandas DataFrame
	df = pd.read_csv(input_file, sep='\t')

	# Write the data to the specified output file
	for column in df.columns[1:]:  # Exclude the first column (chromosome segment)
		df[[df.columns[0], column]].to_csv(output_file, sep='\t', index=False, header=False)

def fix_canoe_tsv(input_file, output_file):
    data = pd.read_csv(input_file, sep='\t', header=None)
    data.columns = ["Sample_ID", "SV type", "Coordinates"]

    # Modify chromosome values
    #data['#Chrom'] = data['Coordinates'].str.split(':').str[0].replace({'23': 'X', '24': 'Y'})
    data[['Start', 'End']] = data['Coordinates'].str.split(':').str[1].str.split('-', expand=True)

    # Write to output file
    with open(output_file, "w") as o:
        o.write("#Chrom\tStart\tEnd\tSV type\tSamples_ID\n")
        for index, row in data.iterrows():
            if row['Sample_ID'].startswith("no CNV found") or row['Sample_ID'].startswith("error in the analysis"):
                o.write(row['Sample_ID'] + '\n')
            else:
                o.write(f"{row['#Chrom']}\t{row['Start']}\t{row['End']}\t{row['SV type']}\t{row['Sample_ID']}\n")


def addOtherSamples(sexless_list, sexlist, infos, run, exclude_tag):
	new_sample_list = []
	project = os.path.dirname(run)
	runID = os.path.basename(run)

	if not os.path.exists(project):
		return infos, sexlist, new_sample_list

	searchrun = systemcall(f"find {project} -maxdepth 1 -mindepth 1 -type d -mtime -30 ! -name {runID}")
	if not searchrun:
		searchrun = systemcall(f"find {project} -maxdepth 1 -mindepth 1 -type d -mtime -60 ! -name {runID}")
		if not searchrun:
			for v in sexless_list:
				sexlist.remove(v)
			return infos, sexlist, new_sample_list
		runlist = searchrun

	else:
		runlist = searchrun

	for x in sexless_list:
		infos_supp = {}

		for runs in runlist:
			if os.path.exists(os.path.join(runs, "STARKCopyComplete.txt")):
				for sampleID in os.listdir(runs):
					if os.path.isdir(os.path.join(runs, sampleID)):
						tags = systemcall(f"find {os.path.join(runs, sampleID)} -maxdepth 2 -name {sampleID}.tag")[0]
						if tags:
							sex = ''
							row = open(tags, 'r').readline().strip().split('!')
							for tag in row:
								if 'SEX' in tag and ('_' in tag or '#' in tag):
									sex = tag.split('_')[-1] if '_' in tag else tag.split('#')[-1]
								elif any(exclude in tag for exclude in exclude_tag):
									sex = ''
								break
							if sex == x:
								bam = systemcall(f"find {os.path.join(runs, sampleID)} -maxdepth 2 -name '*.bam' ! -name '*validation*'")[0]
								if sex and bam and len(infos_supp) < 4:
									infos_supp[sampleID] = {'sex': sex, 'bam': bam}
								else:
									infos.update(infos_supp)
									return infos, sexlist, new_sample_list
	for v in sexless_list:
		sexlist.remove(v)
	return infos, sexlist, new_sample_list

def createplotstats(coverage):
	'''
	Generate necessary stats to plot CANOES analysis bar and boxplot
	Return a dictionary with those stats
	'''
	dictionary = {'header': [], 'depth': []}
	with open(coverage, 'r') as file:
		for line in file:
			if line.startswith('chrom'):
				header_line = line.strip().split('\t')
				dictionary['header'].append('Chr:Start-End')
				dictionary['header'].extend(header_line[3:])
			elif line.startswith('chrX') or line.startswith('chrY'):
				continue
			else:
				line_elements = line.strip().split('\t')
				metrics = []
				for depth in map(int, line_elements[3:]):
					try:
						metrics.append(str(round(float(depth / (int(line_elements[2]) - int(line_elements[1]))), 4)).replace(',', '.'))
					except ZeroDivisionError:
						metrics.append('0')
				dictionary['depth'].append(line_elements[0] + ':' + line_elements[1] + '-' + line_elements[2] + '\t' + '\t'.join(metrics))
	return dictionary

def parse_samplesheet(samplesheet_path):
	"""
	samplesheet_path: absolute path of a samplesheet file, Illumina format
	return: a dataframe containing 9 columns :
	Sample_ID, Sample_Plate, Sample_Well, I7_Index_ID, index, Manifest, GenomeFolder, Sample_Project, Description
	The description field contains tags separated by ! ; the name of the tag and the value is separated by # (ex: SEX#F!APP#DIAG.BBS_RP)
	"""
	header_line = next(line.strip().split(',') for line in open(samplesheet_path) if 'Sample_ID' in line)
	df = pd.read_csv(samplesheet_path, skiprows=1, names=header_line)
	df['Description'] = df['Description'].apply(lambda x: dict(item.split('#') for item in x.split('!')))
	
	return df

def getSampleInfos(samplesheet_path, exclude_samples):
	"""
	samplesheet_path: absolute path of a samplesheet file, Illumina format
	return a dictionary with Sample_ID from samplesheet as key and 'gender': 'F or M or NULL'
	"""
	result_dict = {}
	
	if samplesheet_path:
		samplesheet = parse_samplesheet(samplesheet_path)
		
		for _, rows in samplesheet.iterrows():
			sampleID = rows["Sample_ID"]
			
			if any(exclude in sampleID for exclude in exclude_samples):
				continue
			
			result_dict[sampleID] = {'gender': next((tag.split('_')[-1] for tag in rows['Description'].split('!') if 'SEX' in tag), '')}
	
	return result_dict

def populate_dictionary(dictionary, samples_list, extensions_list, files_list, pattern_include=None, pattern_exclude=None, split_index=0):
	for sample in samples_list:
		for ext in extensions_list:
			for file in files_list:
				file_parts = os.path.basename(file).split(".")
				
				if split_index >= len(file_parts):
					continue  # Skip if split_index is out of range
				
				file_base = file_parts[split_index]
				
				if file_base != sample or not os.path.basename(file).endswith(ext):
					continue

				if pattern_exclude and pattern_exclude in file:
					print('pattern_exclude')
					continue

				if pattern_include and pattern_include not in file:
					print('pattern_include')
					continue

				dictionary.setdefault(sample, {})[ext] = file


def filter_files(files_list, filter_in=None, filter_out=None):
	return [file_name for file_name in files_list if (not filter_in or filter_in in file_name) and (not filter_out or filter_out not in file_name)]


def find_item_in_dict(sample_list, ext_list, dictionary, include_ext, exclude_ext=None):
	""" Function to search in a dictionary for a non-empty file path by iterating through sample_list and ext_list with inclusion and exclusion filters """
	search_result = ""
	
	for sample in sample_list:
		for ext in ext_list:
			try:
				items = dictionary.get(sample, {}).get(ext, [])
				if include_ext in items and (exclude_ext is None or exclude_ext not in items):
					if os.path.exists(items) and os.path.getsize(items) != 0:
						search_result = items
			except KeyError:
				pass
	
	return search_result

def searchfiles(directory, search_arg, recursive_arg):
	""" Function to search all files in a directory, adding a search arguement append to the directory and a recursive_search options (True/False) """
	return sorted(filter(os.path.isfile, glob.glob(directory + search_arg, recursive=recursive_arg)))

def log_file(logfile, text, sep, items_list=None, items=None):
	""" Function to log a variable value or a list of values into a log file """
	with open(logfile, 'a+') as f:
		f.write(f"{text}{sep}")
		if items_list:
			for item in items_list:
				f.write(f"{str(item) if item != '' else 'None'}{sep}")
		else:
			f.write(f"{str(items) if items != '' else 'None'}{sep}")

def extractlistfromfiles(file_list, ext_list, sep, position):
	""" Function for creating list from a file list, with a specific extension, a separator and the position of the string we want to extract """
	return list(set(os.path.basename(files).split(sep)[position] for files in file_list if any(files.endswith(ext) for ext in ext_list)))

def deconbedfromdesign(inputbed, outputbed, sepgenecolumn):
	""" Function to extract a DECON bed 4 columns from another bed, assuming that the 4th column contains a gene name with a unique separator """
	df = pd.read_csv(inputbed, sep='\t', names=["Chr", "Start", "End", "Gene"], index_col=False)
	df['Gene'] = df['Gene'].str.split(sepgenecolumn).str[0] # we extract the gene name == string before separator (ex TP73_chr1_3598878_3599018 sep is _chr to get TP73)
	df.to_csv(outputbed, sep='\t', index=False, header=False)

def create_list(txtlistoutput, file_list, ext_list, pattern):
	""" Function to create a txt file from a list of files filtered by extension and pattern """
	with open(txtlistoutput, 'a+') as f:
		f.writelines(f"{files}\n" for files in file_list if any(files.endswith(ext) and pattern in files for ext in ext_list))

def intersectbed(inputbed, refbed, outputbed):
	""" Function to intersect a bed with a ref bed using bedtools """
	shell(f"intersectBed -a {inputbed} -b {refbed} -loj > {outputbed}")

def concatenatelist(inputlist, outputfile):
	""" Function to concatenate lines tab separated in a file """
	shell(f"xargs --delimiter='\\n' cat <{inputlist} >> {outputfile}")

def keep_unknown(columnname, pattern, prefix):
	""" Function to rename gene """
	for i, gene in enumerate(df[columnname]):
		if gene == pattern:
			df.at[i, columnname] = f"{prefix}{i}"

def extract_tag(tagfile, tag, tagsep, sep):
	""" Function to extract a tag """
	row = open(tagfile, 'r').readline().strip().split(tagsep)
	output_tag = next((items.split(sep)[-1] for items in row if tag in items and sep in items), "")
	return output_tag

def kmerisation(kmerSize, bedFile, kmerBedFile):
	""" Bed kmerisation """
	print(f"{kmerSize} kmerisation of your bed {bedFile} in progress")
	
	with open(bedFile, 'r') as readBed, open(kmerBedFile, 'w+') as writeKbed:
		for line in readBed:
			if line.startswith('#'):
				continue
			
			chr, start, end, gene = line.split()[:4]
			diff = int(end) - int(start)
			
			while diff >= kmerSize:
				newEnd = int(start) + kmerSize - 1
				writeKbed.write(f"{chr}\t{start}\t{newEnd}\t{gene}\n")
				start, diff = newEnd + 1, diff - kmerSize
			
			if diff > 0:
				writeKbed.write(f"{chr}\t{start}\t{end}\t{gene}\n")
	
	print('Kmerisation done')

def replace_path(file_paths, old_substring, new_substring):
	return [path.replace(old_substring, new_substring).lstrip("/") for path in file_paths]

def generate_html_report(result_dict, run_name, service_name, sample_list, output_file='report.html'):
	# Set up Jinja2 environment
	env = Environment(loader=FileSystemLoader(config['TEMPLATE_DIR']))
	template = env.get_template('template.html')

	# Render the template with the provided data
	rendered_html = template.render(
		runDict=result_dict,
		runName=run_name,
		serviceName=service_name,
		sample_list=sample_list
	)

	# Save the rendered HTML to the output file
	with open(output_file, 'w') as f:
		f.write(rendered_html)

	print(f"HTML report generated successfully: {output_file}")

def merge_pdfs(files, output_path):
	pdf_merger = PdfWriter()

	for file in files:
		with open(file, 'rb') as pdf_file:
			pdf_merger.append(pdf_file)

	with open(output_path, 'wb') as output_file:
		pdf_merger.write(output_file)

### END OF FUNCTIONS ###

serviceName = config['serviceName']
#date_time = datetime.now().strftime("%Y%m%d-%H%M%S")
date_time = "123456-7890"
runName = os.path.basename(os.path.normpath(config['run']))
resultDir = f"/app/res/{runName}/{date_time}"
outputDir = config['OUTPUT_DIR'] if config['OUTPUT_DIR'] else config['run']

try:
	config['GROUP_NAME'] = os.path.normpath(config['run']).split('/')[4]
	config['APP_NAME'] = os.path.normpath(config['run']).split('/')[5]
except IndexError: 
	pass
depotDir = f"{config['depository']}/{config['GROUP_NAME']}/{config['APP_NAME']}/{runName}"

directories = [resultDir, outputDir]
if config['DEPOT_COPY']:
	directories.append(depotDir)
for directory in directories:
	os.makedirs(directory, exist_ok=True)

# Search files in repository 
files_list = searchfiles(os.path.normpath(config['run']), config['SEARCH_ARGUMENT'],  config['RECURSIVE_SEARCH'])

# Create sample and aligner list
sample_list = extractlistfromfiles(files_list, config['PROCESS_FILE'], '.', 0)
aligner_list = extractlistfromfiles(files_list, config['PROCESS_FILE'], '.', 1)

# Exclude samples from the exclude_list , case insensitive
sample_list = [sample for sample in sample_list if not any(sample.upper().startswith(exclude.upper()) for exclude in config['EXCLUDE_SAMPLE'])]

# If filter_sample_list variable is not empty, it will force the sample list
if config['FILTER_SAMPLE']:
	sample_list = list(config['FILTER_SAMPLE'])

# For validation analyse bam will be sample.aligner.validation.bam, so we append .validation to all the aligner strings
if config['VALIDATION_ONLY']:
	filter_files(files_list, filter_in='validation')
	append_aligner = '.validation'
	aligner_list = [sub + append_aligner for sub in aligner_list]
else:
	filter_files(files_list, None ,filter_out='validation')

runDict = defaultdict(dict)
populate_dictionary(runDict, sample_list, config['EXT_INDEX_LIST'], files_list, None, 'validation')

# Set a filelist with all the files tag ; file format is sample.tag
tagfile_list = [runDict[sample]['.tag'] for sample in sample_list if sample in runDict and '.tag' in runDict[sample]]

# Extract the tags from the list
# tag ex SEX#M!POOL#POOL_HFV72AFX3_M_10#POOL_HFV72AFX3_F_11!
for tagfile in tagfile_list:
	for sample in sample_list:
		runDict[os.path.basename(tagfile).split(".")[0]]['gender'] = extract_tag(tagfile, 'SEX', '!', '#')

# Extract the gender_list from dictionary with the key gender
gender_list = ['A'] + ['XX' if runDict[sample]['gender'] == 'F' else 'XY' for sample in sample_list if sample in runDict and 'gender' in runDict[sample]]
# Removing duplicate
gender_list = list(set(gender_list))

# Find bed file (Design)
config['BED_FILE'] = config['BED_FILE'] or find_item_in_dict(sample_list, config['EXT_INDEX_LIST'], runDict, '.design.bed', '.genes.bed')
# Find genes file (Panel); we can't use .genes files because .list.genes and .genes are not distinctable from the indexing we made
config['GENES_FILE'] = config['GENES_FILE'] or find_item_in_dict(sample_list, config['EXT_INDEX_LIST'], runDict, '.genes.bed', '.list.genes')
# Find transcripts files (NM)
config['TRANSCRIPTS_FILE'] = config['TRANSCRIPTS_FILE'] or find_item_in_dict(sample_list, config['EXT_INDEX_LIST'], runDict, '.transcripts', '.list.transcripts')
# If transcript file exist, create the annotation file for AnnotSV
annotation_file = f"{resultDir}/{serviceName}.{date_time}.AnnotSV.txt"
if os.path.exists(config['TRANSCRIPTS_FILE']) and os.path.getsize(config['TRANSCRIPTS_FILE']):
	df = pd.read_csv(config['TRANSCRIPTS_FILE'], sep='\t', names=["NM", "Gene"])
	with open(annotation_file, 'w+') as f:
		f.write('\t'.join(df['NM']))
else:
	with open(annotation_file, 'w') as f:
		f.write("No NM found")

# Find list.genes files 
config['LIST_GENES'] = config['LIST_GENES'] or find_item_in_dict(sample_list, config['EXT_INDEX_LIST'], runDict, '.list.genes', '.list.transcripts')

# Transform list_genes into a list if list_genes exist, else use genes_file if exist
panel_list = []
panel_list_trunc = []
if config['LIST_GENES']:
	with open(config['LIST_GENES']) as f:
		panel_list = f.read().splitlines()
elif config['GENES_FILE'] and not config['LIST_GENES']:
	panel_list.append(os.path.basename(config['GENES_FILE']).split(".bed",1)[0]) # we split the .bed ext because list don't have .bed but genes_file does
# cp files from panel_list to resultDir and rename them
for panel in panel_list:
	inputfile = f"{os.path.dirname(config['LIST_GENES'])}/{panel}.bed"# panel_list don't have the bed extension, need that for the copy
	# cut sample. and .genes from file name so files will have the same name as the truncated name list
	panel_trunc = panel.split(".", 1)[1].split(".genes",1)[0]
	outputfile = f"{resultDir}/{panel_trunc}"
	copy2(inputfile, outputfile)
	# Create a new list for expand, names are filenames without sample and .genes.bed ext
	panel_list_trunc.append(panel_trunc)

if config['KMER']:
	kmerisation(config['KMER'], config['BED_FILE'], deconbed_file)

logfile = f"{resultDir}/{serviceName}.{date_time}.parameters.log"
if not config['BED_FILE']:
	log_file(logfile, 'No bed found, CANOES cannot continue, exiting', "\n")
	exit()

# Create file_list of bam by gender with the runDict, depending on the gender_list
# A = all ; XX = Female only ; XY = Male only
# files_list_A contains the full path of all files (bam files for ex)
# Warning the key dictionnary for sexe is A/M/F but the gender_list is A/XY/XX
files_list_A = [runDict[sample]['.bam'] for sample in sample_list if sample in runDict and '.bam' in runDict[sample]]
files_list_XX = [files for files in files_list_A if runDict.get(os.path.basename(files).split(".")[0], {}).get('gender') == 'F']
files_list_XY = [files for files in files_list_A if runDict.get(os.path.basename(files).split(".")[0], {}).get('gender') == 'M']

# Creating a txt list for the bam files per aligner per gender (A, M & F)
for aligner in aligner_list:
	for gender in gender_list:
		bamlist = f"{resultDir}/{serviceName}.{date_time}.{gender}.list.txt"
		create_list(bamlist, globals()[f"files_list_{gender}"], config['PROCESS_FILE'], aligner)

# Option to remove gender in the gender_list to force A, XX or XY analysis only ; the calling of Y for male subject is not done
gender_list = [gender for gender in gender_list if not (config['REMOVE_M'] and gender == 'XY') and not (config['REMOVE_F'] and gender == 'XX') and not (config['REMOVE_A'] and gender == 'A')]

# Process .ped files 
ped_file = find_item_in_dict(sample_list, config['EXT_INDEX_LIST'], runDict, '.ped')
# Add 'hpo' key with None value for all individuals
for individual_id in runDict:
	runDict[individual_id]['hpo'] = None

if os.path.exists(ped_file) and os.path.getsize(ped_file) > 0:
	df = pd.read_csv(ped_file, sep='\t', dtype=str)
	# Iterate over rows and update runDict
	for index, row in df.iterrows():
		individual_id = row['Individual ID']
		hpo_list = row['HPOList']
		
		if isinstance(hpo_list, float):
			hpo_list = str(hpo_list)
		hpo_list = hpo_list.replace(" ", "")
		
		if individual_id in sample_list:
			runDict[individual_id]['hpo'] = hpo_list

log_items = [
	('Start of the analysis:', date_time),
	('Analysing run:', runName),
	('List of all samples:', sample_list),
	('List of gender from tags:', gender_list),
	('Input files for autosomes analysis:', files_list_A),
	('Input files for ChrX analysis, Female only:', files_list_XX),
	('Input files for ChrX analysis, Male only:', files_list_XY),
	('Aligner list from files:', aligner_list),
	('Design bed file:', config['BED_FILE']),
	('Panel bed file:', config['GENES_FILE']),
	('Transcripts file:', config['TRANSCRIPTS_FILE']),
	('Genes list file', config['LIST_GENES'])
]

for item in log_items:
	log_file(logfile, item[0], "\n", items_list=item[1] if isinstance(item[1], list) else None, items=item[1] if not isinstance(item[1], list) else None)

# Copy2 bed_file & genes_file & transcripts_file for debug
if config['DEBUG_MODE']:
	try:
		files_to_copy = (config['BED_FILE'], config['GENES_FILE'], config['TRANSCRIPTS_FILE'])
		for file_path in files_to_copy:
			copy2(file_path, resultDir)
	except FileNotFoundError:
		pass

print(dict(runDict))

################################################## RULES ##################################################

# check the number of sample for copy or merge vcf rule
sample_count = len(sample_list) 

# Priority order
ruleorder: copy_bam > copy_cram > cramtobam > indexing

rule all:
	""" Output a design vcf.gz with the bams and the bed provided """
	input:
		f"{resultDir}/GCpercent.tsv",
		#expand(f"{resultDir}/{{sample}}/{serviceName}/{{sample}}_{date_time}_{serviceName}/{serviceName}.{date_time}.{{sample}}.{{aligner}}.barplot.png", aligner=aligner_list, sample=sample_list),
		expand(f"{resultDir}/{serviceName}/{serviceName}.{date_time}.{{aligner}}.allsamples.multicoverage.tsv", aligner=aligner_list),
		expand(f"{resultDir}/{serviceName}/{serviceName}.{date_time}.{{aligner}}.{gender}.CNVCall.tsv", aligner=aligner_list, gender=gender_list),
		expand(f"{resultDir}/{serviceName}/{serviceName}.{date_time}.allsamples.{{aligner}}.CNVCall.tsv", aligner=aligner_list)

rule help:
	"""
	General help for CANOES
	Launch snakemake -s snakefile_canoes -c(numberofthreads) --config run=absolutepathoftherundirectory
	To launch the snakemake file, use --config to replace variables that must be properly set for the pipeline to work ie run path directory
	Every variable defined in the yaml file can be change
	Separate multiple variable with a space (ex  --config run=runname transProb=0.05 var1=0.05 var2=12)
	Use option --configfile another.yaml to replace and merge existing default config.yaml file variables
	Use -p to display shell commands
	Use --lt to display docstrings of rules
	Use -n for a dry run
	Input file = bam or cram files (if bai is needed, it will be generate)
	Output file = vcf annoted with AnnotSV 3.x for each sample/bam, and a global vcf file will all the samples ; a set of vcf files by design/panel
	"""

rule copy_bam:
	output: temp(f"{resultDir}/{{sample}}.{{aligner}}.bam")
	params:
		process=config['PROCESS_CMD'],
		download_link=lambda wildcards: runDict[wildcards.sample]['.bam']
	shell: "[ \"{params.process}\" = \"ls\" ] && ln -sfn {params.download_link} {output} || rsync -azvh {params.download_link} {output}"

rule copy_cram:
	output: temp(f"{resultDir}/{{sample}}.{{aligner}}.cram")
	params:
		process=config['PROCESS_CMD'],
		download_link=lambda wildcards: runDict[wildcards.sample]['.cram']
	shell: "[ \"{params.process}\" = \"ls\" ] && ln -sfn {params.download_link} {output} || rsync -azvh {params.download_link} {output}"

rule cramtobam:
	""" Extract bam from a cram file with samtools, need a reference genome """
	input: rules.copy_cram.output
	output: temp(f"{resultDir}/{{sample}}.{{aligner}}.bam")
	params: refgenome=config['REFGENEFA_PATH']
	shell: "samtools view -b -T {params.refgenome} -o {output} {input}"

rule indexing:
	""" Indexing bam files with samtools """
	input: f"{resultDir}/{{sample}}.{{aligner}}.bam"
	output: temp(f"{resultDir}/{{sample}}.{{aligner}}.bai")
	threads: workflow.cores
	shell: "samtools index -b -@ {threads} {input} {output}"

rule gc_percent:
	""" Compute GC percent from the reference genome and a bed file, removing header 
		Output a tsv with 4 columns : chromosome start stop gc	
	"""
	input: config["REFGENEFA_PATH"]
	output: 
			gc_percent_header=temp(f"{resultDir}/GCpercent_header.tsv"),
			gc_percent = f"{resultDir}/GCpercent.tsv"
	params: 
		bedfile = config['BED_FILE'],
		java_option = config['JAVA_OPTION']
	log: f"{resultDir}/GCpercent.log"
	shell: 
		"""
			gatk --java-options {params.java_option} AnnotateIntervals -L {params.bedfile} -R {input} -imr OVERLAPPING_ONLY -O {output.gc_percent_header} 2> {log}
			cat {output.gc_percent_header} | awk 'NR>27{{print $0}}' > {output.gc_percent}
	 	"""


# We can pipe the result in the previous rule
#rule filter_gc_percent_file:
#	""" Filter out the GC file, removing the 27 first lines """
#	input:  rules.gc_percent.output
#	output: f"{resultDir}/GCpercent.tsv"
#	shell: " cat {input} | awk 'NR>27{{print $0}}' > {output} "

rule bam_to_multicov:
	""" From each sample bam file generate a coverage file filtered with the bed provided """
	input: 
		bam = f"{resultDir}/{{sample}}.{{aligner}}.bam",
		bai = f"{resultDir}/{{sample}}.{{aligner}}.bai"
	output:	f"{resultDir}/{{sample}}/{serviceName}/{{sample}}_{date_time}_{serviceName}/{serviceName}.{date_time}.{{sample}}.{{aligner}}.multicov.tsv"
	params: scripts = config["scripts"],
			bed = config['BED_FILE']
	log: f"{resultDir}/{{sample}}/{serviceName}/{{sample}}_{date_time}_{serviceName}/{serviceName}.{date_time}.{{sample}}.{{aligner}}.multicov.log"
	shell:" python3 {params.scripts}/pybedtools.py --bam {input.bam} --bed {params.bed} -q 20 -o {output} 1> {log} "


rule merge_multicov:
	""" Merge all multicoverage files and add header """
	input: expand(f"{resultDir}/{{sample}}/{serviceName}/{{sample}}_{date_time}_{serviceName}/{serviceName}.{date_time}.{{sample}}.{{aligner}}.multicov.tsv", aligner=aligner_list, sample=sample_list)
	output: f"{resultDir}/{serviceName}/{serviceName}.{date_time}.{{aligner}}.allsamples.multicoverage.tsv"
	log: f"{resultDir}/{serviceName}/{serviceName}.{date_time}.{{aligner}}.allsamples.multicoverage.log"
	params: scripts = config["scripts"]
	shell: " python3 {params.scripts}/merge_multicov.py {input} -o {output} "


rule plot_coverage_stats:
	""" Generate coverage calculation """
	input:  rules.merge_multicov.output
	output:	f"{resultDir}/{serviceName}/{serviceName}.{date_time}.{{aligner}}.allsamples.multicoverage.stats.tsv"
	params: scripts = config["scripts"]
	shell: " python3 {params.scripts}/plot_coverage_stats.py {input} {output} "


#rule split_per_sample:
#	""" Split multicoverage datas per sample """
#	input:  rules.plot_coverage_stats.output
#	output: f"{resultDir}/{{sample}}/{serviceName}/{{sample}}_{date_time}_{serviceName}/{serviceName}.{date_time}.{{sample}}.{{aligner}}.multicoverage.stats.tsv"
#	run: extract_columns_per_sample(input[0], output[0])


rule plot_coverage:
	""" Generate coverage plots per sample """
	input: rules.plot_coverage_stats.output
	output: 
			f"{resultDir}/{{sample}}/{serviceName}/{{sample}}_{date_time}_{serviceName}/{serviceName}.{date_time}.{{sample}}.{{aligner}}.barplot.png"
			#f"{resultDir}/{{sample}}/{serviceName}/{{sample}}_{date_time}_{serviceName}/{serviceName}.{date_time}.{{sample}}.{{aligner}}.boxplot.png"
	params: scripts = config['scripts']
	log:
		log=f"{resultDir}/{{sample}}/{serviceName}/{{sample}}_{date_time}_{serviceName}/{serviceName}.{date_time}.{{sample}}.{{aligner}}.plotCoverage.log",
		err=f"{resultDir}/{{sample}}/{serviceName}/{{sample}}_{date_time}_{serviceName}/{serviceName}.{date_time}.{{sample}}.{{aligner}}.plotCoverage.err"
	shell: " Rscript {params.scripts}/plotCoverage.R --sample {sample} --input {input} --output {output} 1> {log.log} 2> {log.err} "

rule canoes_calling:
	""" CANOES analysis """
	input: 
			read = rules.merge_multicov.output,
			gc = rules.gc_percent.output.gc_percent
	params:
			scripts = config["scripts"],
			chromosome= lambda wildcards: "{gender}",
			bamlist= lambda wildcards: f"{resultDir}/{serviceName}.{date_time}.{{gender}}.list.txt",
			pvalue = config["pval"],
			distance = config["dist"],
			tnumeric = config["tnum"],
			numreference = config["numref"],
			hom = config["homdel"]
	output:
			cnvcall = f"{resultDir}/{serviceName}/{serviceName}.{date_time}.{{aligner}}.{gender}.CNVCall.tsv"
			#pdf =  f"{resultDir}/{serviceName}/{serviceName}.{date_time}.{{aligner}}.{gender}.Genotyping.pdf"
	log: 
		log = f"{resultDir}/{serviceName}/{serviceName}.{date_time}.{{aligner}}.{gender}.CANOEScalling.log",
		err = f"{resultDir}/{serviceName}/{serviceName}.{date_time}.{{aligner}}.{gender}.CANOEScalling.err"
	shell:
		"""
			Rscript {params.scripts}/CANOES.v2.R --gcfile {input.gc} --readsfile {input.read} --chromosome {params.chromosome} \
			--samples {params.bamlist} --homdel {params.hom} --numref {params.numreference} --tnum {params.tnumeric} --distance {params.distance} \
			--pvalue {params.pvalue} --output {output.cnvcall} 1> {log.log} 2> {log.err}
		"""

rule merge_makeCNVcalls:
	""" Merge all CNV call results """
	input: expand(f"{resultDir}/{serviceName}/{serviceName}.{date_time}.{{aligner}}.{gender}.CNVCall.tsv", gender=gender_list, aligner=aligner_list)
	output: f"{resultDir}/{serviceName}/{serviceName}.{date_time}.allsamples.{{aligner}}.CNVCall.tsv"
	shell: "cat {input} | sort -u | sort -r >> {output}"


# Should be variantconvert tsv to vcf ?
rule canoes_to_bed:
	""" CANOES CNVcall tsv to bed conversion """
	input: rules.merge_makeCNVcalls.output
	output: f"{resultDir}/{serviceName}/{serviceName}.{date_time}.allsamples.{{aligner}}.CNVCall.bed"
	run: fix_canoe_tsv(input[0], output[0])

rule bedtovcf:
	""" Convert CANOES bed to vcf """
	input: rules.canoes_to_bed.output
	output: f"{resultDir}/{serviceName}/{serviceName}.{date_time}.allsamples.{{aligner}}.vcf"
	log: f"{resultDir}/{serviceName}/{serviceName}.{date_time}.allsamples.{{aligner}}.bedtovcf.log"
	params:	scripts = config["scripts"]
	shell: "variantconvert convert -i {input} -o {output} -c {params.scripts}/fileconversion/config_canoes_bed.json 2> {log} "

rule sortvcf:
	""" Sort vcf """
	input: rules.bedtovcf.output
	output: temp(f"{resultDir}/{serviceName}.{date_time}.allsamples.{{aligner}}.Design.vcf")
	params: dummypath=config['DUMMY_PATH']
	shell: "grep '^#' {input} > {output} && grep -v '^#' {input} | sort -k1,1V -k2,2g >> {output} && [[ -s {output} ]] || cat {params.dummypath}/empty.vcf > {output}"

# bgzip -c to keep input file
rule vcf2gz:
	""" Compress vcf with bgzip """
	input: rules.sortvcf.output
	output: f"{resultDir}/{serviceName}.{date_time}.allsamples.{{aligner}}.Design.vcf.gz"
	params: dummypath=config['DUMMY_PATH']
	shell: "bgzip -c {input} > {output} && [[ -s {output} ]] || cat {params.dummypath}/empty.vcf.gz > {output} ; tabix {output}"


rule splitvcf:
	"""
	Split vcf into each individual sample with bcftools
	-s {sample}: comma-separated list of samples to include
	-Oz: output vcf compressed
	-c1: minimum allele count (INFO/AC) of sites to be printed
	"""
	input: rules.vcf2gz.output
	output: f"{resultDir}/{{sample}}/{serviceName}/{{sample}}_{date_time}_{serviceName}/{serviceName}.{date_time}.{{sample}}.{{aligner}}.Design.Noannotation.vcf.gz"
	params: dummypath=config['DUMMY_PATH']
	shell: "mkdir -p {resultDir}/{wildcards.sample}/{serviceName} && bcftools view -c1 -Oz -s {wildcards.sample} -o {output} {input} && [[ -s {output} ]] || cat {params.dummypath}/empty.vcf | sed 's/SAMPLENAME/{wildcards.sample}/g' | gzip > {output} ; tabix {output}"


rule AnnotSV:
	"""
	Annotate and rank Structural Variations from a vcf file
	-annotationMode can be split by exons/introns or full by genes
	-txtFile: path to a file containing a list of preferred genes transcripts for annotation
	-genomeBuild must be specified if not hg19
	"""
	input: rules.splitvcf.output
	output: f"{resultDir}/{{sample}}/{serviceName}/{{sample}}_{date_time}_{serviceName}/{serviceName}.{date_time}.{{sample}}.{{aligner}}.AnnotSV.Design_unsort.vcf"
	log: f"{resultDir}/{{sample}}/{serviceName}/{{sample}}_{date_time}_{serviceName}/{serviceName}.{date_time}.{{sample}}.{{aligner}}.AnnotSV.log"
	params:
		output=f"{resultDir}/{{sample}}/{serviceName}/{{sample}}_{date_time}_{serviceName}/{serviceName}.{date_time}.{{sample}}.{{aligner}}.AnnotSV.Design_unsort",
		genome=config['genomeBuild'],
		overlap=config['overlap'],
		mode=config['annotationMode'],
		annotation=config['annotationdir'],
		hpo=lambda wildcards: runDict[wildcards.sample]['hpo']
	shell: 
		"""
		AnnotSV -SVinputFile {input} -outputFile {params.output} -annotationMode {params.mode} -annotationsDir {params.annotation} -hpo {params.hpo} -txFile {annotation_file} -genomeBuild {params.genome} -overlap {params.overlap} -vcf 1 > {log} && [[ -s {output} ]] || echo "#No data to annotate" > {output}
		"""

use rule sortvcf as sortvcf1 with:
	input: rules.AnnotSV.output
	output: f"{resultDir}/{{sample}}/{serviceName}/{{sample}}_{date_time}_{serviceName}/{serviceName}.{date_time}.{{sample}}.{{aligner}}.Design.vcf"

use rule vcf2gz as vcf2gz1 with:
	input: rules.sortvcf1.output
	output: f"{resultDir}/{{sample}}/{serviceName}/{{sample}}_{date_time}_{serviceName}/{serviceName}.{date_time}.{{sample}}.{{aligner}}.Design.vcf.gz"

rule mergedesign:
	""" Copy or merge multiple vcfs using bcftools """
	input: expand(f"{resultDir}/{{sample}}/{serviceName}/{{sample}}_{date_time}_{serviceName}/{serviceName}.{date_time}.{{sample}}.{{aligner}}.Design.vcf.gz", sample=sample_list, aligner=aligner_list)
	output: f"{resultDir}/{serviceName}.{date_time}.allsamples.Design.vcf.gz"
	shell: f"if [ {sample_count} -eq 1 ]; then cp {{input}} {{output}} && tabix {{output}}; else bcftools merge {{input}} -O z -o {{output}} && tabix {{output}}; fi"

rule convertvcf_alldesign:
	""" vcf to tsv conversion """
	input: rules.mergedesign.output
	output: f"{resultDir}/{serviceName}.{date_time}.allsamples.AnnotSV.Design.tsv"
	log: log2=f"{resultDir}/{serviceName}.{date_time}.allsamples.AnnotSV.Design.vcf2tsv_converter.log"
	shell: "vcf2tsvpy --keep_rejected_calls --input_vcf {input} --out_tsv {output}"

onstart:
	shell(f"touch {os.path.join(outputDir, f'{serviceName}Running.txt')}")
	with open(logfile, "a+") as f:
		f.write("\n")
		f.write("Global parameters of the analysis for debug only")
		json.dump(config, f, ensure_ascii=False, indent=2)
		f.write("\n")

onsuccess:
	include = config['INCLUDE_RSYNC']
	shell(f"rm -f {outputDir}/{serviceName}Running.txt")
	shell(f"touch {outputDir}/{serviceName}Complete.txt")
	date_time_end = datetime.now().strftime("%Y%m%d-%H%M%S")
	with open(logfile, "a+") as f:
		f.write(f"End of the analysis : {date_time_end}\n")
	
	# Clear existing output directories
	for sample in sample_list:
		shell(f"rm -f {outputDir}/{sample}/{serviceName}/* || true")

	# Copy results to the main output directory
	shell("rsync -azvh --include={include} --exclude='*' {resultDir}/ {outputDir}")

	# Copy individual sample results to their respective directories
	for sample in sample_list:
		shell(f"cp {outputDir}/{sample}/{serviceName}/{sample}_{date_time}_{serviceName}/* {outputDir}/{sample}/{serviceName}/ || true")

	# Optionally, perform DEPOT_COPY
	if config['DEPOT_COPY']:
		shell("rsync -azvh --include={include} --exclude='*' {resultDir}/ {depotDir}")
		for sample in sample_list:
			shell(f"cp {outputDir}/{sample}/{serviceName}/{sample}_{date_time}_{serviceName}/* {depotDir}/{sample}/{serviceName}/ || true")
	
	# Generate dictionary for results
	result_files_list_sample = searchfiles(os.path.normpath(config['run']), f"/*/{serviceName}/*", False)
	result_files_list_all = searchfiles(os.path.normpath(config['run']), f"/*", False)
	result_files_list = result_files_list_all + result_files_list_sample
	replaced_paths = replace_path(result_files_list, config['run'], "")
	sample_list.insert(0,"allsamples")
	resultDict = defaultdict(dict)
	populate_dictionary(resultDict, sample_list, config['RESULT_EXT_LIST'], replaced_paths, pattern_include=serviceName, split_index=2)	
	print(dict(resultDict))
	# Generate html report
	generate_html_report(resultDict, runName, serviceName, sample_list, f"{outputDir}/{serviceName}_report.html")
	copy2('/app/template/style.css', outputDir)

onerror:
	include_log = config['INCLUDE_LOG_RSYNC']
	shell(f"touch {outputDir}/{serviceName}Failed.txt")
	shell(f"rm -f {config['OUTPUT_DIR']}/{serviceName}Running.txt")
	shell("rsync -azvh --include={include_log} --exclude='*' {resultDir}/ {outputDir}")